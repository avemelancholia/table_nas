{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7fa0db-3f3a-4a77-80f0-30b154d61b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.common.config import CfgNode\n",
    "\n",
    "from naslib import utils\n",
    "from naslib.search_spaces import SimpleCellSearchSpace\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.utils.custom_dataset import CustomDataset\n",
    "from naslib.optimizers import DARTSOptimizer, GDASOptimizer, RandomSearch\n",
    "from naslib.search_spaces import NasBench301SearchSpace, SimpleCellSearchSpace\n",
    "from naslib.utils import set_seed, setup_logger, get_config_from_args\n",
    "from naslib.search_spaces.core.query_metrics import Metric\n",
    "from naslib.search_spaces.core.graph import Graph\n",
    "from naslib.search_spaces.nasbenchasr.primitives import ASRPrimitive, CellLayerNorm, Head, ops, PadConvReluNorm\n",
    "from naslib.utils import get_project_root\n",
    "from naslib.search_spaces.core import primitives as core_ops\n",
    "from naslib.search_spaces.nasbenchasr.conversions import flatten, \\\n",
    "    copy_structure, make_compact_mutable, make_compact_immutable\n",
    "from naslib.search_spaces.nasbenchasr.encodings import encode_asr\n",
    "from naslib.utils.encodings import EncodingType\n",
    "from naslib.utils.log import log_every_n_seconds, log_first_n\n",
    "from naslib.search_spaces.core.primitives import AbstractPrimitive, Identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Path('/home/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632db49f-1d96-485f-95f8-30f2bc016128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_closest_square_shape(x):\n",
    "    len_x = len(x)\n",
    "    closest_square = int(np.round(np.sqrt(len_x)))\n",
    "    if closest_square ** 2 < len_x: \n",
    "        closest_square += 1\n",
    "    x = np.pad(x, (0, closest_square  ** 2 - len_x))\n",
    "    return x.reshape((1, closest_square, closest_square))\n",
    "\n",
    "class TabNasTorchDataset(Dataset):\n",
    "    def __init__(self, root_dir, name, queue, norm='min_max'):\n",
    "        super().__init__()\n",
    "        \n",
    "        bin_exists = False\n",
    "        num_exists = False\n",
    "        \n",
    "        assert queue in ['train', 'test', 'val']\n",
    "\n",
    "        self.root_dir = root_dir \n",
    "        self.name = name\n",
    "        self.type = queue\n",
    "\n",
    "        x_num = f'X_num_{queue}.npy'\n",
    "        x_bin = f'X_bin_{queue}.npy'\n",
    "        y = f'Y_{queue}.npy'\n",
    "        \n",
    "        \n",
    "        if (root_dir / name/ x_bin).exists():\n",
    "            bin_exists = True\n",
    "            \n",
    "        if (root_dir / name/ x_num).exists():\n",
    "            num_exists = True\n",
    "        \n",
    "        self.y = np.load(root_dir / name / y)\n",
    "        \n",
    "        if num_exists:\n",
    "            x_num = np.load(root_dir / name / x_num)\n",
    "            x_num = (x_num - np.min(x_num, axis=0)) / (np.max(x_num, axis=0) - np.min(x_num, axis=0))\n",
    "        \n",
    "        if bin_exists:\n",
    "            x_bin = np.load(root_dir / name / x_bin)\n",
    "        \n",
    "        if num_exists and bin_exists:\n",
    "            self.x = np.concatenate((x_num, x_bin), axis=1)\n",
    "        elif num_exists:\n",
    "            self.x = x_num\n",
    "        elif bin_exists:\n",
    "            self.x = x_bin\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.num_features = self.x.shape[1]\n",
    "        self.num_classes = int(np.max(self.y)) + 1\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i]\n",
    "        y = self.y[i]\n",
    "\n",
    "        return  x, y.astype(np.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33258b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/table_nas/asr_cell.yaml') as f:\n",
    "    config = CfgNode.load_cfg(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e11a04-8a8a-4602-a84c-7114ef765608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasDataset(CustomDataset):\n",
    "    def __init__(self, config, ds_train, ds_test, mode='train'):\n",
    "        super().__init__(config, mode)\n",
    "        self.ds_train = ds_train\n",
    "        self.ds_test = ds_test\n",
    "\n",
    "    def get_transforms(self, config):\n",
    "        return Compose([ToTensor()]), Compose([ToTensor()])\n",
    "\n",
    "\n",
    "    def get_data(self, data, train_transform, valid_transform):\n",
    "        train_data = self.ds_train\n",
    "        test_data = self.ds_test\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35850045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasTrainer(Trainer):\n",
    "    def _store_accuracies(self, logits, target, split):\n",
    "        \"\"\"Update the accuracy counters\"\"\"\n",
    "        logits = logits.clone().detach().cpu()\n",
    "        target = target.clone().detach().cpu()\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 1))\n",
    "        n = logits.size(0)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.train_top1.update(prec1.data.item(), n)\n",
    "            self.train_top5.update(prec5.data.item(), n)\n",
    "        elif split == \"val\":\n",
    "            self.val_top1.update(prec1.data.item(), n)\n",
    "            self.val_top5.update(prec5.data.item(), n)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown split: {}. Expected either 'train' or 'val'\")    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        retrain:bool=True,\n",
    "        search_model:str=\"\",\n",
    "        resume_from:str=\"\",\n",
    "        best_arch:Graph=None,\n",
    "        dataset_api:object=None,\n",
    "        metric:Metric=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the final architecture as given from the optimizer.\n",
    "\n",
    "        If the search space has an interface to a benchmark then query that.\n",
    "        Otherwise train as defined in the config.\n",
    "\n",
    "        Args:\n",
    "            retrain (bool)      : Reset the weights from the architecure search\n",
    "            search_model (str)  : Path to checkpoint file that was created during search. If not provided,\n",
    "                                  then try to load 'model_final.pth' from search\n",
    "            resume_from (str)   : Resume retraining from the given checkpoint file.\n",
    "            best_arch           : Parsed model you want to directly evaluate and ignore the final model\n",
    "                                  from the optimizer.\n",
    "            dataset_api         : Dataset API to use for querying model performance.\n",
    "            metric              : Metric to query the benchmark for.\n",
    "        \"\"\"\n",
    "        logger.info(\"Start evaluation\")\n",
    "        if not best_arch:\n",
    "\n",
    "            if not search_model:\n",
    "                search_model = os.path.join(\n",
    "                    self.config.save, \"search\", \"model_final.pth\"\n",
    "                )\n",
    "            self._setup_checkpointers(search_model)  # required to load the architecture\n",
    "\n",
    "            best_arch = self.optimizer.get_final_architecture()\n",
    "        #logger.info(f\"Final architecture hash: {best_arch.get_hash()}\")\n",
    "\n",
    "        if True:\n",
    "            best_arch.to(self.device)\n",
    "            if retrain:\n",
    "                logger.info(\"Starting retraining from scratch\")\n",
    "                best_arch.reset_weights(inplace=True)\n",
    "\n",
    "                (\n",
    "                    self.train_queue,\n",
    "                    self.valid_queue,\n",
    "                    self.test_queue,\n",
    "                ) = self.build_eval_dataloaders(self.config)\n",
    "\n",
    "                optim = self.build_eval_optimizer(best_arch.parameters(), self.config)\n",
    "                scheduler = self.build_eval_scheduler(optim, self.config)\n",
    "\n",
    "                start_epoch = self._setup_checkpointers(\n",
    "                    resume_from,\n",
    "                    search=False,\n",
    "                    period=self.config.evaluation.checkpoint_freq,\n",
    "                    model=best_arch,  # checkpointables start here\n",
    "                    optim=optim,\n",
    "                    scheduler=scheduler,\n",
    "                )\n",
    "\n",
    "                grad_clip = self.config.evaluation.grad_clip\n",
    "                loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                self.train_top1.reset()\n",
    "                self.train_top5.reset()\n",
    "                self.val_top1.reset()\n",
    "                self.val_top5.reset()\n",
    "\n",
    "                # Enable drop path\n",
    "                best_arch.update_edges(\n",
    "                    update_func=lambda edge: edge.data.set(\n",
    "                        \"op\", DropPathWrapper(edge.data.op)\n",
    "                    ),\n",
    "                    scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                    private_edge_data=True,\n",
    "                )\n",
    "\n",
    "                # train from scratch\n",
    "                epochs = self.config.evaluation.epochs\n",
    "                for e in range(start_epoch, epochs):\n",
    "                    best_arch.train()\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        log_first_n(\n",
    "                            logging.INFO,\n",
    "                            \"cuda consumption\\n {}\".format(torch.cuda.memory_summary()),\n",
    "                            n=20,\n",
    "                        )\n",
    "\n",
    "                    # update drop path probability\n",
    "                    drop_path_prob = self.config.evaluation.drop_path_prob * e / epochs\n",
    "                    best_arch.update_edges(\n",
    "                        update_func=lambda edge: edge.data.set(\n",
    "                            \"drop_path_prob\", drop_path_prob\n",
    "                        ),\n",
    "                        scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                        private_edge_data=True,\n",
    "                    )\n",
    "\n",
    "                    # Train queue\n",
    "                    for i, (input_train, target_train) in enumerate(self.train_queue):\n",
    "                        input_train = input_train.to(self.device)\n",
    "                        target_train = target_train.to(self.device, non_blocking=True)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        logits_train = best_arch(input_train)\n",
    "                        train_loss = loss(logits_train, target_train)\n",
    "                        if hasattr(\n",
    "                            best_arch, \"auxilary_logits\"\n",
    "                        ):  # darts specific stuff\n",
    "                            log_first_n(logging.INFO, \"Auxiliary is used\", n=10)\n",
    "                            auxiliary_loss = loss(\n",
    "                                best_arch.auxilary_logits(), target_train\n",
    "                            )\n",
    "                            train_loss += (\n",
    "                                self.config.evaluation.auxiliary_weight * auxiliary_loss\n",
    "                            )\n",
    "                        train_loss.backward()\n",
    "                        if grad_clip:\n",
    "                            torch.nn.utils.clip_grad_norm_(\n",
    "                                best_arch.parameters(), grad_clip\n",
    "                            )\n",
    "                        optim.step()\n",
    "\n",
    "                        self._store_accuracies(logits_train, target_train, \"train\")\n",
    "                        log_every_n_seconds(\n",
    "                            logging.INFO,\n",
    "                            \"Epoch {}-{}, Train loss: {:.5}, learning rate: {}\".format(\n",
    "                                e, i, train_loss, scheduler.get_last_lr()\n",
    "                            ),\n",
    "                            n=5,\n",
    "                        )\n",
    "\n",
    "                    # Validation queue\n",
    "                    if self.valid_queue:\n",
    "                        best_arch.eval()\n",
    "                        for i, (input_valid, target_valid) in enumerate(\n",
    "                            self.valid_queue\n",
    "                        ):\n",
    "\n",
    "                            input_valid = input_valid.to(self.device).float()\n",
    "                            target_valid = target_valid.to(self.device).float()\n",
    "\n",
    "                            # just log the validation accuracy\n",
    "                            with torch.no_grad():\n",
    "                                logits_valid = best_arch(input_valid)\n",
    "                                self._store_accuracies(\n",
    "                                    logits_valid, target_valid, \"val\"\n",
    "                                )\n",
    "\n",
    "                    scheduler.step()\n",
    "                    self.periodic_checkpointer.step(e)\n",
    "                    self._log_and_reset_accuracies(e)\n",
    "\n",
    "            # Disable drop path\n",
    "            best_arch.update_edges(\n",
    "                update_func=lambda edge: edge.data.set(\n",
    "                    \"op\", edge.data.op.get_embedded_ops()\n",
    "                ),\n",
    "                scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "            # measure final test accuracy\n",
    "            top1 = utils.AverageMeter()\n",
    "            top5 = utils.AverageMeter()\n",
    "\n",
    "            best_arch.eval()\n",
    "\n",
    "            for i, data_test in enumerate(self.test_queue):\n",
    "                input_test, target_test = data_test\n",
    "                input_test = input_test.to(self.device)\n",
    "                target_test = target_test.to(self.device, non_blocking=True)\n",
    "\n",
    "                n = input_test.size(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = best_arch(input_test)\n",
    "\n",
    "                    prec1 = utils.accuracy(logits, target_test, topk=(1,))\n",
    "                    top1.update(prec1.data.item(), n)\n",
    "\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Inference batch {} of {}.\".format(i, len(self.test_queue)),\n",
    "                    n=5,\n",
    "                )\n",
    "\n",
    "            logger.info(\n",
    "                \"Evaluation finished. Test accuracies: top-1 = {:.5}\".format(\n",
    "                    top1.avg\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return top1.avg\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_search_dataloaders(config):\n",
    "        return train_queue, valid_queue, _\n",
    "     \n",
    "    @staticmethod\n",
    "    def build_eval_dataloaders(config):\n",
    "        return train_queue, valid_queue, test_queue\n",
    "    \n",
    "class DropPathWrapper(AbstractPrimitive):\n",
    "    \"\"\"\n",
    "    A wrapper for the drop path training regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        super().__init__(locals())\n",
    "        self.op = op\n",
    "        self.device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, x, edge_data):\n",
    "        x = self.op(x, edge_data)\n",
    "        if (\n",
    "            edge_data.drop_path_prob > 0.0\n",
    "            and not isinstance(self.op, Identity)\n",
    "            and self.training\n",
    "        ):\n",
    "            keep_prob = 1.0 - edge_data.drop_path_prob\n",
    "            mask = torch.FloatTensor(x.size(0), 1).bernoulli_(keep_prob)\n",
    "            mask = mask.to(self.device)\n",
    "            x.div_(keep_prob)\n",
    "            x.mul_(mask)\n",
    "        return x\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return self.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d509cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 9\n"
     ]
    }
   ],
   "source": [
    "name = 'otto'\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "NUM_FEATURES = ds_train.num_features\n",
    "NUM_CLASSES = ds_train.num_classes\n",
    "print(NUM_FEATURES, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446b7d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e96b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OP_NAMES = ['linear', 'zero', 'resblock', 'linear_bottleneck']\n",
    "class Head(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters, num_classes):\n",
    "        super().__init__(locals())\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=filters, out_features=num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        output = self.layers[0](x)\n",
    "        output = nn.functional.relu(output)\n",
    "        return output\n",
    "    \n",
    "class TransformerOP(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters):\n",
    "        super().__init__(locals())\n",
    "        self.bn1 = nn.BatchNorm1d(filters)\n",
    "        self.bn2 = nn.BatchNorm1d(filters)\n",
    "        self.mha = nn.MultiheadAttention(filters, filters // 4, batch_first=True)\n",
    "        self.ff = nn.Linear(filters, filters)\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn1(x)\n",
    "        out, _ = self.mha(x, x, x, need_weights=False)\n",
    "        out = x + out \n",
    "        \n",
    "        out2 = self.bn2(out)\n",
    "        out2 = self.ff(out2)\n",
    "        return out2 + out\n",
    "    \n",
    "\n",
    "class LinearOP(ASRPrimitive):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0, name='Linear'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear})'\n",
    "\n",
    "class LinearBottleneckOP(ASRPrimitive):\n",
    "    def __init__(self, filters, filters_wide, dropout_rate=0, name='Linear'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "\n",
    "        self.linear1 = nn.Linear(filters, filters_wide)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.dropout1 = nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.linear2 = nn.Linear(filters_wide, filters)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.dropout2 = nn.Dropout(p=DROPOUT_RATE)\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear2})'\n",
    "    \n",
    "class ResBlockLinear(ASRPrimitive):\n",
    "    def __init__(self, features, dropout_rate=0, name='resblock'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(features)\n",
    "        self.linear1 = nn.Linear(features, features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.dropout2 = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "        self.linear2 = nn.Linear(features, features)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = torch.clamp_max_(out, 20)\n",
    "\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        return x + out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear2})'\n",
    "\n",
    "class TabNasSearchSpace(Graph):\n",
    "    \"\"\"\n",
    "    Contains the interface to the tabular benchmark of nas-bench-asr.\n",
    "    Note: currently we do not support building a naslib object for\n",
    "    nas-bench-asr architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    QUERYABLE = True\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'cells_stage_1',\n",
    "        'cells_stage_2',\n",
    "        'cells_stage_3',\n",
    "        'cells_stage_4'\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.load_labeled = False\n",
    "        self.max_epoch = 100\n",
    "        self.max_nodes = 3\n",
    "        self.accs = None\n",
    "        self.compact = None\n",
    "\n",
    "        self.n_blocks = NUM_BLOCKS\n",
    "        self.n_cells_per_block = [1] * NUM_BLOCKS\n",
    "        self.features = NUM_FEATURES\n",
    "        self.filters = [NUM_FILTERS] * NUM_BLOCKS\n",
    "\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.dropout_rate = 0.0\n",
    "        self.use_norm = True\n",
    "\n",
    "        self._create_macro_graph()\n",
    "\n",
    "    def _create_macro_graph(self):\n",
    "        cell = self._create_cell()\n",
    "\n",
    "        # Macrograph defintion\n",
    "        n_nodes = self.n_blocks + 2\n",
    "        self.add_nodes_from(range(1, n_nodes + 1))\n",
    "\n",
    "        for node in range(1, n_nodes):\n",
    "            self.add_edge(node, node + 1)\n",
    "\n",
    "        # Create the cell blocks and add them as subgraphs of nodes 2 ... 5\n",
    "        for idx, node in enumerate(range(2, 2 + self.n_blocks)):\n",
    "            scope = f'cells_stage_{idx + 1}'\n",
    "            cells_block = self._create_cells_block(cell, n=self.n_cells_per_block[idx], scope=scope)\n",
    "            self.nodes[node]['subgraph'] = cells_block.set_input([node - 1])\n",
    "\n",
    "            # Assign the list of operations to the cell edges\n",
    "            cells_block.update_edges(\n",
    "                update_func=lambda edge: _set_cell_edge_ops(edge, filters=self.filters[idx], use_norm=self.use_norm),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            )\n",
    "\n",
    "        start_node = 1\n",
    "        for idx, node in enumerate(range(start_node, start_node + self.n_blocks)):\n",
    "            if node == start_node:\n",
    "                op = LinearOP(self.features, self.filters[0])\n",
    "            else:\n",
    "                op = core_ops.Identity()\n",
    "\n",
    "            self.edges[node, node + 1].set('op', op)\n",
    "        # Assign the LSTM + Linear layer to the last edge in the macro graph\n",
    "        self.edges[self.n_blocks + 1, self.n_blocks + 2].set('op', Head(self.filters[0], self.num_classes))\n",
    "\n",
    "    def _create_cells_block(self, cell, n, scope):\n",
    "        block = Graph()\n",
    "        block.name = f'{n}_cells_block'\n",
    "\n",
    "        block.add_nodes_from(range(1, n + 2))\n",
    "\n",
    "        for node in range(2, n + 2):\n",
    "            block.add_node(node, subgraph=cell.copy().set_scope(scope).set_input([node - 1]))\n",
    "\n",
    "        for node in range(1, n + 2):\n",
    "            block.add_edge(node, node + 1)\n",
    "\n",
    "        return block\n",
    "\n",
    "    def _create_cell(self):\n",
    "        cell = Graph()\n",
    "        cell.name = 'cell'\n",
    "\n",
    "        cell.add_nodes_from(range(1, 8))\n",
    "\n",
    "        # Create edges\n",
    "        for i in range(1, 7):\n",
    "            cell.add_edge(i, i + 1)\n",
    "\n",
    "        for i in range(1, 6, 2):\n",
    "            for j in range(i + 2, 8, 2):\n",
    "                cell.add_edge(i, j)\n",
    "\n",
    "        cell.add_node(8)\n",
    "        cell.add_edge(7, 8)  # For optional layer normalization\n",
    "\n",
    "        return cell\n",
    "\n",
    "    def query(self, metric=None, dataset=None, path=None, epoch=-1,\n",
    "              full_lc=False, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Query results from nas-bench-asr\n",
    "        \"\"\"\n",
    "        metric_to_asr = {\n",
    "            Metric.VAL_ACCURACY: \"val_per\",\n",
    "            Metric.TEST_ACCURACY: \"test_per\",\n",
    "            Metric.PARAMETERS: \"params\",\n",
    "            Metric.FLOPS: \"flops\",\n",
    "        }\n",
    "\n",
    "        assert self.compact is not None\n",
    "        assert metric in [\n",
    "            Metric.TRAIN_ACCURACY,\n",
    "            Metric.TRAIN_LOSS,\n",
    "            Metric.VAL_ACCURACY,\n",
    "            Metric.TEST_ACCURACY,\n",
    "            Metric.PARAMETERS,\n",
    "            Metric.FLOPS,\n",
    "            Metric.TRAIN_TIME,\n",
    "            Metric.RAW,\n",
    "        ]\n",
    "        query_results = dataset_api[\"asr_data\"].full_info(self.compact)\n",
    "\n",
    "        if metric != Metric.VAL_ACCURACY:\n",
    "            if metric == Metric.TEST_ACCURACY:\n",
    "                return query_results[metric_to_asr[metric]]\n",
    "            elif (metric == Metric.PARAMETERS) or (metric == Metric.FLOPS):\n",
    "                return query_results['info'][metric_to_asr[metric]]\n",
    "            elif metric in [Metric.TRAIN_ACCURACY, Metric.TRAIN_LOSS,\n",
    "                            Metric.TRAIN_TIME, Metric.RAW]:\n",
    "                return -1\n",
    "        else:\n",
    "            if full_lc and epoch == -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]]\n",
    "                ]\n",
    "            elif full_lc and epoch != -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]][:epoch]\n",
    "                ]\n",
    "            else:\n",
    "                # return the value of the metric only at the specified epoch\n",
    "                return float(query_results[metric_to_asr[metric]][epoch])\n",
    "\n",
    "    def get_compact(self):\n",
    "        assert self.compact is not None\n",
    "        return self.compact\n",
    "\n",
    "    def get_hash(self):\n",
    "        return self.get_compact()\n",
    "\n",
    "    def set_compact(self, compact):\n",
    "        self.compact = make_compact_immutable(compact)\n",
    "\n",
    "    def sample_random_architecture(self, dataset_api):\n",
    "        search_space = [[len(OP_NAMES)] + [2] * (idx + 1) for idx in\n",
    "                        range(self.max_nodes)]\n",
    "        flat = flatten(search_space)\n",
    "        m = [random.randrange(opts) for opts in flat]\n",
    "        m = copy_structure(m, search_space)\n",
    "\n",
    "        compact = m\n",
    "        self.set_compact(compact)\n",
    "        return compact\n",
    "\n",
    "    def mutate(self, parent, mutation_rate=1, dataset_api=None):\n",
    "        \"\"\"\n",
    "        This will mutate the cell in one of two ways:\n",
    "        change an edge; change an op.\n",
    "        Todo: mutate by adding/removing nodes.\n",
    "        Todo: mutate the list of hidden nodes.\n",
    "        Todo: edges between initial hidden nodes are not mutated.\n",
    "        \"\"\"\n",
    "        parent_compact = parent.get_compact()\n",
    "        parent_compact = make_compact_mutable(parent_compact)\n",
    "        compact = copy.deepcopy(parent_compact)\n",
    "\n",
    "        for _ in range(int(mutation_rate)):\n",
    "            mutation_type = np.random.choice([2])\n",
    "\n",
    "            if mutation_type == 1:\n",
    "                # change an edge\n",
    "                # first pick up a node\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                # pick up an edge id\n",
    "                edge_id = np.random.choice(len(node[1:])) + 1\n",
    "                # edge ops are in [identity, zero] ([0, 1])\n",
    "                new_edge_op = int(not compact[node_id][edge_id])\n",
    "                # apply the mutation\n",
    "                compact[node_id][edge_id] = new_edge_op\n",
    "\n",
    "            elif mutation_type == 2:\n",
    "                # change an op\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                op_id = node[0]\n",
    "                list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                list_of_ops_ids.remove(op_id)\n",
    "                new_op_id = random.choice(list_of_ops_ids)\n",
    "                compact[node_id][0] = new_op_id\n",
    "\n",
    "        self.set_compact(compact)\n",
    "\n",
    "    def get_nbhd(self, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Return all neighbors of the architecture\n",
    "        \"\"\"\n",
    "        compact = self.get_compact()\n",
    "        # edges, ops, hiddens = compact\n",
    "        nbhd = []\n",
    "\n",
    "        def add_to_nbhd(new_compact, nbhd):\n",
    "            print(new_compact)\n",
    "            nbr = NasBenchASRSearchSpace()\n",
    "            nbr.set_compact(new_compact)\n",
    "            nbr_model = torch.nn.Module()\n",
    "            nbr_model.arch = nbr\n",
    "            nbhd.append(nbr_model)\n",
    "            return nbhd\n",
    "\n",
    "        for node_id in range(len(compact)):\n",
    "            node = compact[node_id]\n",
    "            for edge_id in range(len(node)):\n",
    "                if edge_id == 0:\n",
    "                    edge_op = compact[node_id][0]\n",
    "                    list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                    list_of_ops_ids.remove(edge_op)\n",
    "                    for op_id in list_of_ops_ids:\n",
    "                        new_compact = copy.deepcopy(compact)\n",
    "                        new_compact = make_compact_mutable(new_compact)\n",
    "                        new_compact[node_id][0] = op_id\n",
    "                        nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "                else:\n",
    "                    edge_op = compact[node_id][edge_id]\n",
    "                    new_edge_op = int(not edge_op)\n",
    "                    new_compact = copy.deepcopy(compact)\n",
    "                    new_compact = make_compact_mutable(new_compact)\n",
    "                    new_compact[node_id][edge_id] = new_edge_op\n",
    "                    nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "\n",
    "        random.shuffle(nbhd)\n",
    "        return nbhd\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'asr'\n",
    "\n",
    "    def get_max_epochs(self):\n",
    "        return 39\n",
    "\n",
    "    def encode(self, encoding_type=EncodingType.ADJACENCY_ONE_HOT):\n",
    "        return encode_asr(self, encoding_type=encoding_type)\n",
    "\n",
    "\n",
    "def _set_cell_edge_ops(edge, filters, use_norm):\n",
    "    if use_norm and edge.head == 7:\n",
    "        edge.data.set('op', core_ops.Identity())\n",
    "        edge.data.finalize()\n",
    "    elif edge.head % 2 == 0:  # Edge from intermediate node\n",
    "        edge.data.set(\n",
    "            'op', [\n",
    "                LinearOP(filters, filters),\n",
    "                ops['zero'](filters, filters),\n",
    "                ResBlockLinear(filters),\n",
    "                LinearBottleneckOP(filters, filters * 2),\n",
    "            ]\n",
    "        )\n",
    "    elif edge.tail % 2 == 0:  # Edge to intermediate node. Should always be Identity.\n",
    "        edge.data.finalize()\n",
    "    else:\n",
    "        edge.data.set(\n",
    "            'op',\n",
    "            [\n",
    "                core_ops.Zero(stride=1),\n",
    "                core_ops.Identity()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2f4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 2\n",
      "[05/28 00:20:54 nl.defaults.trainer]: Beginning search\n",
      "[05/28 00:20:54 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.002826, +0.000328, 0\n",
      "+0.000483, -0.001804, 0\n",
      "+0.000851, +0.000043, 0\n",
      "+0.001851, +0.000371, +0.000160, -0.000492, 0\n",
      "+0.000204, -0.000484, 0\n",
      "+0.000801, +0.000750, 0\n",
      "-0.000643, -0.000732, +0.000894, +0.000387, 2\n",
      "+0.000060, +0.001411, 1\n",
      "-0.000180, -0.000453, -0.000440, +0.000443, 3\n",
      "[05/28 00:20:54 nl.defaults.trainer]: Epoch 0-0, Train loss: 0.71760, validation loss: 0.71858, learning rate: [0.025]\n",
      "[05/28 00:20:54 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    3396 KB |   70354 KB |    1048 MB |    1045 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3396 KB |   70354 KB |    1048 MB |    1045 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    3396 KB |   70354 KB |    1048 MB |    1045 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3396 KB |   70354 KB |    1048 MB |    1045 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12988 KB |   15944 KB |    1225 MB |    1212 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |   12988 KB |   15944 KB |    1225 MB |    1212 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1048    |    1185    |    8076    |    7028    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1048    |    1185    |    8076    |    7028    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1048    |    1185    |    8076    |    7028    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1048    |    1185    |    8076    |    7028    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      35    |      35    |      35    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      35    |      35    |      35    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      41    |    4847    |    4832    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      15    |      41    |    4847    |    4832    |\n",
      "|===========================================================================|\n",
      "\n",
      "[05/28 00:20:54 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    3400 KB |   70775 KB |    1571 MB |    1568 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3400 KB |   70775 KB |    1571 MB |    1568 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    3400 KB |   70775 KB |    1571 MB |    1568 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3400 KB |   70775 KB |    1571 MB |    1568 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12983 KB |   15944 KB |    1835 MB |    1822 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |   12983 KB |   15944 KB |    1835 MB |    1822 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1057    |    1346    |   11765    |   10708    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1057    |    1346    |   11765    |   10708    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1057    |    1346    |   11765    |   10708    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1057    |    1346    |   11765    |   10708    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      35    |      35    |      35    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      35    |      35    |      35    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      41    |    7394    |    7357    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      37    |      41    |    7394    |    7357    |\n",
      "|===========================================================================|\n",
      "\n",
      "[05/28 00:20:54 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    3400 KB |   70775 KB |    2095 MB |    2092 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3400 KB |   70775 KB |    2095 MB |    2092 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    3400 KB |   70775 KB |    2095 MB |    2092 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |    3400 KB |   70775 KB |    2095 MB |    2092 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   71680 KB |   71680 KB |   71680 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12983 KB |   15944 KB |    2445 MB |    2432 MB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 MB |       0 MB |\n",
      "|       from small pool |   12983 KB |   15944 KB |    2445 MB |    2432 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1057    |    1346    |   15454    |   14397    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1057    |    1346    |   15454    |   14397    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1057    |    1346    |   15454    |   14397    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |    1057    |    1346    |   15454    |   14397    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      35    |      35    |      35    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      35    |      35    |      35    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      41    |    9910    |    9874    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      36    |      41    |    9910    |    9874    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/28 00:20:57 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0013, -0.0026], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0018,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0014], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0001,  0.0002, -0.0001,  0.0007], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "[05/28 00:20:57 nl.defaults.trainer]: Epoch 0 done. Train accuracy: 47.47410, Validation accuracy: 47.33887\n",
      "[05/28 00:20:57 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.003594, -0.000444, 0\n",
      "+0.001254, -0.002573, 0\n",
      "+0.001605, -0.000712, 0\n",
      "+0.001807, +0.001346, -0.000848, +0.000415, 0\n",
      "+0.001072, -0.001352, 0\n",
      "+0.001695, -0.000146, 0\n",
      "-0.000101, +0.000235, -0.000139, +0.000726, 3\n",
      "+0.000929, +0.000541, 0\n",
      "-0.000541, +0.000205, -0.001030, +0.001210, 3\n",
      "[05/28 00:20:59 nl.defaults.trainer]: Epoch 1-14, Train loss: 0.69315, validation loss: 0.69315, learning rate: [0.022622261579968096]\n",
      "[05/28 00:21:01 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0012, -0.0026], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0018,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0013], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-9.9549e-05,  2.3371e-04, -1.3800e-04,  7.1601e-04], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "[05/28 00:21:01 nl.defaults.trainer]: Epoch 1 done. Train accuracy: 47.14475, Validation accuracy: 47.22900\n",
      "[05/28 00:21:01 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.003587, -0.000443, 0\n",
      "+0.001250, -0.002565, 0\n",
      "+0.001601, -0.000710, 0\n",
      "+0.001772, +0.001341, -0.000846, +0.000412, 0\n",
      "+0.001069, -0.001348, 0\n",
      "+0.001691, -0.000146, 0\n",
      "-0.000100, +0.000234, -0.000138, +0.000716, 3\n",
      "+0.000927, +0.000540, 0\n",
      "-0.000534, +0.000204, -0.001028, +0.001205, 3\n",
      "[05/28 00:21:04 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0012, -0.0026], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0013], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-9.7585e-05,  2.3176e-04, -1.3721e-04,  7.0293e-04], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "[05/28 00:21:04 nl.defaults.trainer]: Epoch 2 done. Train accuracy: 47.14652, Validation accuracy: 46.96568\n",
      "[05/28 00:21:04 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.003577, -0.000442, 0\n",
      "+0.001245, -0.002554, 0\n",
      "+0.001595, -0.000708, 0\n",
      "+0.001726, +0.001334, -0.000843, +0.000409, 0\n",
      "+0.001065, -0.001343, 0\n",
      "+0.001685, -0.000145, 0\n",
      "-0.000098, +0.000232, -0.000137, +0.000703, 3\n",
      "+0.000925, +0.000538, 0\n",
      "-0.000525, +0.000203, -0.001025, +0.001198, 3\n",
      "[05/28 00:21:04 nl.defaults.trainer]: Epoch 3-0, Train loss: 0.69315, validation loss: 0.69315, learning rate: [0.008702738420031903]\n",
      "[05/28 00:21:07 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0012, -0.0025], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0013], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-9.5296e-05,  2.2947e-04, -1.3626e-04,  6.8766e-04], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "[05/28 00:21:08 nl.defaults.trainer]: Epoch 3 done. Train accuracy: 47.14653, Validation accuracy: 46.64307\n",
      "[05/28 00:21:08 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.003565, -0.000440, 0\n",
      "+0.001239, -0.002542, 0\n",
      "+0.001589, -0.000705, 0\n",
      "+0.001674, +0.001326, -0.000840, +0.000404, 0\n",
      "+0.001060, -0.001337, 0\n",
      "+0.001679, -0.000145, 0\n",
      "-0.000095, +0.000229, -0.000136, +0.000688, 3\n",
      "+0.000923, +0.000537, 0\n",
      "-0.000515, +0.000202, -0.001022, +0.001190, 3\n",
      "[05/28 00:21:09 nl.defaults.trainer]: Epoch 4-14, Train loss: 0.69315, validation loss: 0.69315, learning rate: [0.002477738420031905]\n",
      "[05/28 00:21:11 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0012, -0.0025], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0013], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-9.2747e-05,  2.2687e-04, -1.3520e-04,  6.7063e-04], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "[05/28 00:21:11 nl.defaults.trainer]: Epoch 4 done. Train accuracy: 47.14652, Validation accuracy: 46.86628\n",
      "[05/28 00:21:11 nl.defaults.trainer]: Saving architectural weight tensors: /home/experiments/arch_weights.pt\n",
      "[05/28 00:21:11 nl.defaults.trainer]: Training finished\n",
      "[05/28 00:21:11 naslib]: Start evaluation\n",
      "[05/28 00:21:11 nl.defaults.trainer]: loading model from file /home/experiments/search/model_final.pth\n",
      "[05/28 00:21:11 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0036, -0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0012, -0.0025], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016, -0.0007], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0016,  0.0013, -0.0008,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0011, -0.0013], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0017, -0.0001], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-9.2747e-05,  2.2687e-04, -1.3520e-04,  6.7063e-04], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0009, 0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0005,  0.0002, -0.0010,  0.0012], device='cuda:0',\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/28 00:21:11 naslib]: Starting retraining from scratch\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DEVICE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13602/3904077107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNasTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Search for an architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Evaluate the best architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13602/3721028640.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, retrain, search_model, resume_from, best_arch, dataset_api, metric)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     ),\n\u001b[1;32m     91\u001b[0m                     \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_arch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER_SCOPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mprivate_edge_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naslib/naslib/search_spaces/core/graph.py\u001b[0m in \u001b[0;36mupdate_edges\u001b[0;34m(self, update_func, scope, private_edge_data)\u001b[0m\n\u001b[1;32m    720\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                         \u001b[0medge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttrDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                         \u001b[0mupdate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delete_flagged_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13602/3721028640.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 best_arch.update_edges(\n\u001b[1;32m     88\u001b[0m                     update_func=lambda edge: edge.data.set(\n\u001b[0;32m---> 89\u001b[0;31m                         \u001b[0;34m\"op\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropPathWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     ),\n\u001b[1;32m     91\u001b[0m                     \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_arch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER_SCOPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13602/3721028640.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEVICE' is not defined"
     ]
    }
   ],
   "source": [
    "from naslib.optimizers import DARTSOptimizer #+\n",
    "from naslib.optimizers import GSparseOptimizer\n",
    "from naslib.optimizers import OneShotNASOptimizer #+\n",
    "from naslib.optimizers import GDASOptimizer #+\n",
    "from naslib.optimizers import DrNASOptimizer #+\n",
    "from naslib.optimizers import RandomNASOptimizer\n",
    "\n",
    "from naslib.optimizers import RandomSearch\n",
    "from naslib.optimizers import RegularizedEvolution\n",
    "from naslib.optimizers import LocalSearch\n",
    "from naslib.optimizers import Bananas\n",
    "from naslib.optimizers import BasePredictor\n",
    "from naslib.optimizers import Npenas\n",
    "name = 'higgs-small'\n",
    "DROPOUT_RATE = 0.\n",
    "NUM_FILTERS = 32\n",
    "NUM_BLOCKS = 4\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "NUM_FEATURES = ds_train.num_features\n",
    "NUM_CLASSES = ds_train.num_classes\n",
    "\n",
    "print(NUM_FEATURES, NUM_CLASSES)\n",
    "dataset = TabNasDataset(config, ds_train, ds_test)\n",
    "\n",
    "train_queue, valid_queue, test_queue, train_transform, valid_transform = dataset.get_loaders()\n",
    "    \n",
    "search_space = TabNasSearchSpace()\n",
    "logger = setup_logger(config.save + \"/darts\")\n",
    "logger.setLevel(logging.INFO)  \n",
    "\n",
    "optimizer = DARTSOptimizer(**config)\n",
    "optimizer.adapt_search_space(search_space, config.dataset)\n",
    "\n",
    "\n",
    "trainer = TabNasTrainer(optimizer, config)\n",
    "trainer.search()  # Search for an architecture\n",
    "trainer.evaluate()  # Evaluate the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae39aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /home/experiments/eval/*\n",
    ","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43937654",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /home/experiments/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105c7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6269eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/experiments/metrics.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d27fd4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'darts_covtype': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 36.46},\n",
       " 'darts_higgs-small': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 47.144},\n",
       " 'darts_otto': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 3.1189},\n",
       " 'darts_adult': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 76.377},\n",
       " 'darts_churn': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 76.35},\n",
       " 'gdas_covtype': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 61.111},\n",
       " 'gdas_higgs-small': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 52.856},\n",
       " 'gdas_otto': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 3.1189},\n",
       " 'gdas_adult': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 76.377},\n",
       " 'gdas_churn': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 79.65},\n",
       " 'drnaso_covtype': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 36.46},\n",
       " 'drnaso_higgs-small': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 52.856},\n",
       " 'drnaso_otto': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 26.05},\n",
       " 'drnaso_adult': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 76.377},\n",
       " 'drnaso_churn': {'train_acc': [],\n",
       "  'val_acc': [],\n",
       "  'train_loss': [],\n",
       "  'val_loss': [],\n",
       "  'test_acc': 79.65}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4a398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e61f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
