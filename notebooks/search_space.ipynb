{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7fa0db-3f3a-4a77-80f0-30b154d61b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.common.config import CfgNode\n",
    "\n",
    "from naslib.search_spaces import SimpleCellSearchSpace\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.utils.custom_dataset import CustomDataset\n",
    "from naslib.optimizers import DARTSOptimizer, GDASOptimizer, RandomSearch\n",
    "from naslib.search_spaces import NasBench301SearchSpace, SimpleCellSearchSpace\n",
    "from naslib.utils import set_seed, setup_logger, get_config_from_args\n",
    "from naslib.search_spaces.core.query_metrics import Metric\n",
    "from naslib.search_spaces.core.graph import Graph\n",
    "from naslib.search_spaces.nasbenchasr.primitives import ASRPrimitive, CellLayerNorm, Head, ops, PadConvReluNorm\n",
    "from naslib.utils import get_project_root\n",
    "from naslib.search_spaces.core import primitives as core_ops\n",
    "from naslib.search_spaces.nasbenchasr.conversions import flatten, \\\n",
    "    copy_structure, make_compact_mutable, make_compact_immutable\n",
    "from naslib.search_spaces.nasbenchasr.encodings import encode_asr\n",
    "from naslib.utils.encodings import EncodingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Path('/home/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632db49f-1d96-485f-95f8-30f2bc016128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_closest_square_shape(x):\n",
    "    len_x = len(x)\n",
    "    closest_square = int(np.round(np.sqrt(len_x)))\n",
    "    if closest_square ** 2 < len_x: \n",
    "        closest_square += 1\n",
    "    x = np.pad(x, (0, closest_square  ** 2 - len_x))\n",
    "    return x.reshape((1, closest_square, closest_square))\n",
    "\n",
    "class NasDataset(Dataset):\n",
    "    def __init__(self, root_dir, name, queue, norm='min_max'):\n",
    "        super().__init__()\n",
    "        \n",
    "        bin_exists = False\n",
    "        num_exists = False\n",
    "        \n",
    "        assert queue in ['train', 'test', 'val']\n",
    "\n",
    "        self.root_dir = root_dir \n",
    "        self.name = name\n",
    "        self.type = queue\n",
    "\n",
    "        x_num = f'X_num_{queue}.npy'\n",
    "        x_bin = f'X_bin_{queue}.npy'\n",
    "        y = f'Y_{queue}.npy'\n",
    "        \n",
    "        \n",
    "        if (root_dir / name/ x_bin).exists():\n",
    "            bin_exists = True\n",
    "            \n",
    "        if (root_dir / name/ x_num).exists():\n",
    "            num_exists = True\n",
    "        \n",
    "        self.y = np.load(root_dir / name / y)\n",
    "        \n",
    "        if num_exists:\n",
    "            x_num = np.load(root_dir / name / x_num)\n",
    "            x_num = (x_num - np.min(x_num, axis=0)) / (np.max(x_num, axis=0) - np.min(x_num, axis=0))\n",
    "        \n",
    "        if bin_exists:\n",
    "            x_bin = np.load(root_dir / name / x_bin)\n",
    "        \n",
    "        if num_exists and bin_exists:\n",
    "            self.x = np.concatenate((x_num, x_bin), axis=1)\n",
    "        elif num_exists:\n",
    "            self.x = x_num\n",
    "        elif bin_exists:\n",
    "            self.x = x_bin\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.num_features = self.x.shape[1]\n",
    "        self.num_classes = int(np.max(self.y)) + 1\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i]\n",
    "        y = self.y[i]\n",
    "\n",
    "        return  x, y.astype(np.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33258b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/table_nas/config.yaml') as f:\n",
    "    config = CfgNode.load_cfg(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e11a04-8a8a-4602-a84c-7114ef765608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasDataset(CustomDataset):\n",
    "    def __init__(self, config, ds_train, ds_test, mode='train'):\n",
    "        super().__init__(config, mode)\n",
    "        self.ds_train = ds_train\n",
    "        self.ds_test = ds_test\n",
    "\n",
    "    def get_transforms(self, config):\n",
    "        return Compose([ToTensor()]), Compose([ToTensor()])\n",
    "\n",
    "\n",
    "    def get_data(self, data, train_transform, valid_transform):\n",
    "        train_data = self.ds_train\n",
    "        test_data = self.ds_test\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35850045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NasTrainer(Trainer):\n",
    "    @staticmethod\n",
    "    def build_search_dataloaders(config):\n",
    "        return train_queue, valid_queue, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e96b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OP_NAMES = ['linear', 'zero']\n",
    "class Head(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters, num_classes):\n",
    "        super().__init__(locals())\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=filters, out_features=num_classes+1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        output = self.layers[0](x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class FFN(ASRPrimitive):\n",
    "        def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            d_token: int,\n",
    "            d_hidden: int,\n",
    "            bias_first: bool,\n",
    "            bias_second: bool,\n",
    "            dropout: float,\n",
    "            activation: ModuleType,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.linear_first = nn.Linear(\n",
    "                d_token,\n",
    "                d_hidden * (2 if _is_glu_activation(activation) else 1),\n",
    "                bias_first,\n",
    "            )\n",
    "            self.activation = _make_nn_module(activation)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.linear_second = nn.Linear(d_hidden, d_token, bias_second)\n",
    "\n",
    "        def forward(self, x: Tensor, edge_data=None) -> Tensor:\n",
    "            x = self.linear_first(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear_second(x)\n",
    "            return x\n",
    "    \n",
    "\n",
    "class LinearOP(ASRPrimitive):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0, name='Linear'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear})'\n",
    "    \n",
    "\n",
    "class NasTabSearchSpace(Graph):\n",
    "    \"\"\"\n",
    "    Contains the interface to the tabular benchmark of nas-bench-asr.\n",
    "    Note: currently we do not support building a naslib object for\n",
    "    nas-bench-asr architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    QUERYABLE = True\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'cells_stage_1',\n",
    "        'cells_stage_2',\n",
    "        'cells_stage_3',\n",
    "        'cells_stage_4'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.load_labeled = False\n",
    "        self.max_epoch = 40\n",
    "        self.max_nodes = 3\n",
    "        self.accs = None\n",
    "        self.compact = None\n",
    "\n",
    "        self.n_blocks = 4\n",
    "        self.n_cells_per_block = [3, 4, 5, 6]\n",
    "        self.features = num_features\n",
    "        self.filters = [600, 600, 600, 600]\n",
    "        self.cnn_time_reduction_kernels = [8, 8, 8, 8]\n",
    "        self.cnn_time_reduction_strides = [1, 1, 2, 2]\n",
    "        self.scells_per_block = [3, 4, 5, 6]\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = 0.0\n",
    "        self.use_norm = True\n",
    "\n",
    "        self._create_macro_graph()\n",
    "\n",
    "    def _create_macro_graph(self):\n",
    "        cell = self._create_cell()\n",
    "\n",
    "        # Macrograph defintion\n",
    "        n_nodes = self.n_blocks + 2\n",
    "        self.add_nodes_from(range(1, n_nodes + 1))\n",
    "\n",
    "        for node in range(1, n_nodes):\n",
    "            self.add_edge(node, node + 1)\n",
    "\n",
    "        # Create the cell blocks and add them as subgraphs of nodes 2 ... 5\n",
    "        for idx, node in enumerate(range(2, 2 + self.n_blocks)):\n",
    "            scope = f'cells_stage_{idx + 1}'\n",
    "            cells_block = self._create_cells_block(cell, n=self.n_cells_per_block[idx], scope=scope)\n",
    "            self.nodes[node]['subgraph'] = cells_block.set_input([node - 1])\n",
    "\n",
    "            # Assign the list of operations to the cell edges\n",
    "            cells_block.update_edges(\n",
    "                update_func=lambda edge: _set_cell_edge_ops(edge, filters=self.filters[idx], use_norm=self.use_norm),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            )\n",
    "\n",
    "        start_node = 1\n",
    "        for idx, node in enumerate(range(start_node, start_node + self.n_blocks)):\n",
    "            if node == start_node:\n",
    "                op = LinearOP(self.features, 600)\n",
    "            else:\n",
    "                op = core_ops.Identity()\n",
    "\n",
    "            self.edges[node, node + 1].set('op', op)\n",
    "        # Assign the LSTM + Linear layer to the last edge in the macro graph\n",
    "        self.edges[self.n_blocks + 1, self.n_blocks + 2].set('op', Head(600, 5))\n",
    "\n",
    "    def _create_cells_block(self, cell, n, scope):\n",
    "        block = Graph()\n",
    "        block.name = f'{n}_cells_block'\n",
    "\n",
    "        block.add_nodes_from(range(1, n + 2))\n",
    "\n",
    "        for node in range(2, n + 2):\n",
    "            block.add_node(node, subgraph=cell.copy().set_scope(scope).set_input([node - 1]))\n",
    "\n",
    "        for node in range(1, n + 2):\n",
    "            block.add_edge(node, node + 1)\n",
    "\n",
    "        return block\n",
    "\n",
    "    def _create_cell(self):\n",
    "        cell = Graph()\n",
    "        cell.name = 'cell'\n",
    "\n",
    "        cell.add_nodes_from(range(1, 8))\n",
    "\n",
    "        # Create edges\n",
    "        for i in range(1, 7):\n",
    "            cell.add_edge(i, i + 1)\n",
    "\n",
    "        for i in range(1, 6, 2):\n",
    "            for j in range(i + 2, 8, 2):\n",
    "                cell.add_edge(i, j)\n",
    "\n",
    "        cell.add_node(8)\n",
    "        cell.add_edge(7, 8)  # For optional layer normalization\n",
    "\n",
    "        return cell\n",
    "\n",
    "    def query(self, metric=None, dataset=None, path=None, epoch=-1,\n",
    "              full_lc=False, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Query results from nas-bench-asr\n",
    "        \"\"\"\n",
    "        metric_to_asr = {\n",
    "            Metric.VAL_ACCURACY: \"val_per\",\n",
    "            Metric.TEST_ACCURACY: \"test_per\",\n",
    "            Metric.PARAMETERS: \"params\",\n",
    "            Metric.FLOPS: \"flops\",\n",
    "        }\n",
    "\n",
    "        assert self.compact is not None\n",
    "        assert metric in [\n",
    "            Metric.TRAIN_ACCURACY,\n",
    "            Metric.TRAIN_LOSS,\n",
    "            Metric.VAL_ACCURACY,\n",
    "            Metric.TEST_ACCURACY,\n",
    "            Metric.PARAMETERS,\n",
    "            Metric.FLOPS,\n",
    "            Metric.TRAIN_TIME,\n",
    "            Metric.RAW,\n",
    "        ]\n",
    "        query_results = dataset_api[\"asr_data\"].full_info(self.compact)\n",
    "\n",
    "        if metric != Metric.VAL_ACCURACY:\n",
    "            if metric == Metric.TEST_ACCURACY:\n",
    "                return query_results[metric_to_asr[metric]]\n",
    "            elif (metric == Metric.PARAMETERS) or (metric == Metric.FLOPS):\n",
    "                return query_results['info'][metric_to_asr[metric]]\n",
    "            elif metric in [Metric.TRAIN_ACCURACY, Metric.TRAIN_LOSS,\n",
    "                            Metric.TRAIN_TIME, Metric.RAW]:\n",
    "                return -1\n",
    "        else:\n",
    "            if full_lc and epoch == -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]]\n",
    "                ]\n",
    "            elif full_lc and epoch != -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]][:epoch]\n",
    "                ]\n",
    "            else:\n",
    "                # return the value of the metric only at the specified epoch\n",
    "                return float(query_results[metric_to_asr[metric]][epoch])\n",
    "\n",
    "    def get_compact(self):\n",
    "        assert self.compact is not None\n",
    "        return self.compact\n",
    "\n",
    "    def get_hash(self):\n",
    "        return self.get_compact()\n",
    "\n",
    "    def set_compact(self, compact):\n",
    "        self.compact = make_compact_immutable(compact)\n",
    "\n",
    "    def sample_random_architecture(self, dataset_api):\n",
    "        search_space = [[len(OP_NAMES)] + [2] * (idx + 1) for idx in\n",
    "                        range(self.max_nodes)]\n",
    "        flat = flatten(search_space)\n",
    "        m = [random.randrange(opts) for opts in flat]\n",
    "        m = copy_structure(m, search_space)\n",
    "\n",
    "        compact = m\n",
    "        self.set_compact(compact)\n",
    "        return compact\n",
    "\n",
    "    def mutate(self, parent, mutation_rate=1, dataset_api=None):\n",
    "        \"\"\"\n",
    "        This will mutate the cell in one of two ways:\n",
    "        change an edge; change an op.\n",
    "        Todo: mutate by adding/removing nodes.\n",
    "        Todo: mutate the list of hidden nodes.\n",
    "        Todo: edges between initial hidden nodes are not mutated.\n",
    "        \"\"\"\n",
    "        parent_compact = parent.get_compact()\n",
    "        parent_compact = make_compact_mutable(parent_compact)\n",
    "        compact = copy.deepcopy(parent_compact)\n",
    "\n",
    "        for _ in range(int(mutation_rate)):\n",
    "            mutation_type = np.random.choice([2])\n",
    "\n",
    "            if mutation_type == 1:\n",
    "                # change an edge\n",
    "                # first pick up a node\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                # pick up an edge id\n",
    "                edge_id = np.random.choice(len(node[1:])) + 1\n",
    "                # edge ops are in [identity, zero] ([0, 1])\n",
    "                new_edge_op = int(not compact[node_id][edge_id])\n",
    "                # apply the mutation\n",
    "                compact[node_id][edge_id] = new_edge_op\n",
    "\n",
    "            elif mutation_type == 2:\n",
    "                # change an op\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                op_id = node[0]\n",
    "                list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                list_of_ops_ids.remove(op_id)\n",
    "                new_op_id = random.choice(list_of_ops_ids)\n",
    "                compact[node_id][0] = new_op_id\n",
    "\n",
    "        self.set_compact(compact)\n",
    "\n",
    "    def get_nbhd(self, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Return all neighbors of the architecture\n",
    "        \"\"\"\n",
    "        compact = self.get_compact()\n",
    "        # edges, ops, hiddens = compact\n",
    "        nbhd = []\n",
    "\n",
    "        def add_to_nbhd(new_compact, nbhd):\n",
    "            print(new_compact)\n",
    "            nbr = NasBenchASRSearchSpace()\n",
    "            nbr.set_compact(new_compact)\n",
    "            nbr_model = torch.nn.Module()\n",
    "            nbr_model.arch = nbr\n",
    "            nbhd.append(nbr_model)\n",
    "            return nbhd\n",
    "\n",
    "        for node_id in range(len(compact)):\n",
    "            node = compact[node_id]\n",
    "            for edge_id in range(len(node)):\n",
    "                if edge_id == 0:\n",
    "                    edge_op = compact[node_id][0]\n",
    "                    list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                    list_of_ops_ids.remove(edge_op)\n",
    "                    for op_id in list_of_ops_ids:\n",
    "                        new_compact = copy.deepcopy(compact)\n",
    "                        new_compact = make_compact_mutable(new_compact)\n",
    "                        new_compact[node_id][0] = op_id\n",
    "                        nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "                else:\n",
    "                    edge_op = compact[node_id][edge_id]\n",
    "                    new_edge_op = int(not edge_op)\n",
    "                    new_compact = copy.deepcopy(compact)\n",
    "                    new_compact = make_compact_mutable(new_compact)\n",
    "                    new_compact[node_id][edge_id] = new_edge_op\n",
    "                    nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "\n",
    "        random.shuffle(nbhd)\n",
    "        return nbhd\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'asr'\n",
    "\n",
    "    def get_max_epochs(self):\n",
    "        return 39\n",
    "\n",
    "    def encode(self, encoding_type=EncodingType.ADJACENCY_ONE_HOT):\n",
    "        return encode_asr(self, encoding_type=encoding_type)\n",
    "\n",
    "\n",
    "def _set_cell_edge_ops(edge, filters, use_norm):\n",
    "    if use_norm and edge.head == 7:\n",
    "        edge.data.set('op', core_ops.Identity())\n",
    "        edge.data.finalize()\n",
    "    elif edge.head % 2 == 0:  # Edge from intermediate node\n",
    "        edge.data.set(\n",
    "            'op', [\n",
    "                LinearOP(filters, filters),\n",
    "                ops['zero'](filters, filters)\n",
    "            ]\n",
    "        )\n",
    "    elif edge.tail % 2 == 0:  # Edge to intermediate node. Should always be Identity.\n",
    "        edge.data.finalize()\n",
    "    else:\n",
    "        edge.data.set(\n",
    "            'op',\n",
    "            [\n",
    "                core_ops.Zero(stride=1),\n",
    "                core_ops.Identity()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/24 07:56:37 nl.defaults.trainer]: Beginning search\n",
      "[05/24 07:56:40 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "-0.000156, -0.000116, 1\n",
      "+0.000500, +0.001154, 1\n",
      "+0.000190, -0.000586, 0\n",
      "+0.000544, -0.000631, 0\n",
      "-0.000397, +0.002099, 1\n",
      "-0.000823, -0.001026, 0\n",
      "-0.001486, -0.001633, 0\n",
      "+0.000798, +0.000570, 0\n",
      "+0.000623, +0.000665, 1\n",
      "[05/24 07:56:40 nl.defaults.trainer]: Epoch 0-0, Train loss: 6.14617, validation loss: 6.32109, learning rate: [0.025]\n",
      "[05/24 07:56:40 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  237393 KB |  240189 KB |  516084 KB |  278691 KB |\n",
      "|       from large pool |  236532 KB |  239345 KB |  467541 KB |  231009 KB |\n",
      "|       from small pool |     861 KB |    4357 KB |   48543 KB |   47682 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  237393 KB |  240189 KB |  516084 KB |  278691 KB |\n",
      "|       from large pool |  236532 KB |  239345 KB |  467541 KB |  231009 KB |\n",
      "|       from small pool |     861 KB |    4357 KB |   48543 KB |   47682 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  251904 KB |  251904 KB |  251904 KB |       0 B  |\n",
      "|       from large pool |  245760 KB |  245760 KB |  245760 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12462 KB |   23859 KB |  584229 KB |  571767 KB |\n",
      "|       from large pool |    9228 KB |   20480 KB |  530559 KB |  521331 KB |\n",
      "|       from small pool |    3234 KB |    3643 KB |   53670 KB |   50436 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     380    |     817    |    8211    |    7831    |\n",
      "|       from large pool |     162    |     164    |     324    |     162    |\n",
      "|       from small pool |     218    |     709    |    7887    |    7669    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     380    |     817    |    8211    |    7831    |\n",
      "|       from large pool |     162    |     164    |     324    |     162    |\n",
      "|       from small pool |     218    |     709    |    7887    |    7669    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      15    |      15    |       0    |\n",
      "|       from large pool |      12    |      12    |      12    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |    5618    |    5582    |\n",
      "|       from large pool |       2    |       2    |      74    |      72    |\n",
      "|       from small pool |      34    |      44    |    5544    |    5510    |\n",
      "|===========================================================================|\n",
      "\n",
      "[05/24 07:56:40 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  237398 KB |  242501 KB |     847 MB |  630466 KB |\n",
      "|       from large pool |  236532 KB |  239345 KB |     753 MB |  534813 KB |\n",
      "|       from small pool |     866 KB |    4643 KB |      94 MB |   95653 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  237398 KB |  242501 KB |     847 MB |  630466 KB |\n",
      "|       from large pool |  236532 KB |  239345 KB |     753 MB |  534813 KB |\n",
      "|       from small pool |     866 KB |    4643 KB |      94 MB |   95653 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  251904 KB |  251904 KB |  251904 KB |       0 B  |\n",
      "|       from large pool |  245760 KB |  245760 KB |  245760 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12458 KB |   23859 KB |     918 MB |     905 MB |\n",
      "|       from large pool |    9228 KB |   20480 KB |     814 MB |     805 MB |\n",
      "|       from small pool |    3230 KB |    3643 KB |     103 MB |     100 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     389    |     942    |   16180    |   15791    |\n",
      "|       from large pool |     162    |     164    |     540    |     378    |\n",
      "|       from small pool |     227    |     780    |   15640    |   15413    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     389    |     942    |   16180    |   15791    |\n",
      "|       from large pool |     162    |     164    |     540    |     378    |\n",
      "|       from small pool |     227    |     780    |   15640    |   15413    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      15    |      15    |       0    |\n",
      "|       from large pool |      12    |      12    |      12    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |   10976    |   10939    |\n",
      "|       from large pool |       2    |       2    |     236    |     234    |\n",
      "|       from small pool |      35    |      44    |   10740    |   10705    |\n",
      "|===========================================================================|\n",
      "\n",
      "[05/24 07:56:40 nl.defaults.trainer]: cuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  237398 KB |  242501 KB |    1191 MB |     959 MB |\n",
      "|       from large pool |  236532 KB |  239345 KB |    1049 MB |     818 MB |\n",
      "|       from small pool |     866 KB |    4643 KB |     141 MB |     140 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  237398 KB |  242501 KB |    1191 MB |     959 MB |\n",
      "|       from large pool |  236532 KB |  239345 KB |    1049 MB |     818 MB |\n",
      "|       from small pool |     866 KB |    4643 KB |     141 MB |     140 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  251904 KB |  251904 KB |  251904 KB |       0 B  |\n",
      "|       from large pool |  245760 KB |  245760 KB |  245760 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   12458 KB |   23859 KB |    1265 MB |    1253 MB |\n",
      "|       from large pool |    9228 KB |   20480 KB |    1111 MB |    1102 MB |\n",
      "|       from small pool |    3230 KB |    3643 KB |     154 MB |     150 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     389    |     942    |   24149    |   23760    |\n",
      "|       from large pool |     162    |     164    |     756    |     594    |\n",
      "|       from small pool |     227    |     780    |   23393    |   23166    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     389    |     942    |   24149    |   23760    |\n",
      "|       from large pool |     162    |     164    |     756    |     594    |\n",
      "|       from small pool |     227    |     780    |   23393    |   23166    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      15    |      15    |       0    |\n",
      "|       from large pool |      12    |      12    |      12    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |   16350    |   16313    |\n",
      "|       from large pool |       2    |       2    |     398    |     396    |\n",
      "|       from small pool |      35    |      44    |   15952    |   15917    |\n",
      "|===========================================================================|\n",
      "\n",
      "[05/24 07:56:45 nl.defaults.trainer]: Epoch 0-36, Train loss: 1.06431, validation loss: 1.96069, learning rate: [0.025]\n",
      "[05/24 07:56:50 nl.defaults.trainer]: Epoch 0-77, Train loss: 0.60115, validation loss: 0.17175, learning rate: [0.025]\n",
      "[05/24 07:56:55 nl.defaults.trainer]: Epoch 0-112, Train loss: 1.01626, validation loss: 1.36133, learning rate: [0.025]\n",
      "[05/24 07:57:00 nl.defaults.trainer]: Epoch 0-140, Train loss: 0.59996, validation loss: 0.74651, learning rate: [0.025]\n",
      "[05/24 07:57:05 nl.defaults.trainer]: Epoch 0-176, Train loss: 0.53712, validation loss: 0.75541, learning rate: [0.025]\n",
      "[05/24 07:57:10 nl.defaults.trainer]: Epoch 0-217, Train loss: 0.77006, validation loss: 0.38553, learning rate: [0.025]\n",
      "[05/24 07:57:16 nl.defaults.trainer]: Epoch 0-253, Train loss: 0.68760, validation loss: 0.72177, learning rate: [0.025]\n",
      "[05/24 07:57:21 nl.defaults.trainer]: Epoch 0-285, Train loss: 0.80449, validation loss: 0.69648, learning rate: [0.025]\n",
      "[05/24 07:57:26 nl.defaults.trainer]: Epoch 0-321, Train loss: 0.83041, validation loss: 1.11539, learning rate: [0.025]\n",
      "[05/24 07:57:31 nl.defaults.trainer]: Epoch 0-363, Train loss: 0.71597, validation loss: 0.87681, learning rate: [0.025]\n",
      "[05/24 07:57:36 nl.defaults.trainer]: Epoch 0-396, Train loss: 0.63840, validation loss: 0.63646, learning rate: [0.025]\n",
      "[05/24 07:57:41 nl.defaults.trainer]: Epoch 0-423, Train loss: 0.72256, validation loss: 0.72089, learning rate: [0.025]\n",
      "[05/24 07:57:46 nl.defaults.trainer]: Epoch 0-460, Train loss: 0.55685, validation loss: 0.82650, learning rate: [0.025]\n",
      "[05/24 07:57:51 nl.defaults.trainer]: Epoch 0-501, Train loss: 0.64883, validation loss: 0.79664, learning rate: [0.025]\n",
      "[05/24 07:57:56 nl.defaults.trainer]: Epoch 0-533, Train loss: 0.69993, validation loss: 0.70018, learning rate: [0.025]\n",
      "[05/24 07:58:01 nl.defaults.trainer]: Epoch 0-562, Train loss: 0.83697, validation loss: 0.94590, learning rate: [0.025]\n"
     ]
    }
   ],
   "source": [
    "name = 'classif-cat-large-0-covertype'\n",
    "ds_train = NasDataset(datasets, name, 'train')\n",
    "ds_test = NasDataset(datasets, name, 'test')\n",
    "\n",
    "dataset = TabNasDataset(config, ds_train, ds_test)\n",
    "\n",
    "train_queue, valid_queue, test_queue, train_transform, valid_transform = dataset.get_loaders()\n",
    "    \n",
    "search_space = NasTabSearchSpace(ds_train.num_features, ds_train.num_classes)\n",
    "logger = setup_logger(config.save + \"/log.log\")\n",
    "logger.setLevel(logging.INFO)  # default DEBUG is very verbose\n",
    "\n",
    "optimizer = DARTSOptimizer(**config.search)\n",
    "optimizer.adapt_search_space(search_space, config.dataset)\n",
    "\n",
    "\n",
    "trainer = NasTrainer(optimizer, config)\n",
    "trainer.search()  # Search for an architecture\n",
    "trainer.evaluate()  # Evaluate the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e61f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
