{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7fa0db-3f3a-4a77-80f0-30b154d61b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.common.config import CfgNode\n",
    "\n",
    "from naslib import utils\n",
    "from naslib.search_spaces import SimpleCellSearchSpace\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.utils.custom_dataset import CustomDataset\n",
    "from naslib.optimizers import DARTSOptimizer, GDASOptimizer, RandomSearch\n",
    "from naslib.search_spaces import NasBench301SearchSpace, SimpleCellSearchSpace\n",
    "from naslib.utils import set_seed, setup_logger, get_config_from_args\n",
    "from naslib.search_spaces.core.query_metrics import Metric\n",
    "from naslib.search_spaces.core.graph import Graph\n",
    "from naslib.search_spaces.nasbenchasr.primitives import ASRPrimitive, CellLayerNorm, Head, ops, PadConvReluNorm\n",
    "from naslib.utils import get_project_root\n",
    "from naslib.search_spaces.core import primitives as core_ops\n",
    "from naslib.search_spaces.nasbenchasr.conversions import flatten, \\\n",
    "    copy_structure, make_compact_mutable, make_compact_immutable\n",
    "from naslib.search_spaces.nasbenchasr.encodings import encode_asr\n",
    "from naslib.utils.encodings import EncodingType\n",
    "from naslib.utils.log import log_every_n_seconds, log_first_n\n",
    "from naslib.search_spaces.core.primitives import AbstractPrimitive, Identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Path('/home/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632db49f-1d96-485f-95f8-30f2bc016128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_closest_square_shape(x):\n",
    "    len_x = len(x)\n",
    "    closest_square = int(np.round(np.sqrt(len_x)))\n",
    "    if closest_square ** 2 < len_x: \n",
    "        closest_square += 1\n",
    "    x = np.pad(x, (0, closest_square  ** 2 - len_x))\n",
    "    return x.reshape((1, closest_square, closest_square))\n",
    "\n",
    "class TabNasTorchDataset(Dataset):\n",
    "    def __init__(self, root_dir, name, queue, norm='min_max'):\n",
    "        super().__init__()\n",
    "        \n",
    "        bin_exists = False\n",
    "        num_exists = False\n",
    "        \n",
    "        assert queue in ['train', 'test', 'val']\n",
    "\n",
    "        self.root_dir = root_dir \n",
    "        self.name = name\n",
    "        self.type = queue\n",
    "\n",
    "        x_num = f'X_num_{queue}.npy'\n",
    "        x_bin = f'X_bin_{queue}.npy'\n",
    "        y = f'Y_{queue}.npy'\n",
    "        \n",
    "        \n",
    "        if (root_dir / name/ x_bin).exists():\n",
    "            bin_exists = True\n",
    "            \n",
    "        if (root_dir / name/ x_num).exists():\n",
    "            num_exists = True\n",
    "        \n",
    "        self.y = np.load(root_dir / name / y)\n",
    "        \n",
    "        if num_exists:\n",
    "            x_num = np.load(root_dir / name / x_num)\n",
    "            x_num = (x_num - np.min(x_num, axis=0)) / (np.max(x_num, axis=0) - np.min(x_num, axis=0))\n",
    "        \n",
    "        if bin_exists:\n",
    "            x_bin = np.load(root_dir / name / x_bin)\n",
    "        \n",
    "        if num_exists and bin_exists:\n",
    "            self.x = np.concatenate((x_num, x_bin), axis=1)\n",
    "        elif num_exists:\n",
    "            self.x = x_num\n",
    "        elif bin_exists:\n",
    "            self.x = x_bin\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.num_features = self.x.shape[1]\n",
    "        self.num_classes = int(np.max(self.y)) + 1\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i]\n",
    "        y = self.y[i]\n",
    "\n",
    "        return  x, y.astype(np.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33258b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/table_nas/config.yaml') as f:\n",
    "    config = CfgNode.load_cfg(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e11a04-8a8a-4602-a84c-7114ef765608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasDataset(CustomDataset):\n",
    "    def __init__(self, config, ds_train, ds_test, mode='train'):\n",
    "        super().__init__(config, mode)\n",
    "        self.ds_train = ds_train\n",
    "        self.ds_test = ds_test\n",
    "\n",
    "    def get_transforms(self, config):\n",
    "        return Compose([ToTensor()]), Compose([ToTensor()])\n",
    "\n",
    "\n",
    "    def get_data(self, data, train_transform, valid_transform):\n",
    "        train_data = self.ds_train\n",
    "        test_data = self.ds_test\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35850045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasTrainer(Trainer):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        retrain:bool=True,\n",
    "        search_model:str=\"\",\n",
    "        resume_from:str=\"\",\n",
    "        best_arch:Graph=None,\n",
    "        dataset_api:object=None,\n",
    "        metric:Metric=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the final architecture as given from the optimizer.\n",
    "\n",
    "        If the search space has an interface to a benchmark then query that.\n",
    "        Otherwise train as defined in the config.\n",
    "\n",
    "        Args:\n",
    "            retrain (bool)      : Reset the weights from the architecure search\n",
    "            search_model (str)  : Path to checkpoint file that was created during search. If not provided,\n",
    "                                  then try to load 'model_final.pth' from search\n",
    "            resume_from (str)   : Resume retraining from the given checkpoint file.\n",
    "            best_arch           : Parsed model you want to directly evaluate and ignore the final model\n",
    "                                  from the optimizer.\n",
    "            dataset_api         : Dataset API to use for querying model performance.\n",
    "            metric              : Metric to query the benchmark for.\n",
    "        \"\"\"\n",
    "        logger.info(\"Start evaluation\")\n",
    "        if not best_arch:\n",
    "\n",
    "            if not search_model:\n",
    "                search_model = os.path.join(\n",
    "                    self.config.save, \"search\", \"model_final.pth\"\n",
    "                )\n",
    "            self._setup_checkpointers(search_model)  # required to load the architecture\n",
    "\n",
    "            best_arch = self.optimizer.get_final_architecture()\n",
    "        #logger.info(f\"Final architecture hash: {best_arch.get_hash()}\")\n",
    "\n",
    "        if True:\n",
    "            best_arch.to(self.device)\n",
    "            if retrain:\n",
    "                logger.info(\"Starting retraining from scratch\")\n",
    "                best_arch.reset_weights(inplace=True)\n",
    "\n",
    "                (\n",
    "                    self.train_queue,\n",
    "                    self.valid_queue,\n",
    "                    self.test_queue,\n",
    "                ) = self.build_eval_dataloaders(self.config)\n",
    "\n",
    "                optim = self.build_eval_optimizer(best_arch.parameters(), self.config)\n",
    "                scheduler = self.build_eval_scheduler(optim, self.config)\n",
    "\n",
    "                start_epoch = self._setup_checkpointers(\n",
    "                    resume_from,\n",
    "                    search=False,\n",
    "                    period=self.config.evaluation.checkpoint_freq,\n",
    "                    model=best_arch,  # checkpointables start here\n",
    "                    optim=optim,\n",
    "                    scheduler=scheduler,\n",
    "                )\n",
    "\n",
    "                grad_clip = self.config.evaluation.grad_clip\n",
    "                loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                self.train_top1.reset()\n",
    "                self.train_top5.reset()\n",
    "                self.val_top1.reset()\n",
    "                self.val_top5.reset()\n",
    "\n",
    "                # Enable drop path\n",
    "                best_arch.update_edges(\n",
    "                    update_func=lambda edge: edge.data.set(\n",
    "                        \"op\", DropPathWrapper(edge.data.op)\n",
    "                    ),\n",
    "                    scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                    private_edge_data=True,\n",
    "                )\n",
    "\n",
    "                # train from scratch\n",
    "                epochs = self.config.evaluation.epochs\n",
    "                for e in range(start_epoch, epochs):\n",
    "                    best_arch.train()\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        log_first_n(\n",
    "                            logging.INFO,\n",
    "                            \"cuda consumption\\n {}\".format(torch.cuda.memory_summary()),\n",
    "                            n=20,\n",
    "                        )\n",
    "\n",
    "                    # update drop path probability\n",
    "                    drop_path_prob = self.config.evaluation.drop_path_prob * e / epochs\n",
    "                    best_arch.update_edges(\n",
    "                        update_func=lambda edge: edge.data.set(\n",
    "                            \"drop_path_prob\", drop_path_prob\n",
    "                        ),\n",
    "                        scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                        private_edge_data=True,\n",
    "                    )\n",
    "\n",
    "                    # Train queue\n",
    "                    for i, (input_train, target_train) in enumerate(self.train_queue):\n",
    "                        input_train = input_train.to(self.device)\n",
    "                        target_train = target_train.to(self.device, non_blocking=True)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        logits_train = best_arch(input_train)\n",
    "                        train_loss = loss(logits_train, target_train)\n",
    "                        if hasattr(\n",
    "                            best_arch, \"auxilary_logits\"\n",
    "                        ):  # darts specific stuff\n",
    "                            log_first_n(logging.INFO, \"Auxiliary is used\", n=10)\n",
    "                            auxiliary_loss = loss(\n",
    "                                best_arch.auxilary_logits(), target_train\n",
    "                            )\n",
    "                            train_loss += (\n",
    "                                self.config.evaluation.auxiliary_weight * auxiliary_loss\n",
    "                            )\n",
    "                        train_loss.backward()\n",
    "                        if grad_clip:\n",
    "                            torch.nn.utils.clip_grad_norm_(\n",
    "                                best_arch.parameters(), grad_clip\n",
    "                            )\n",
    "                        optim.step()\n",
    "\n",
    "                        self._store_accuracies(logits_train, target_train, \"train\")\n",
    "                        log_every_n_seconds(\n",
    "                            logging.INFO,\n",
    "                            \"Epoch {}-{}, Train loss: {:.5}, learning rate: {}\".format(\n",
    "                                e, i, train_loss, scheduler.get_last_lr()\n",
    "                            ),\n",
    "                            n=5,\n",
    "                        )\n",
    "\n",
    "                    # Validation queue\n",
    "                    if self.valid_queue:\n",
    "                        best_arch.eval()\n",
    "                        for i, (input_valid, target_valid) in enumerate(\n",
    "                            self.valid_queue\n",
    "                        ):\n",
    "\n",
    "                            input_valid = input_valid.to(self.device).float()\n",
    "                            target_valid = target_valid.to(self.device).float()\n",
    "\n",
    "                            # just log the validation accuracy\n",
    "                            with torch.no_grad():\n",
    "                                logits_valid = best_arch(input_valid)\n",
    "                                self._store_accuracies(\n",
    "                                    logits_valid, target_valid, \"val\"\n",
    "                                )\n",
    "\n",
    "                    scheduler.step()\n",
    "                    self.periodic_checkpointer.step(e)\n",
    "                    self._log_and_reset_accuracies(e)\n",
    "\n",
    "            # Disable drop path\n",
    "            best_arch.update_edges(\n",
    "                update_func=lambda edge: edge.data.set(\n",
    "                    \"op\", edge.data.op.get_embedded_ops()\n",
    "                ),\n",
    "                scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "            # measure final test accuracy\n",
    "            top1 = utils.AverageMeter()\n",
    "            top5 = utils.AverageMeter()\n",
    "\n",
    "            best_arch.eval()\n",
    "\n",
    "            for i, data_test in enumerate(self.test_queue):\n",
    "                input_test, target_test = data_test\n",
    "                input_test = input_test.to(self.device)\n",
    "                target_test = target_test.to(self.device, non_blocking=True)\n",
    "\n",
    "                n = input_test.size(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = best_arch(input_test)\n",
    "\n",
    "                    prec1, prec5 = utils.accuracy(logits, target_test, topk=(1, 5))\n",
    "                    top1.update(prec1.data.item(), n)\n",
    "                    top5.update(prec5.data.item(), n)\n",
    "\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Inference batch {} of {}.\".format(i, len(self.test_queue)),\n",
    "                    n=5,\n",
    "                )\n",
    "\n",
    "            logger.info(\n",
    "                \"Evaluation finished. Test accuracies: top-1 = {:.5}, top-5 = {:.5}\".format(\n",
    "                    top1.avg, top5.avg\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return top1.avg\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_search_dataloaders(config):\n",
    "        return train_queue, valid_queue, _\n",
    "     \n",
    "    @staticmethod\n",
    "    def build_eval_dataloaders(config):\n",
    "        return train_queue, valid_queue, test_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d509cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 7\n"
     ]
    }
   ],
   "source": [
    "name = 'covtype'\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "NUM_FEATURES = ds_train.num_features\n",
    "NUM_CLASSES = ds_train.num_classes\n",
    "print(NUM_FEATURES, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "446b7d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e96b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OP_NAMES = ['linear', 'zero', 'resblock']\n",
    "class Head(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters, num_classes):\n",
    "        super().__init__(locals())\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=filters, out_features=num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        output = self.layers[0](x)\n",
    "        output = nn.functional.relu(output)\n",
    "        return output\n",
    "    \n",
    "class TransformerOP(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters):\n",
    "        super().__init__(locals())\n",
    "        self.bn1 = nn.BatchNorm1d(filters)\n",
    "        self.bn2 = nn.BatchNorm1d(filters)\n",
    "        self.mha = nn.MultiheadAttention(filters, filters // 4, batch_first=True)\n",
    "        self.ff = nn.Linear(filters, filters)\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn1(x)\n",
    "        out, _ = self.mha(x, x, x, need_weights=False)\n",
    "        out = x + out \n",
    "        \n",
    "        out2 = self.bn2(out)\n",
    "        out2 = self.ff(out2)\n",
    "        return out2 + out\n",
    "    \n",
    "class DropPathWrapper(AbstractPrimitive):\n",
    "    \"\"\"\n",
    "    A wrapper for the drop path training regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        super().__init__(locals())\n",
    "        self.op = op\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, x, edge_data):\n",
    "        x = self.op(x, edge_data)\n",
    "        if (\n",
    "            edge_data.drop_path_prob > 0.0\n",
    "            and not isinstance(self.op, Identity)\n",
    "            and self.training\n",
    "        ):\n",
    "            keep_prob = 1.0 - edge_data.drop_path_prob\n",
    "            mask = torch.FloatTensor(x.size(0), 1).bernoulli_(keep_prob)\n",
    "            mask = mask.to(self.device)\n",
    "            x.div_(keep_prob)\n",
    "            x.mul_(mask)\n",
    "        return x\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return self.op\n",
    "class LinearOP(ASRPrimitive):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0, name='Linear'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear})'\n",
    "    \n",
    "class ResBlockLinear(ASRPrimitive):\n",
    "    def __init__(self, features, dropout_rate=0, name='resblock'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(features)\n",
    "        self.linear1 = nn.Linear(features, features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.dropout2 = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "        self.linear2 = nn.Linear(features, features)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = torch.clamp_max_(out, 20)\n",
    "\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        return x + out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear2})'\n",
    "\n",
    "class TabNasSearchSpace(Graph):\n",
    "    \"\"\"\n",
    "    Contains the interface to the tabular benchmark of nas-bench-asr.\n",
    "    Note: currently we do not support building a naslib object for\n",
    "    nas-bench-asr architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    QUERYABLE = True\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'cells_stage_1',\n",
    "        'cells_stage_2',\n",
    "        'cells_stage_3',\n",
    "        'cells_stage_4'\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.load_labeled = False\n",
    "        self.max_epoch = 40\n",
    "        self.max_nodes = 3\n",
    "        self.accs = None\n",
    "        self.compact = None\n",
    "\n",
    "        self.n_blocks = NUM_BLOCKS\n",
    "        self.n_cells_per_block = [1] * NUM_BLOCKS\n",
    "        self.features = NUM_FEATURES\n",
    "        self.filters = [NUM_FILTERS] * NUM_BLOCKS\n",
    "\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.dropout_rate = 0.0\n",
    "        self.use_norm = True\n",
    "\n",
    "        self._create_macro_graph()\n",
    "\n",
    "    def _create_macro_graph(self):\n",
    "        cell = self._create_cell()\n",
    "\n",
    "        # Macrograph defintion\n",
    "        n_nodes = self.n_blocks + 2\n",
    "        self.add_nodes_from(range(1, n_nodes + 1))\n",
    "\n",
    "        for node in range(1, n_nodes):\n",
    "            self.add_edge(node, node + 1)\n",
    "\n",
    "        # Create the cell blocks and add them as subgraphs of nodes 2 ... 5\n",
    "        for idx, node in enumerate(range(2, 2 + self.n_blocks)):\n",
    "            scope = f'cells_stage_{idx + 1}'\n",
    "            cells_block = self._create_cells_block(cell, n=self.n_cells_per_block[idx], scope=scope)\n",
    "            self.nodes[node]['subgraph'] = cells_block.set_input([node - 1])\n",
    "\n",
    "            # Assign the list of operations to the cell edges\n",
    "            cells_block.update_edges(\n",
    "                update_func=lambda edge: _set_cell_edge_ops(edge, filters=self.filters[idx], use_norm=self.use_norm),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            )\n",
    "\n",
    "        start_node = 1\n",
    "        for idx, node in enumerate(range(start_node, start_node + self.n_blocks)):\n",
    "            if node == start_node:\n",
    "                op = LinearOP(self.features, self.filters[0])\n",
    "            else:\n",
    "                op = core_ops.Identity()\n",
    "\n",
    "            self.edges[node, node + 1].set('op', op)\n",
    "        # Assign the LSTM + Linear layer to the last edge in the macro graph\n",
    "        self.edges[self.n_blocks + 1, self.n_blocks + 2].set('op', Head(self.filters[0], self.num_classes))\n",
    "\n",
    "    def _create_cells_block(self, cell, n, scope):\n",
    "        block = Graph()\n",
    "        block.name = f'{n}_cells_block'\n",
    "\n",
    "        block.add_nodes_from(range(1, n + 2))\n",
    "\n",
    "        for node in range(2, n + 2):\n",
    "            block.add_node(node, subgraph=cell.copy().set_scope(scope).set_input([node - 1]))\n",
    "\n",
    "        for node in range(1, n + 2):\n",
    "            block.add_edge(node, node + 1)\n",
    "\n",
    "        return block\n",
    "\n",
    "    def _create_cell(self):\n",
    "        cell = Graph()\n",
    "        cell.name = 'cell'\n",
    "\n",
    "        cell.add_nodes_from(range(1, 8))\n",
    "\n",
    "        # Create edges\n",
    "        for i in range(1, 7):\n",
    "            cell.add_edge(i, i + 1)\n",
    "\n",
    "        for i in range(1, 6, 2):\n",
    "            for j in range(i + 2, 8, 2):\n",
    "                cell.add_edge(i, j)\n",
    "\n",
    "        cell.add_node(8)\n",
    "        cell.add_edge(7, 8)  # For optional layer normalization\n",
    "\n",
    "        return cell\n",
    "\n",
    "    def query(self, metric=None, dataset=None, path=None, epoch=-1,\n",
    "              full_lc=False, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Query results from nas-bench-asr\n",
    "        \"\"\"\n",
    "        metric_to_asr = {\n",
    "            Metric.VAL_ACCURACY: \"val_per\",\n",
    "            Metric.TEST_ACCURACY: \"test_per\",\n",
    "            Metric.PARAMETERS: \"params\",\n",
    "            Metric.FLOPS: \"flops\",\n",
    "        }\n",
    "\n",
    "        assert self.compact is not None\n",
    "        assert metric in [\n",
    "            Metric.TRAIN_ACCURACY,\n",
    "            Metric.TRAIN_LOSS,\n",
    "            Metric.VAL_ACCURACY,\n",
    "            Metric.TEST_ACCURACY,\n",
    "            Metric.PARAMETERS,\n",
    "            Metric.FLOPS,\n",
    "            Metric.TRAIN_TIME,\n",
    "            Metric.RAW,\n",
    "        ]\n",
    "        query_results = dataset_api[\"asr_data\"].full_info(self.compact)\n",
    "\n",
    "        if metric != Metric.VAL_ACCURACY:\n",
    "            if metric == Metric.TEST_ACCURACY:\n",
    "                return query_results[metric_to_asr[metric]]\n",
    "            elif (metric == Metric.PARAMETERS) or (metric == Metric.FLOPS):\n",
    "                return query_results['info'][metric_to_asr[metric]]\n",
    "            elif metric in [Metric.TRAIN_ACCURACY, Metric.TRAIN_LOSS,\n",
    "                            Metric.TRAIN_TIME, Metric.RAW]:\n",
    "                return -1\n",
    "        else:\n",
    "            if full_lc and epoch == -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]]\n",
    "                ]\n",
    "            elif full_lc and epoch != -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]][:epoch]\n",
    "                ]\n",
    "            else:\n",
    "                # return the value of the metric only at the specified epoch\n",
    "                return float(query_results[metric_to_asr[metric]][epoch])\n",
    "\n",
    "    def get_compact(self):\n",
    "        assert self.compact is not None\n",
    "        return self.compact\n",
    "\n",
    "    def get_hash(self):\n",
    "        return self.get_compact()\n",
    "\n",
    "    def set_compact(self, compact):\n",
    "        self.compact = make_compact_immutable(compact)\n",
    "\n",
    "    def sample_random_architecture(self, dataset_api):\n",
    "        search_space = [[len(OP_NAMES)] + [2] * (idx + 1) for idx in\n",
    "                        range(self.max_nodes)]\n",
    "        flat = flatten(search_space)\n",
    "        m = [random.randrange(opts) for opts in flat]\n",
    "        m = copy_structure(m, search_space)\n",
    "\n",
    "        compact = m\n",
    "        self.set_compact(compact)\n",
    "        return compact\n",
    "\n",
    "    def mutate(self, parent, mutation_rate=1, dataset_api=None):\n",
    "        \"\"\"\n",
    "        This will mutate the cell in one of two ways:\n",
    "        change an edge; change an op.\n",
    "        Todo: mutate by adding/removing nodes.\n",
    "        Todo: mutate the list of hidden nodes.\n",
    "        Todo: edges between initial hidden nodes are not mutated.\n",
    "        \"\"\"\n",
    "        parent_compact = parent.get_compact()\n",
    "        parent_compact = make_compact_mutable(parent_compact)\n",
    "        compact = copy.deepcopy(parent_compact)\n",
    "\n",
    "        for _ in range(int(mutation_rate)):\n",
    "            mutation_type = np.random.choice([2])\n",
    "\n",
    "            if mutation_type == 1:\n",
    "                # change an edge\n",
    "                # first pick up a node\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                # pick up an edge id\n",
    "                edge_id = np.random.choice(len(node[1:])) + 1\n",
    "                # edge ops are in [identity, zero] ([0, 1])\n",
    "                new_edge_op = int(not compact[node_id][edge_id])\n",
    "                # apply the mutation\n",
    "                compact[node_id][edge_id] = new_edge_op\n",
    "\n",
    "            elif mutation_type == 2:\n",
    "                # change an op\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                op_id = node[0]\n",
    "                list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                list_of_ops_ids.remove(op_id)\n",
    "                new_op_id = random.choice(list_of_ops_ids)\n",
    "                compact[node_id][0] = new_op_id\n",
    "\n",
    "        self.set_compact(compact)\n",
    "\n",
    "    def get_nbhd(self, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Return all neighbors of the architecture\n",
    "        \"\"\"\n",
    "        compact = self.get_compact()\n",
    "        # edges, ops, hiddens = compact\n",
    "        nbhd = []\n",
    "\n",
    "        def add_to_nbhd(new_compact, nbhd):\n",
    "            print(new_compact)\n",
    "            nbr = NasBenchASRSearchSpace()\n",
    "            nbr.set_compact(new_compact)\n",
    "            nbr_model = torch.nn.Module()\n",
    "            nbr_model.arch = nbr\n",
    "            nbhd.append(nbr_model)\n",
    "            return nbhd\n",
    "\n",
    "        for node_id in range(len(compact)):\n",
    "            node = compact[node_id]\n",
    "            for edge_id in range(len(node)):\n",
    "                if edge_id == 0:\n",
    "                    edge_op = compact[node_id][0]\n",
    "                    list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                    list_of_ops_ids.remove(edge_op)\n",
    "                    for op_id in list_of_ops_ids:\n",
    "                        new_compact = copy.deepcopy(compact)\n",
    "                        new_compact = make_compact_mutable(new_compact)\n",
    "                        new_compact[node_id][0] = op_id\n",
    "                        nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "                else:\n",
    "                    edge_op = compact[node_id][edge_id]\n",
    "                    new_edge_op = int(not edge_op)\n",
    "                    new_compact = copy.deepcopy(compact)\n",
    "                    new_compact = make_compact_mutable(new_compact)\n",
    "                    new_compact[node_id][edge_id] = new_edge_op\n",
    "                    nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "\n",
    "        random.shuffle(nbhd)\n",
    "        return nbhd\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'asr'\n",
    "\n",
    "    def get_max_epochs(self):\n",
    "        return 39\n",
    "\n",
    "    def encode(self, encoding_type=EncodingType.ADJACENCY_ONE_HOT):\n",
    "        return encode_asr(self, encoding_type=encoding_type)\n",
    "\n",
    "\n",
    "def _set_cell_edge_ops(edge, filters, use_norm):\n",
    "    if use_norm and edge.head == 7:\n",
    "        edge.data.set('op', core_ops.Identity())\n",
    "        edge.data.finalize()\n",
    "    elif edge.head % 2 == 0:  # Edge from intermediate node\n",
    "        edge.data.set(\n",
    "            'op', [\n",
    "                LinearOP(filters, filters),\n",
    "                ops['zero'](filters, filters),\n",
    "                ResBlockLinear(filters),\n",
    "                #TransformerOP(filters)\n",
    "            ]\n",
    "        )\n",
    "    elif edge.tail % 2 == 0:  # Edge to intermediate node. Should always be Identity.\n",
    "        edge.data.finalize()\n",
    "    else:\n",
    "        edge.data.set(\n",
    "            'op',\n",
    "            [\n",
    "                core_ops.Zero(stride=1),\n",
    "                core_ops.Identity()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc2f4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/26 00:00:01 nl.defaults.trainer]: Beginning search\n",
      "[05/26 00:00:01 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000365, -0.001063, 0\n",
      "-0.000773, +0.000993, 1\n",
      "-0.000160, -0.000284, 0\n",
      "-0.001207, -0.001312, -0.000661, 2\n",
      "-0.000086, -0.001162, 0\n",
      "-0.000057, +0.000444, 1\n",
      "+0.000083, -0.000238, +0.001047, 2\n",
      "+0.000057, -0.000984, 0\n",
      "+0.000416, +0.001141, -0.001012, 1\n",
      "[05/26 00:00:01 nl.defaults.trainer]: Epoch 0-0, Train loss: 2.83811, validation loss: 2.85240, learning rate: [0.025]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1171/812330156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNasTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Search for an architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Evaluate the best architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naslib/naslib/defaults/trainer.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, resume_from, summary_writer, after_epoch, report_incumbent)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     )\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0mdata_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     data_val = (\n\u001b[1;32m    133\u001b[0m                         \u001b[0mdata_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    611\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                           'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from naslib.optimizers import DARTSOptimizer #+\n",
    "from naslib.optimizers import GSparseOptimizer\n",
    "from naslib.optimizers import OneShotNASOptimizer #+\n",
    "from naslib.optimizers import RandomNASOptimizer\n",
    "from naslib.optimizers import GDASOptimizer #+\n",
    "from naslib.optimizers import DrNASOptimizer #+\n",
    "from naslib.optimizers import RandomSearch\n",
    "from naslib.optimizers import RegularizedEvolution\n",
    "from naslib.optimizers import LocalSearch\n",
    "from naslib.optimizers import Bananas\n",
    "from naslib.optimizers import BasePredictor\n",
    "from naslib.optimizers import Npenas\n",
    "name = 'covtype'\n",
    "DROPOUT_RATE = 0.\n",
    "NUM_FILTERS = 32\n",
    "NUM_BLOCKS = 4\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "dataset = TabNasDataset(config, ds_train, ds_test)\n",
    "\n",
    "train_queue, valid_queue, test_queue, train_transform, valid_transform = dataset.get_loaders()\n",
    "    \n",
    "search_space = TabNasSearchSpace()\n",
    "logger = setup_logger(config.save + \"/log.log\")\n",
    "logger.setLevel(logging.INFO)  \n",
    "\n",
    "optimizer = DARTSOptimizer(**config.search)\n",
    "optimizer.adapt_search_space(search_space, config.dataset)\n",
    "\n",
    "\n",
    "trainer = TabNasTrainer(optimizer, config)\n",
    "trainer.search()  # Search for an architecture\n",
    "trainer.evaluate()  # Evaluate the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375ac54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c7307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269eb82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fd4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4a398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e61f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
