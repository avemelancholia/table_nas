{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7fa0db-3f3a-4a77-80f0-30b154d61b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fvcore.common.config import CfgNode\n",
    "\n",
    "from naslib import utils\n",
    "from naslib.search_spaces import SimpleCellSearchSpace\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.utils.custom_dataset import CustomDataset\n",
    "from naslib.optimizers import DARTSOptimizer, GDASOptimizer, RandomSearch\n",
    "from naslib.search_spaces import NasBench301SearchSpace, SimpleCellSearchSpace\n",
    "from naslib.utils import set_seed, setup_logger, get_config_from_args\n",
    "from naslib.search_spaces.core.query_metrics import Metric\n",
    "from naslib.search_spaces.core.graph import Graph\n",
    "from naslib.search_spaces.nasbenchasr.primitives import ASRPrimitive, CellLayerNorm, Head, ops, PadConvReluNorm\n",
    "from naslib.utils import get_project_root\n",
    "from naslib.search_spaces.core import primitives as core_ops\n",
    "from naslib.search_spaces.nasbenchasr.conversions import flatten, \\\n",
    "    copy_structure, make_compact_mutable, make_compact_immutable\n",
    "from naslib.search_spaces.nasbenchasr.encodings import encode_asr\n",
    "from naslib.utils.encodings import EncodingType\n",
    "from naslib.utils.log import log_every_n_seconds, log_first_n\n",
    "from naslib.search_spaces.core.primitives import AbstractPrimitive, Identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Path('/home/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632db49f-1d96-485f-95f8-30f2bc016128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_closest_square_shape(x):\n",
    "    len_x = len(x)\n",
    "    closest_square = int(np.round(np.sqrt(len_x)))\n",
    "    if closest_square ** 2 < len_x: \n",
    "        closest_square += 1\n",
    "    x = np.pad(x, (0, closest_square  ** 2 - len_x))\n",
    "    return x.reshape((1, closest_square, closest_square))\n",
    "\n",
    "class TabNasTorchDataset(Dataset):\n",
    "    def __init__(self, root_dir, name, queue, norm='min_max'):\n",
    "        super().__init__()\n",
    "        \n",
    "        bin_exists = False\n",
    "        num_exists = False\n",
    "        \n",
    "        assert queue in ['train', 'test', 'val']\n",
    "\n",
    "        self.root_dir = root_dir \n",
    "        self.name = name\n",
    "        self.type = queue\n",
    "\n",
    "        x_num = f'X_num_{queue}.npy'\n",
    "        x_bin = f'X_bin_{queue}.npy'\n",
    "        y = f'Y_{queue}.npy'\n",
    "        \n",
    "        \n",
    "        if (root_dir / name/ x_bin).exists():\n",
    "            bin_exists = True\n",
    "            \n",
    "        if (root_dir / name/ x_num).exists():\n",
    "            num_exists = True\n",
    "        \n",
    "        self.y = np.load(root_dir / name / y)\n",
    "        \n",
    "        if num_exists:\n",
    "            x_num = np.load(root_dir / name / x_num)\n",
    "            x_num = (x_num - np.min(x_num, axis=0)) / (np.max(x_num, axis=0) - np.min(x_num, axis=0))\n",
    "        \n",
    "        if bin_exists:\n",
    "            x_bin = np.load(root_dir / name / x_bin)\n",
    "        \n",
    "        if num_exists and bin_exists:\n",
    "            self.x = np.concatenate((x_num, x_bin), axis=1)\n",
    "        elif num_exists:\n",
    "            self.x = x_num\n",
    "        elif bin_exists:\n",
    "            self.x = x_bin\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.num_features = self.x.shape[1]\n",
    "        self.num_classes = int(np.max(self.y)) + 1\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i]\n",
    "        y = self.y[i]\n",
    "\n",
    "        return  x, y.astype(np.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33258b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/table_nas/darts_cell.yaml') as f:\n",
    "    config = CfgNode.load_cfg(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e11a04-8a8a-4602-a84c-7114ef765608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasDataset(CustomDataset):\n",
    "    def __init__(self, config, ds_train, ds_test, mode='train'):\n",
    "        super().__init__(config, mode)\n",
    "        self.ds_train = ds_train\n",
    "        self.ds_test = ds_test\n",
    "\n",
    "    def get_transforms(self, config):\n",
    "        return Compose([ToTensor()]), Compose([ToTensor()])\n",
    "\n",
    "\n",
    "    def get_data(self, data, train_transform, valid_transform):\n",
    "        train_data = self.ds_train\n",
    "        test_data = self.ds_test\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35850045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNasTrainer(Trainer):\n",
    "    def evaluate(\n",
    "        self,\n",
    "        retrain:bool=True,\n",
    "        search_model:str=\"\",\n",
    "        resume_from:str=\"\",\n",
    "        best_arch:Graph=None,\n",
    "        dataset_api:object=None,\n",
    "        metric:Metric=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the final architecture as given from the optimizer.\n",
    "\n",
    "        If the search space has an interface to a benchmark then query that.\n",
    "        Otherwise train as defined in the config.\n",
    "\n",
    "        Args:\n",
    "            retrain (bool)      : Reset the weights from the architecure search\n",
    "            search_model (str)  : Path to checkpoint file that was created during search. If not provided,\n",
    "                                  then try to load 'model_final.pth' from search\n",
    "            resume_from (str)   : Resume retraining from the given checkpoint file.\n",
    "            best_arch           : Parsed model you want to directly evaluate and ignore the final model\n",
    "                                  from the optimizer.\n",
    "            dataset_api         : Dataset API to use for querying model performance.\n",
    "            metric              : Metric to query the benchmark for.\n",
    "        \"\"\"\n",
    "        logger.info(\"Start evaluation\")\n",
    "        if not best_arch:\n",
    "\n",
    "            if not search_model:\n",
    "                search_model = os.path.join(\n",
    "                    self.config.save, \"search\", \"model_final.pth\"\n",
    "                )\n",
    "            self._setup_checkpointers(search_model)  # required to load the architecture\n",
    "\n",
    "            best_arch = self.optimizer.get_final_architecture()\n",
    "        #logger.info(f\"Final architecture hash: {best_arch.get_hash()}\")\n",
    "\n",
    "        if True:\n",
    "            best_arch.to(self.device)\n",
    "            if retrain:\n",
    "                logger.info(\"Starting retraining from scratch\")\n",
    "                best_arch.reset_weights(inplace=True)\n",
    "\n",
    "                (\n",
    "                    self.train_queue,\n",
    "                    self.valid_queue,\n",
    "                    self.test_queue,\n",
    "                ) = self.build_eval_dataloaders(self.config)\n",
    "\n",
    "                optim = self.build_eval_optimizer(best_arch.parameters(), self.config)\n",
    "                scheduler = self.build_eval_scheduler(optim, self.config)\n",
    "\n",
    "                start_epoch = self._setup_checkpointers(\n",
    "                    resume_from,\n",
    "                    search=False,\n",
    "                    period=self.config.evaluation.checkpoint_freq,\n",
    "                    model=best_arch,  # checkpointables start here\n",
    "                    optim=optim,\n",
    "                    scheduler=scheduler,\n",
    "                )\n",
    "\n",
    "                grad_clip = self.config.evaluation.grad_clip\n",
    "                loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "                self.train_top1.reset()\n",
    "                self.train_top5.reset()\n",
    "                self.val_top1.reset()\n",
    "                self.val_top5.reset()\n",
    "\n",
    "                # Enable drop path\n",
    "                best_arch.update_edges(\n",
    "                    update_func=lambda edge: edge.data.set(\n",
    "                        \"op\", DropPathWrapper(edge.data.op)\n",
    "                    ),\n",
    "                    scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                    private_edge_data=True,\n",
    "                )\n",
    "\n",
    "                # train from scratch\n",
    "                epochs = self.config.evaluation.epochs\n",
    "                for e in range(start_epoch, epochs):\n",
    "                    best_arch.train()\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        log_first_n(\n",
    "                            logging.INFO,\n",
    "                            \"cuda consumption\\n {}\".format(torch.cuda.memory_summary()),\n",
    "                            n=20,\n",
    "                        )\n",
    "\n",
    "                    # update drop path probability\n",
    "                    drop_path_prob = self.config.evaluation.drop_path_prob * e / epochs\n",
    "                    best_arch.update_edges(\n",
    "                        update_func=lambda edge: edge.data.set(\n",
    "                            \"drop_path_prob\", drop_path_prob\n",
    "                        ),\n",
    "                        scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                        private_edge_data=True,\n",
    "                    )\n",
    "\n",
    "                    # Train queue\n",
    "                    for i, (input_train, target_train) in enumerate(self.train_queue):\n",
    "                        input_train = input_train.to(self.device)\n",
    "                        target_train = target_train.to(self.device, non_blocking=True)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        logits_train = best_arch(input_train)\n",
    "                        train_loss = loss(logits_train, target_train)\n",
    "                        if hasattr(\n",
    "                            best_arch, \"auxilary_logits\"\n",
    "                        ):  # darts specific stuff\n",
    "                            log_first_n(logging.INFO, \"Auxiliary is used\", n=10)\n",
    "                            auxiliary_loss = loss(\n",
    "                                best_arch.auxilary_logits(), target_train\n",
    "                            )\n",
    "                            train_loss += (\n",
    "                                self.config.evaluation.auxiliary_weight * auxiliary_loss\n",
    "                            )\n",
    "                        train_loss.backward()\n",
    "                        if grad_clip:\n",
    "                            torch.nn.utils.clip_grad_norm_(\n",
    "                                best_arch.parameters(), grad_clip\n",
    "                            )\n",
    "                        optim.step()\n",
    "\n",
    "                        self._store_accuracies(logits_train, target_train, \"train\")\n",
    "                        log_every_n_seconds(\n",
    "                            logging.INFO,\n",
    "                            \"Epoch {}-{}, Train loss: {:.5}, learning rate: {}\".format(\n",
    "                                e, i, train_loss, scheduler.get_last_lr()\n",
    "                            ),\n",
    "                            n=5,\n",
    "                        )\n",
    "\n",
    "                    # Validation queue\n",
    "                    if self.valid_queue:\n",
    "                        best_arch.eval()\n",
    "                        for i, (input_valid, target_valid) in enumerate(\n",
    "                            self.valid_queue\n",
    "                        ):\n",
    "\n",
    "                            input_valid = input_valid.to(self.device).float()\n",
    "                            target_valid = target_valid.to(self.device).float()\n",
    "\n",
    "                            # just log the validation accuracy\n",
    "                            with torch.no_grad():\n",
    "                                logits_valid = best_arch(input_valid)\n",
    "                                self._store_accuracies(\n",
    "                                    logits_valid, target_valid, \"val\"\n",
    "                                )\n",
    "\n",
    "                    scheduler.step()\n",
    "                    self.periodic_checkpointer.step(e)\n",
    "                    self._log_and_reset_accuracies(e)\n",
    "\n",
    "            # Disable drop path\n",
    "            best_arch.update_edges(\n",
    "                update_func=lambda edge: edge.data.set(\n",
    "                    \"op\", edge.data.op.get_embedded_ops()\n",
    "                ),\n",
    "                scope=best_arch.OPTIMIZER_SCOPE,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "            # measure final test accuracy\n",
    "            top1 = utils.AverageMeter()\n",
    "            top5 = utils.AverageMeter()\n",
    "\n",
    "            best_arch.eval()\n",
    "\n",
    "            for i, data_test in enumerate(self.test_queue):\n",
    "                input_test, target_test = data_test\n",
    "                input_test = input_test.to(self.device)\n",
    "                target_test = target_test.to(self.device, non_blocking=True)\n",
    "\n",
    "                n = input_test.size(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = best_arch(input_test)\n",
    "\n",
    "                    prec1, prec5 = utils.accuracy(logits, target_test, topk=(1, 5))\n",
    "                    top1.update(prec1.data.item(), n)\n",
    "                    top5.update(prec5.data.item(), n)\n",
    "\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Inference batch {} of {}.\".format(i, len(self.test_queue)),\n",
    "                    n=5,\n",
    "                )\n",
    "\n",
    "            logger.info(\n",
    "                \"Evaluation finished. Test accuracies: top-1 = {:.5}, top-5 = {:.5}\".format(\n",
    "                    top1.avg, top5.avg\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return top1.avg\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_search_dataloaders(config):\n",
    "        return train_queue, valid_queue, _\n",
    "     \n",
    "    @staticmethod\n",
    "    def build_eval_dataloaders(config):\n",
    "        return train_queue, valid_queue, test_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d509cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 7\n"
     ]
    }
   ],
   "source": [
    "name = 'covtype'\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "NUM_FEATURES = ds_train.num_features\n",
    "NUM_CLASSES = ds_train.num_classes\n",
    "print(NUM_FEATURES, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446b7d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e96b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OP_NAMES = ['linear', 'zero', 'resblock']\n",
    "class Head(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters, num_classes):\n",
    "        super().__init__(locals())\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=filters, out_features=num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        output = self.layers[0](x)\n",
    "        output = nn.functional.relu(output)\n",
    "        return output\n",
    "    \n",
    "class TransformerOP(ASRPrimitive):\n",
    "\n",
    "    def __init__(self, filters):\n",
    "        super().__init__(locals())\n",
    "        self.bn1 = nn.BatchNorm1d(filters)\n",
    "        self.bn2 = nn.BatchNorm1d(filters)\n",
    "        self.mha = nn.MultiheadAttention(filters, filters // 4, batch_first=True)\n",
    "        self.ff = nn.Linear(filters, filters)\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn1(x)\n",
    "        out, _ = self.mha(x, x, x, need_weights=False)\n",
    "        out = x + out \n",
    "        \n",
    "        out2 = self.bn2(out)\n",
    "        out2 = self.ff(out2)\n",
    "        return out2 + out\n",
    "    \n",
    "class DropPathWrapper(AbstractPrimitive):\n",
    "    \"\"\"\n",
    "    A wrapper for the drop path training regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        super().__init__(locals())\n",
    "        self.op = op\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def forward(self, x, edge_data):\n",
    "        x = self.op(x, edge_data)\n",
    "        if (\n",
    "            edge_data.drop_path_prob > 0.0\n",
    "            and not isinstance(self.op, Identity)\n",
    "            and self.training\n",
    "        ):\n",
    "            keep_prob = 1.0 - edge_data.drop_path_prob\n",
    "            mask = torch.FloatTensor(x.size(0), 1).bernoulli_(keep_prob)\n",
    "            mask = mask.to(self.device)\n",
    "            x.div_(keep_prob)\n",
    "            x.mul_(mask)\n",
    "        return x\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return self.op\n",
    "class LinearOP(ASRPrimitive):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0, name='Linear'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.clamp_max_(x, 20)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear})'\n",
    "    \n",
    "class ResBlockLinear(ASRPrimitive):\n",
    "    def __init__(self, features, dropout_rate=0, name='resblock'):\n",
    "        super().__init__(locals())\n",
    "        self.name = name\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(features)\n",
    "        self.linear1 = nn.Linear(features, features)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=DROPOUT_RATE)\n",
    "        self.dropout2 = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "        self.linear2 = nn.Linear(features, features)\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_data=None):\n",
    "        out = self.bn(x)\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = torch.clamp_max_(out, 20)\n",
    "\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        return x + out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__}({self.linear2})'\n",
    "\n",
    "class TabNasSearchSpace(Graph):\n",
    "    \"\"\"\n",
    "    Contains the interface to the tabular benchmark of nas-bench-asr.\n",
    "    Note: currently we do not support building a naslib object for\n",
    "    nas-bench-asr architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    QUERYABLE = True\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'cells_stage_1',\n",
    "        'cells_stage_2',\n",
    "        'cells_stage_3',\n",
    "        'cells_stage_4'\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.load_labeled = False\n",
    "        self.max_epoch = 40\n",
    "        self.max_nodes = 3\n",
    "        self.accs = None\n",
    "        self.compact = None\n",
    "\n",
    "        self.n_blocks = NUM_BLOCKS\n",
    "        self.n_cells_per_block = [1] * NUM_BLOCKS\n",
    "        self.features = NUM_FEATURES\n",
    "        self.filters = [NUM_FILTERS] * NUM_BLOCKS\n",
    "\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.dropout_rate = 0.0\n",
    "        self.use_norm = True\n",
    "\n",
    "        self._create_macro_graph()\n",
    "\n",
    "    def _create_macro_graph(self):\n",
    "        cell = self._create_cell()\n",
    "\n",
    "        # Macrograph defintion\n",
    "        n_nodes = self.n_blocks + 2\n",
    "        self.add_nodes_from(range(1, n_nodes + 1))\n",
    "\n",
    "        for node in range(1, n_nodes):\n",
    "            self.add_edge(node, node + 1)\n",
    "\n",
    "        # Create the cell blocks and add them as subgraphs of nodes 2 ... 5\n",
    "        for idx, node in enumerate(range(2, 2 + self.n_blocks)):\n",
    "            scope = f'cells_stage_{idx + 1}'\n",
    "            cells_block = self._create_cells_block(cell, n=self.n_cells_per_block[idx], scope=scope)\n",
    "            self.nodes[node]['subgraph'] = cells_block.set_input([node - 1])\n",
    "\n",
    "            # Assign the list of operations to the cell edges\n",
    "            cells_block.update_edges(\n",
    "                update_func=lambda edge: _set_cell_edge_ops(edge, filters=self.filters[idx], use_norm=self.use_norm),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            )\n",
    "\n",
    "        start_node = 1\n",
    "        for idx, node in enumerate(range(start_node, start_node + self.n_blocks)):\n",
    "            if node == start_node:\n",
    "                op = LinearOP(self.features, self.filters[0])\n",
    "            else:\n",
    "                op = core_ops.Identity()\n",
    "\n",
    "            self.edges[node, node + 1].set('op', op)\n",
    "        # Assign the LSTM + Linear layer to the last edge in the macro graph\n",
    "        self.edges[self.n_blocks + 1, self.n_blocks + 2].set('op', Head(self.filters[0], self.num_classes))\n",
    "\n",
    "    def _create_cells_block(self, cell, n, scope):\n",
    "        block = Graph()\n",
    "        block.name = f'{n}_cells_block'\n",
    "\n",
    "        block.add_nodes_from(range(1, n + 2))\n",
    "\n",
    "        for node in range(2, n + 2):\n",
    "            block.add_node(node, subgraph=cell.copy().set_scope(scope).set_input([node - 1]))\n",
    "\n",
    "        for node in range(1, n + 2):\n",
    "            block.add_edge(node, node + 1)\n",
    "\n",
    "        return block\n",
    "\n",
    "    def _create_cell(self):\n",
    "        normal_cell = Graph()\n",
    "        normal_cell.name = \"cell\"    # Use the same name for all cells with shared attributes\n",
    "\n",
    "        # Input nodes\n",
    "        normal_cell.add_node(1)\n",
    "        normal_cell.add_node(2)\n",
    "\n",
    "        # Intermediate nodes\n",
    "        normal_cell.add_node(3)\n",
    "        normal_cell.add_node(4)\n",
    "        normal_cell.add_node(5)\n",
    "        normal_cell.add_node(6)\n",
    "\n",
    "        # Output node\n",
    "        normal_cell.add_node(7)\n",
    "\n",
    "        # Edges\n",
    "        for i in range(2, 7):\n",
    "            normal_cell.add_edge(1, i)   # input 1\n",
    "            #normal_cell.add_edge(2, i)\n",
    "            normal_cell.add_edge(i, 7)\n",
    "    \n",
    "        normal_cell.add_edges_from([(3, 4), (3, 5), (3, 6)])\n",
    "        normal_cell.add_edges_from([(4, 5), (4, 6)])\n",
    "        normal_cell.add_edges_from([(5, 6)])\n",
    "\n",
    "        # Edges connecting to the output are always the identity\n",
    "        \n",
    "        return normal_cell\n",
    "\n",
    "    def query(self, metric=None, dataset=None, path=None, epoch=-1,\n",
    "              full_lc=False, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Query results from nas-bench-asr\n",
    "        \"\"\"\n",
    "        metric_to_asr = {\n",
    "            Metric.VAL_ACCURACY: \"val_per\",\n",
    "            Metric.TEST_ACCURACY: \"test_per\",\n",
    "            Metric.PARAMETERS: \"params\",\n",
    "            Metric.FLOPS: \"flops\",\n",
    "        }\n",
    "\n",
    "        assert self.compact is not None\n",
    "        assert metric in [\n",
    "            Metric.TRAIN_ACCURACY,\n",
    "            Metric.TRAIN_LOSS,\n",
    "            Metric.VAL_ACCURACY,\n",
    "            Metric.TEST_ACCURACY,\n",
    "            Metric.PARAMETERS,\n",
    "            Metric.FLOPS,\n",
    "            Metric.TRAIN_TIME,\n",
    "            Metric.RAW,\n",
    "        ]\n",
    "        query_results = dataset_api[\"asr_data\"].full_info(self.compact)\n",
    "\n",
    "        if metric != Metric.VAL_ACCURACY:\n",
    "            if metric == Metric.TEST_ACCURACY:\n",
    "                return query_results[metric_to_asr[metric]]\n",
    "            elif (metric == Metric.PARAMETERS) or (metric == Metric.FLOPS):\n",
    "                return query_results['info'][metric_to_asr[metric]]\n",
    "            elif metric in [Metric.TRAIN_ACCURACY, Metric.TRAIN_LOSS,\n",
    "                            Metric.TRAIN_TIME, Metric.RAW]:\n",
    "                return -1\n",
    "        else:\n",
    "            if full_lc and epoch == -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]]\n",
    "                ]\n",
    "            elif full_lc and epoch != -1:\n",
    "                return [\n",
    "                    loss for loss in query_results[metric_to_asr[metric]][:epoch]\n",
    "                ]\n",
    "            else:\n",
    "                # return the value of the metric only at the specified epoch\n",
    "                return float(query_results[metric_to_asr[metric]][epoch])\n",
    "\n",
    "    def get_compact(self):\n",
    "        assert self.compact is not None\n",
    "        return self.compact\n",
    "\n",
    "    def get_hash(self):\n",
    "        return self.get_compact()\n",
    "\n",
    "    def set_compact(self, compact):\n",
    "        self.compact = make_compact_immutable(compact)\n",
    "\n",
    "    def sample_random_architecture(self, dataset_api):\n",
    "        search_space = [[len(OP_NAMES)] + [2] * (idx + 1) for idx in\n",
    "                        range(self.max_nodes)]\n",
    "        flat = flatten(search_space)\n",
    "        m = [random.randrange(opts) for opts in flat]\n",
    "        m = copy_structure(m, search_space)\n",
    "\n",
    "        compact = m\n",
    "        self.set_compact(compact)\n",
    "        return compact\n",
    "\n",
    "    def mutate(self, parent, mutation_rate=1, dataset_api=None):\n",
    "        \"\"\"\n",
    "        This will mutate the cell in one of two ways:\n",
    "        change an edge; change an op.\n",
    "        Todo: mutate by adding/removing nodes.\n",
    "        Todo: mutate the list of hidden nodes.\n",
    "        Todo: edges between initial hidden nodes are not mutated.\n",
    "        \"\"\"\n",
    "        parent_compact = parent.get_compact()\n",
    "        parent_compact = make_compact_mutable(parent_compact)\n",
    "        compact = copy.deepcopy(parent_compact)\n",
    "\n",
    "        for _ in range(int(mutation_rate)):\n",
    "            mutation_type = np.random.choice([2])\n",
    "\n",
    "            if mutation_type == 1:\n",
    "                # change an edge\n",
    "                # first pick up a node\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                # pick up an edge id\n",
    "                edge_id = np.random.choice(len(node[1:])) + 1\n",
    "                # edge ops are in [identity, zero] ([0, 1])\n",
    "                new_edge_op = int(not compact[node_id][edge_id])\n",
    "                # apply the mutation\n",
    "                compact[node_id][edge_id] = new_edge_op\n",
    "\n",
    "            elif mutation_type == 2:\n",
    "                # change an op\n",
    "                node_id = np.random.choice(3)\n",
    "                node = compact[node_id]\n",
    "                op_id = node[0]\n",
    "                list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                list_of_ops_ids.remove(op_id)\n",
    "                new_op_id = random.choice(list_of_ops_ids)\n",
    "                compact[node_id][0] = new_op_id\n",
    "\n",
    "        self.set_compact(compact)\n",
    "\n",
    "    def get_nbhd(self, dataset_api=None):\n",
    "        \"\"\"\n",
    "        Return all neighbors of the architecture\n",
    "        \"\"\"\n",
    "        compact = self.get_compact()\n",
    "        # edges, ops, hiddens = compact\n",
    "        nbhd = []\n",
    "\n",
    "        def add_to_nbhd(new_compact, nbhd):\n",
    "            print(new_compact)\n",
    "            nbr = NasBenchASRSearchSpace()\n",
    "            nbr.set_compact(new_compact)\n",
    "            nbr_model = torch.nn.Module()\n",
    "            nbr_model.arch = nbr\n",
    "            nbhd.append(nbr_model)\n",
    "            return nbhd\n",
    "\n",
    "        for node_id in range(len(compact)):\n",
    "            node = compact[node_id]\n",
    "            for edge_id in range(len(node)):\n",
    "                if edge_id == 0:\n",
    "                    edge_op = compact[node_id][0]\n",
    "                    list_of_ops_ids = list(range(len(OP_NAMES)))\n",
    "                    list_of_ops_ids.remove(edge_op)\n",
    "                    for op_id in list_of_ops_ids:\n",
    "                        new_compact = copy.deepcopy(compact)\n",
    "                        new_compact = make_compact_mutable(new_compact)\n",
    "                        new_compact[node_id][0] = op_id\n",
    "                        nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "                else:\n",
    "                    edge_op = compact[node_id][edge_id]\n",
    "                    new_edge_op = int(not edge_op)\n",
    "                    new_compact = copy.deepcopy(compact)\n",
    "                    new_compact = make_compact_mutable(new_compact)\n",
    "                    new_compact[node_id][edge_id] = new_edge_op\n",
    "                    nbhd = add_to_nbhd(new_compact, nbhd)\n",
    "\n",
    "        random.shuffle(nbhd)\n",
    "        return nbhd\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'asr'\n",
    "\n",
    "    def get_max_epochs(self):\n",
    "        return 39\n",
    "\n",
    "    def encode(self, encoding_type=EncodingType.ADJACENCY_ONE_HOT):\n",
    "        return encode_asr(self, encoding_type=encoding_type)\n",
    "\n",
    "\n",
    "def _set_cell_edge_ops(edge, filters, use_norm):\n",
    "    if use_norm and edge.head == 7:\n",
    "        edge.data.set('op', core_ops.Identity())\n",
    "        edge.data.finalize()\n",
    "    elif edge.head % 2 == 0:  # Edge from intermediate node\n",
    "        edge.data.set(\n",
    "            'op', [\n",
    "                LinearOP(filters, filters),\n",
    "                ops['zero'](filters, filters),\n",
    "                ResBlockLinear(filters),\n",
    "                #TransformerOP(filters)\n",
    "            ]\n",
    "        )\n",
    "    elif edge.tail % 2 == 0:  # Edge to intermediate node. Should always be Identity.\n",
    "        edge.data.finalize()\n",
    "    else:\n",
    "        edge.data.set(\n",
    "            'op',\n",
    "            [\n",
    "                core_ops.Zero(stride=1),\n",
    "                core_ops.Identity()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/27 08:56:37 nl.defaults.trainer]: Beginning search\n",
      "[05/27 08:56:37 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "-0.000223, -0.001481, 0\n",
      "+0.001136, -0.000868, 0\n",
      "-0.001103, -0.000543, -0.001024, 1\n",
      "+0.001060, +0.000482, 0\n",
      "-0.000106, +0.000691, 1\n",
      "+0.001744, +0.000882, -0.001240, 0\n",
      "-0.000954, +0.001565, -0.001069, 1\n",
      "-0.000161, -0.002817, -0.001053, 0\n",
      "+0.000136, +0.001057, 1\n",
      "+0.001010, +0.000249, +0.000647, 0\n",
      "[05/27 08:56:38 nl.defaults.trainer]: Epoch 0-0, Train loss: 9.43743, validation loss: 9.04373, learning rate: [0.025]\n",
      "[05/27 08:56:43 nl.defaults.trainer]: Epoch 0-21, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:56:48 nl.defaults.trainer]: Epoch 0-42, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:56:53 nl.defaults.trainer]: Epoch 0-63, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:56:58 nl.defaults.trainer]: Epoch 0-85, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:57:03 nl.defaults.trainer]: Epoch 0-106, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:57:08 nl.defaults.trainer]: Epoch 0-128, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:57:13 nl.defaults.trainer]: Epoch 0-148, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.025]\n",
      "[05/27 08:57:17 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0467e-05,  3.0793e-04, -2.0040e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0003], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0023], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 08:57:17 nl.defaults.trainer]: Epoch 0 done. Train accuracy: 1.27322, Validation accuracy: 1.27697\n",
      "[05/27 08:57:17 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002412, 0\n",
      "+0.002077, -0.001809, 0\n",
      "+0.000060, +0.000308, -0.002004, 1\n",
      "+0.002016, -0.000474, 0\n",
      "+0.000835, -0.000250, 0\n",
      "+0.002916, +0.001793, -0.002251, 0\n",
      "+0.000105, +0.002493, -0.002034, 1\n",
      "+0.000496, -0.001896, -0.001908, 0\n",
      "+0.001084, +0.000108, 0\n",
      "+0.002099, +0.001181, -0.000336, 0\n",
      "[05/27 08:57:18 nl.defaults.trainer]: Epoch 1-1, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:24 nl.defaults.trainer]: Epoch 1-24, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:29 nl.defaults.trainer]: Epoch 1-48, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:34 nl.defaults.trainer]: Epoch 1-70, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:39 nl.defaults.trainer]: Epoch 1-92, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:44 nl.defaults.trainer]: Epoch 1-113, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:49 nl.defaults.trainer]: Epoch 1-133, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:54 nl.defaults.trainer]: Epoch 1-156, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02499385667655336]\n",
      "[05/27 08:57:56 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0415e-05,  3.0778e-04, -2.0034e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0003], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0023], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 08:57:56 nl.defaults.trainer]: Epoch 1 done. Train accuracy: 0.47361, Validation accuracy: 0.45642\n",
      "[05/27 08:57:56 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002411, 0\n",
      "+0.002076, -0.001809, 0\n",
      "+0.000060, +0.000308, -0.002003, 1\n",
      "+0.002014, -0.000473, 0\n",
      "+0.000835, -0.000250, 0\n",
      "+0.002915, +0.001792, -0.002251, 0\n",
      "+0.000105, +0.002492, -0.002033, 1\n",
      "+0.000495, -0.001894, -0.001907, 0\n",
      "+0.001084, +0.000108, 0\n",
      "+0.002098, +0.001180, -0.000336, 0\n",
      "[05/27 08:57:59 nl.defaults.trainer]: Epoch 2-9, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:04 nl.defaults.trainer]: Epoch 2-30, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:09 nl.defaults.trainer]: Epoch 2-53, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:15 nl.defaults.trainer]: Epoch 2-74, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:20 nl.defaults.trainer]: Epoch 2-94, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:25 nl.defaults.trainer]: Epoch 2-117, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:30 nl.defaults.trainer]: Epoch 2-138, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:36 nl.defaults.trainer]: Epoch 2-160, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024975432768931983]\n",
      "[05/27 08:58:36 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0344e-05,  3.0757e-04, -2.0025e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0003], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0023], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 08:58:37 nl.defaults.trainer]: Epoch 2 done. Train accuracy: 0.47361, Validation accuracy: 0.47250\n",
      "[05/27 08:58:37 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002411, 0\n",
      "+0.002076, -0.001808, 0\n",
      "+0.000060, +0.000308, -0.002003, 1\n",
      "+0.002013, -0.000473, 0\n",
      "+0.000834, -0.000250, 0\n",
      "+0.002912, +0.001792, -0.002250, 0\n",
      "+0.000105, +0.002491, -0.002032, 1\n",
      "+0.000494, -0.001892, -0.001905, 0\n",
      "+0.001084, +0.000108, 0\n",
      "+0.002098, +0.001180, -0.000336, 0\n",
      "[05/27 08:58:41 nl.defaults.trainer]: Epoch 3-10, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/27 08:58:46 nl.defaults.trainer]: Epoch 3-29, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:58:51 nl.defaults.trainer]: Epoch 3-48, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:58:56 nl.defaults.trainer]: Epoch 3-68, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:01 nl.defaults.trainer]: Epoch 3-88, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:06 nl.defaults.trainer]: Epoch 3-108, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:11 nl.defaults.trainer]: Epoch 3-126, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:16 nl.defaults.trainer]: Epoch 3-144, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:22 nl.defaults.trainer]: Epoch 3-163, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02494474645930835]\n",
      "[05/27 08:59:22 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0257e-05,  3.0732e-04, -2.0014e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 08:59:22 nl.defaults.trainer]: Epoch 3 done. Train accuracy: 0.47361, Validation accuracy: 0.48024\n",
      "[05/27 08:59:22 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002411, 0\n",
      "+0.002075, -0.001807, 0\n",
      "+0.000060, +0.000307, -0.002001, 1\n",
      "+0.002011, -0.000473, 0\n",
      "+0.000833, -0.000250, 0\n",
      "+0.002909, +0.001791, -0.002249, 0\n",
      "+0.000105, +0.002489, -0.002031, 1\n",
      "+0.000492, -0.001889, -0.001903, 0\n",
      "+0.001084, +0.000108, 0\n",
      "+0.002096, +0.001180, -0.000336, 0\n",
      "[05/27 08:59:27 nl.defaults.trainer]: Epoch 4-17, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:32 nl.defaults.trainer]: Epoch 4-37, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:37 nl.defaults.trainer]: Epoch 4-58, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:42 nl.defaults.trainer]: Epoch 4-75, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:47 nl.defaults.trainer]: Epoch 4-93, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:52 nl.defaults.trainer]: Epoch 4-112, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 08:59:57 nl.defaults.trainer]: Epoch 4-133, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 09:00:02 nl.defaults.trainer]: Epoch 4-155, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024901828031365255]\n",
      "[05/27 09:00:04 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0153e-05,  3.0702e-04, -2.0001e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:00:04 nl.defaults.trainer]: Epoch 4 done. Train accuracy: 0.47361, Validation accuracy: 0.43737\n",
      "[05/27 09:00:04 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002410, 0\n",
      "+0.002073, -0.001806, 0\n",
      "+0.000060, +0.000307, -0.002000, 1\n",
      "+0.002009, -0.000472, 0\n",
      "+0.000832, -0.000250, 0\n",
      "+0.002906, +0.001790, -0.002249, 0\n",
      "+0.000105, +0.002487, -0.002030, 1\n",
      "+0.000490, -0.001886, -0.001901, 0\n",
      "+0.001083, +0.000108, 0\n",
      "+0.002095, +0.001180, -0.000336, 0\n",
      "[05/27 09:00:07 nl.defaults.trainer]: Epoch 5-11, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:13 nl.defaults.trainer]: Epoch 5-32, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:18 nl.defaults.trainer]: Epoch 5-53, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:23 nl.defaults.trainer]: Epoch 5-75, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:28 nl.defaults.trainer]: Epoch 5-95, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:33 nl.defaults.trainer]: Epoch 5-116, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:38 nl.defaults.trainer]: Epoch 5-133, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:43 nl.defaults.trainer]: Epoch 5-153, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024846719840409467]\n",
      "[05/27 09:00:46 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 6.0033e-05,  3.0667e-04, -1.9986e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:00:46 nl.defaults.trainer]: Epoch 5 done. Train accuracy: 0.47361, Validation accuracy: 0.45166\n",
      "[05/27 09:00:46 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002409, 0\n",
      "+0.002072, -0.001805, 0\n",
      "+0.000060, +0.000307, -0.001999, 1\n",
      "+0.002006, -0.000471, 0\n",
      "+0.000831, -0.000249, 0\n",
      "+0.002902, +0.001789, -0.002248, 0\n",
      "+0.000104, +0.002485, -0.002029, 1\n",
      "+0.000488, -0.001882, -0.001898, 0\n",
      "+0.001083, +0.000108, 0\n",
      "+0.002093, +0.001179, -0.000336, 0\n",
      "[05/27 09:00:49 nl.defaults.trainer]: Epoch 6-4, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:00:54 nl.defaults.trainer]: Epoch 6-24, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:00:59 nl.defaults.trainer]: Epoch 6-44, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/27 09:01:04 nl.defaults.trainer]: Epoch 6-64, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:09 nl.defaults.trainer]: Epoch 6-84, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:14 nl.defaults.trainer]: Epoch 6-102, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:19 nl.defaults.trainer]: Epoch 6-118, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:25 nl.defaults.trainer]: Epoch 6-140, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:30 nl.defaults.trainer]: Epoch 6-158, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02477947627157218]\n",
      "[05/27 09:01:31 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9897e-05,  3.0627e-04, -1.9969e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:01:31 nl.defaults.trainer]: Epoch 6 done. Train accuracy: 0.47361, Validation accuracy: 0.47250\n",
      "[05/27 09:01:31 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000708, -0.002409, 0\n",
      "+0.002070, -0.001803, 0\n",
      "+0.000060, +0.000306, -0.001997, 1\n",
      "+0.002004, -0.000471, 0\n",
      "+0.000830, -0.000249, 0\n",
      "+0.002897, +0.001787, -0.002246, 0\n",
      "+0.000104, +0.002483, -0.002028, 1\n",
      "+0.000486, -0.001878, -0.001895, 0\n",
      "+0.001082, +0.000108, 0\n",
      "+0.002092, +0.001179, -0.000336, 0\n",
      "[05/27 09:01:35 nl.defaults.trainer]: Epoch 7-8, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:01:40 nl.defaults.trainer]: Epoch 7-23, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:01:46 nl.defaults.trainer]: Epoch 7-41, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:01:51 nl.defaults.trainer]: Epoch 7-59, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:01:56 nl.defaults.trainer]: Epoch 7-77, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:02:01 nl.defaults.trainer]: Epoch 7-95, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:02:06 nl.defaults.trainer]: Epoch 7-114, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:02:11 nl.defaults.trainer]: Epoch 7-132, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:02:16 nl.defaults.trainer]: Epoch 7-151, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024700163686137408]\n",
      "[05/27 09:02:20 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9744e-05,  3.0583e-04, -1.9950e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:02:20 nl.defaults.trainer]: Epoch 7 done. Train accuracy: 0.47361, Validation accuracy: 0.47488\n",
      "[05/27 09:02:20 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000707, -0.002408, 0\n",
      "+0.002068, -0.001802, 0\n",
      "+0.000060, +0.000306, -0.001995, 1\n",
      "+0.002000, -0.000470, 0\n",
      "+0.000828, -0.000248, 0\n",
      "+0.002892, +0.001786, -0.002245, 0\n",
      "+0.000104, +0.002480, -0.002026, 1\n",
      "+0.000484, -0.001873, -0.001892, 0\n",
      "+0.001081, +0.000108, 0\n",
      "+0.002090, +0.001178, -0.000336, 0\n",
      "[05/27 09:02:22 nl.defaults.trainer]: Epoch 8-3, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:27 nl.defaults.trainer]: Epoch 8-23, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:32 nl.defaults.trainer]: Epoch 8-42, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:37 nl.defaults.trainer]: Epoch 8-61, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:42 nl.defaults.trainer]: Epoch 8-79, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:47 nl.defaults.trainer]: Epoch 8-98, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:53 nl.defaults.trainer]: Epoch 8-119, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:02:58 nl.defaults.trainer]: Epoch 8-137, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:03:03 nl.defaults.trainer]: Epoch 8-158, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02460886035605146]\n",
      "[05/27 09:03:05 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9572e-05,  3.0533e-04, -1.9929e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:03:05 nl.defaults.trainer]: Epoch 8 done. Train accuracy: 0.47361, Validation accuracy: 0.45702\n",
      "[05/27 09:03:05 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000707, -0.002407, 0\n",
      "+0.002066, -0.001800, 0\n",
      "+0.000060, +0.000305, -0.001993, 1\n",
      "+0.001997, -0.000469, 0\n",
      "+0.000827, -0.000248, 0\n",
      "+0.002886, +0.001784, -0.002244, 0\n",
      "+0.000104, +0.002477, -0.002024, 1\n",
      "+0.000481, -0.001867, -0.001888, 0\n",
      "+0.001081, +0.000108, 0\n",
      "+0.002087, +0.001178, -0.000336, 0\n",
      "[05/27 09:03:08 nl.defaults.trainer]: Epoch 9-10, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:13 nl.defaults.trainer]: Epoch 9-30, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:18 nl.defaults.trainer]: Epoch 9-50, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/27 09:03:24 nl.defaults.trainer]: Epoch 9-69, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:29 nl.defaults.trainer]: Epoch 9-90, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:34 nl.defaults.trainer]: Epoch 9-111, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:39 nl.defaults.trainer]: Epoch 9-129, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:44 nl.defaults.trainer]: Epoch 9-149, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024505656386677944]\n",
      "[05/27 09:03:48 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9383e-05,  3.0477e-04, -1.9905e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:03:48 nl.defaults.trainer]: Epoch 9 done. Train accuracy: 0.47361, Validation accuracy: 0.46684\n",
      "[05/27 09:03:48 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000707, -0.002406, 0\n",
      "+0.002064, -0.001798, 0\n",
      "+0.000059, +0.000305, -0.001991, 1\n",
      "+0.001993, -0.000468, 0\n",
      "+0.000825, -0.000247, 0\n",
      "+0.002879, +0.001782, -0.002242, 0\n",
      "+0.000103, +0.002473, -0.002022, 1\n",
      "+0.000478, -0.001861, -0.001884, 0\n",
      "+0.001080, +0.000108, 0\n",
      "+0.002085, +0.001177, -0.000335, 0\n",
      "[05/27 09:03:49 nl.defaults.trainer]: Epoch 10-0, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:03:54 nl.defaults.trainer]: Epoch 10-19, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:00 nl.defaults.trainer]: Epoch 10-38, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:05 nl.defaults.trainer]: Epoch 10-57, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:10 nl.defaults.trainer]: Epoch 10-77, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:15 nl.defaults.trainer]: Epoch 10-97, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:20 nl.defaults.trainer]: Epoch 10-115, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:25 nl.defaults.trainer]: Epoch 10-134, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:31 nl.defaults.trainer]: Epoch 10-152, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.024390653627874664]\n",
      "[05/27 09:04:34 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.9173e-05,  3.0416e-04, -1.9879e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0019, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:04:34 nl.defaults.trainer]: Epoch 10 done. Train accuracy: 0.47361, Validation accuracy: 0.46506\n",
      "[05/27 09:04:34 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000706, -0.002405, 0\n",
      "+0.002062, -0.001796, 0\n",
      "+0.000059, +0.000304, -0.001988, 1\n",
      "+0.001988, -0.000467, 0\n",
      "+0.000823, -0.000247, 0\n",
      "+0.002872, +0.001780, -0.002240, 0\n",
      "+0.000103, +0.002470, -0.002020, 1\n",
      "+0.000474, -0.001855, -0.001879, 0\n",
      "+0.001079, +0.000107, 0\n",
      "+0.002082, +0.001176, -0.000335, 0\n",
      "[05/27 09:04:36 nl.defaults.trainer]: Epoch 11-3, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:04:41 nl.defaults.trainer]: Epoch 11-23, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:04:46 nl.defaults.trainer]: Epoch 11-42, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:04:51 nl.defaults.trainer]: Epoch 11-62, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:04:56 nl.defaults.trainer]: Epoch 11-85, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:05:01 nl.defaults.trainer]: Epoch 11-106, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:05:07 nl.defaults.trainer]: Epoch 11-125, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:05:12 nl.defaults.trainer]: Epoch 11-144, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:05:17 nl.defaults.trainer]: Epoch 11-163, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02426396557348011]\n",
      "[05/27 09:05:17 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.8943e-05,  3.0349e-04, -1.9850e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0018, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:05:17 nl.defaults.trainer]: Epoch 11 done. Train accuracy: 0.47361, Validation accuracy: 0.45881\n",
      "[05/27 09:05:17 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000706, -0.002403, 0\n",
      "+0.002059, -0.001793, 0\n",
      "+0.000059, +0.000303, -0.001985, 1\n",
      "+0.001984, -0.000466, 0\n",
      "+0.000820, -0.000246, 0\n",
      "+0.002864, +0.001778, -0.002238, 0\n",
      "+0.000102, +0.002465, -0.002017, 1\n",
      "+0.000470, -0.001847, -0.001874, 0\n",
      "+0.001078, +0.000107, 0\n",
      "+0.002079, +0.001176, -0.000335, 0\n",
      "[05/27 09:05:22 nl.defaults.trainer]: Epoch 12-17, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:27 nl.defaults.trainer]: Epoch 12-35, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:32 nl.defaults.trainer]: Epoch 12-56, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:37 nl.defaults.trainer]: Epoch 12-76, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/27 09:05:43 nl.defaults.trainer]: Epoch 12-96, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:48 nl.defaults.trainer]: Epoch 12-116, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:53 nl.defaults.trainer]: Epoch 12-135, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:05:58 nl.defaults.trainer]: Epoch 12-155, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.02412571724930873]\n",
      "[05/27 09:06:00 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.8691e-05,  3.0275e-04, -1.9818e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0018, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:06:00 nl.defaults.trainer]: Epoch 12 done. Train accuracy: 0.47361, Validation accuracy: 0.46714\n",
      "[05/27 09:06:00 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000706, -0.002402, 0\n",
      "+0.002056, -0.001790, 0\n",
      "+0.000059, +0.000303, -0.001982, 1\n",
      "+0.001978, -0.000465, 0\n",
      "+0.000818, -0.000245, 0\n",
      "+0.002855, +0.001775, -0.002236, 0\n",
      "+0.000102, +0.002461, -0.002015, 1\n",
      "+0.000466, -0.001839, -0.001868, 0\n",
      "+0.001077, +0.000107, 0\n",
      "+0.002076, +0.001175, -0.000335, 0\n",
      "[05/27 09:06:03 nl.defaults.trainer]: Epoch 13-9, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:08 nl.defaults.trainer]: Epoch 13-27, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:13 nl.defaults.trainer]: Epoch 13-45, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:19 nl.defaults.trainer]: Epoch 13-67, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:24 nl.defaults.trainer]: Epoch 13-87, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:29 nl.defaults.trainer]: Epoch 13-107, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:34 nl.defaults.trainer]: Epoch 13-127, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:39 nl.defaults.trainer]: Epoch 13-147, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023976045089765564]\n",
      "[05/27 09:06:42 nl.optimizers.oneshot.darts.optimizer]: Arch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0007, -0.0024], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021, -0.0018], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.8415e-05,  3.0194e-04, -1.9783e-03], device='cuda:1',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0020, -0.0005], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0008, -0.0002], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0028,  0.0018, -0.0022], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0001,  0.0025, -0.0020], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005, -0.0018, -0.0019], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([0.0011, 0.0001], device='cuda:1', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0021,  0.0012, -0.0003], device='cuda:1', requires_grad=True)]\n",
      "[05/27 09:06:43 nl.defaults.trainer]: Epoch 13 done. Train accuracy: 0.47361, Validation accuracy: 0.47697\n",
      "[05/27 09:06:43 nl.optimizers.oneshot.darts.optimizer]: Arch weights (alphas, last column argmax): \n",
      "+0.000705, -0.002400, 0\n",
      "+0.002052, -0.001788, 0\n",
      "+0.000058, +0.000302, -0.001978, 1\n",
      "+0.001972, -0.000463, 0\n",
      "+0.000815, -0.000245, 0\n",
      "+0.002846, +0.001772, -0.002233, 0\n",
      "+0.000101, +0.002456, -0.002012, 1\n",
      "+0.000462, -0.001830, -0.001862, 0\n",
      "+0.001076, +0.000107, 0\n",
      "+0.002072, +0.001174, -0.000335, 0\n",
      "[05/27 09:06:44 nl.defaults.trainer]: Epoch 14-2, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:06:49 nl.defaults.trainer]: Epoch 14-21, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:06:55 nl.defaults.trainer]: Epoch 14-37, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:00 nl.defaults.trainer]: Epoch 14-55, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:05 nl.defaults.trainer]: Epoch 14-75, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:10 nl.defaults.trainer]: Epoch 14-97, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:16 nl.defaults.trainer]: Epoch 14-119, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:21 nl.defaults.trainer]: Epoch 14-137, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n",
      "[05/27 09:07:26 nl.defaults.trainer]: Epoch 14-156, Train loss: 1.94591, validation loss: 1.94591, learning rate: [0.023815096803201947]\n"
     ]
    }
   ],
   "source": [
    "from naslib.optimizers import DARTSOptimizer #+\n",
    "from naslib.optimizers import GSparseOptimizer\n",
    "from naslib.optimizers import OneShotNASOptimizer #+\n",
    "from naslib.optimizers import RandomNASOptimizer\n",
    "from naslib.optimizers import GDASOptimizer #+\n",
    "from naslib.optimizers import DrNASOptimizer #+\n",
    "from naslib.optimizers import RandomSearch\n",
    "from naslib.optimizers import RegularizedEvolution\n",
    "from naslib.optimizers import LocalSearch\n",
    "from naslib.optimizers import Bananas\n",
    "from naslib.optimizers import BasePredictor\n",
    "from naslib.optimizers import Npenas\n",
    "name = 'covtype'\n",
    "DROPOUT_RATE = 0.\n",
    "NUM_FILTERS = 16\n",
    "DEVICE = 'cuda:1'\n",
    "NUM_BLOCKS = 4\n",
    "ds_train = TabNasTorchDataset(datasets, name, 'train')\n",
    "ds_test = TabNasTorchDataset(datasets, name, 'test')\n",
    "\n",
    "dataset = TabNasDataset(config, ds_train, ds_test)\n",
    "\n",
    "train_queue, valid_queue, test_queue, train_transform, valid_transform = dataset.get_loaders()\n",
    "    \n",
    "search_space = TabNasSearchSpace()\n",
    "logger = setup_logger(config.save + \"/darts_new_new.log\")\n",
    "logger.setLevel(logging.INFO)  \n",
    "\n",
    "optimizer = DARTSOptimizer(**config)\n",
    "optimizer.device = DEVICE\n",
    "optimizer.adapt_search_space(search_space, config.dataset)\n",
    "\n",
    "\n",
    "trainer = TabNasTrainer(optimizer, config)\n",
    "trainer.device = DEVICE\n",
    "\n",
    "trainer.search()  # Search for an architecture\n",
    "trainer.evaluate()  # Evaluate the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a375ac54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tabular'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "105c7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkit\n",
      "  Using cached networkit-11.0.tar.gz (6.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from networkit) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from networkit) (1.21.5)\n",
      "Building wheels for collected packages: networkit\n",
      "  Building wheel for networkit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for networkit: filename=networkit-11.0-cp37-cp37m-linux_x86_64.whl size=13986993 sha256=798aa6494b9da4da19169179d20e6d30c13b1b2470706ba2c5da81fd00eff134\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/c1/2b/bf93e15b1a9cdd359994f0b3f8f027946d0987f1dc3b559cfd\n",
      "Successfully built networkit\n",
      "Installing collected packages: networkit\n",
      "Successfully installed networkit-11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install networkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "        cell.add_nodes_from(range(1, 8))\n",
    "\n",
    "        # Create edges\n",
    "        for i in range(1, 7):\n",
    "            cell.add_edge(i, i + 1)\n",
    "\n",
    "        for i in range(1, 6, 2):\n",
    "            for j in range(i + 2, 8, 2):\n",
    "                cell.add_edge(i, j)\n",
    "\n",
    "        cell.add_node(8)\n",
    "        cell.add_edge(7, 8)  # For optional layer normalization\n",
    "\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d27fd4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "G = nx.DiGraph()\n",
    "#Add edges to the graph\n",
    "for i in range(1, 7):\n",
    "    G.add_edge(i, i + 1)\n",
    "\n",
    "for i in range(1, 6, 2):\n",
    "    for j in range(i + 2, 8, 2):\n",
    "        G.add_edge(i, j)\n",
    "#To see the graph structural overview\n",
    "#For visualization\n",
    "options = {\n",
    "    \"font_size\": 16,\n",
    "    \"node_size\": 500,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"black\",\n",
    "    \"linewidths\": 1,\n",
    "    \"width\": 1,\n",
    "\n",
    "}\n",
    "pos = nx.planar_layout(G)\n",
    "\n",
    "nx.draw_networkx(G,pos,  **options) \n",
    "ax = plt.gca()\n",
    "ax.margins(0.20)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9989175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([-1.        , -0.36585366]),\n",
       " 2: array([ 0.70731707, -0.36585366]),\n",
       " 3: array([ 0.53658537, -0.19512195]),\n",
       " 4: array([ 0.02439024, -0.02439024]),\n",
       " 5: array([0.02439024, 0.14634146]),\n",
       " 6: array([-0.14634146,  0.31707317]),\n",
       " 7: array([-0.14634146,  0.48780488])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d4a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/experiments/metrics.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fea52c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darts_covtype 38.621\n",
      "darts_higgs-small 47.144\n",
      "darts_otto 26.05\n",
      "darts_adult 76.377\n",
      "darts_churn 79.65\n",
      "gdas_covtype 48.76\n",
      "gdas_higgs-small 47.144\n",
      "gdas_otto 66.063\n",
      "gdas_adult 76.377\n",
      "gdas_churn 79.65\n",
      "drnaso_covtype 54.375\n",
      "drnaso_higgs-small 47.144\n",
      "drnaso_otto 47.115\n",
      "drnaso_adult 76.377\n",
      "drnaso_churn 79.65\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key, data[key]['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d30607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91203d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741da4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1cda599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'darts_covtype': {'train_acc': [66.14943,\n",
       "   73.40391,\n",
       "   77.70975,\n",
       "   79.48557,\n",
       "   80.79196,\n",
       "   81.69347,\n",
       "   82.27465,\n",
       "   83.10474,\n",
       "   83.59897,\n",
       "   84.25546,\n",
       "   84.71353,\n",
       "   85.11244,\n",
       "   85.53107,\n",
       "   85.86425,\n",
       "   86.05279,\n",
       "   86.42571,\n",
       "   86.65908,\n",
       "   86.88767,\n",
       "   87.04902,\n",
       "   87.24325,\n",
       "   87.40849,\n",
       "   87.64126,\n",
       "   87.90451,\n",
       "   88.02344,\n",
       "   88.11547,\n",
       "   88.21199,\n",
       "   88.42504,\n",
       "   88.54277,\n",
       "   88.63241,\n",
       "   88.8105,\n",
       "   88.91837,\n",
       "   88.95005,\n",
       "   89.05732,\n",
       "   89.17744,\n",
       "   89.21509,\n",
       "   89.39467,\n",
       "   89.44816,\n",
       "   89.57485,\n",
       "   89.65912,\n",
       "   89.76968,\n",
       "   89.83601,\n",
       "   89.95404,\n",
       "   89.99378,\n",
       "   90.03801,\n",
       "   90.13422,\n",
       "   90.18472,\n",
       "   90.36072,\n",
       "   90.36401,\n",
       "   90.47546,\n",
       "   90.5666,\n",
       "   90.58304,\n",
       "   90.68732,\n",
       "   90.77009,\n",
       "   90.87437,\n",
       "   90.89051,\n",
       "   90.9676,\n",
       "   91.05994,\n",
       "   91.15167,\n",
       "   91.21711,\n",
       "   91.29599,\n",
       "   91.31273,\n",
       "   91.37906,\n",
       "   91.46602,\n",
       "   91.50277,\n",
       "   91.61303,\n",
       "   91.73793,\n",
       "   91.83385,\n",
       "   91.91094,\n",
       "   91.87748,\n",
       "   91.98176,\n",
       "   92.07469,\n",
       "   92.16762,\n",
       "   92.20527,\n",
       "   92.23007,\n",
       "   92.26772,\n",
       "   92.33585,\n",
       "   92.54442,\n",
       "   92.52858,\n",
       "   92.57191,\n",
       "   92.70338,\n",
       "   92.73655,\n",
       "   92.74701,\n",
       "   92.88237,\n",
       "   92.92032,\n",
       "   92.91255,\n",
       "   93.0503,\n",
       "   93.06345,\n",
       "   93.08018,\n",
       "   93.14473,\n",
       "   93.22301,\n",
       "   93.26813,\n",
       "   93.32311,\n",
       "   93.3506,\n",
       "   93.35449,\n",
       "   93.39453,\n",
       "   93.41335,\n",
       "   93.48208,\n",
       "   93.49941,\n",
       "   93.54154,\n",
       "   93.48776,\n",
       "   2.97255,\n",
       "   10.47624,\n",
       "   37.42104,\n",
       "   36.91396,\n",
       "   36.78308,\n",
       "   36.58557,\n",
       "   36.46963,\n",
       "   36.36206,\n",
       "   36.27839,\n",
       "   36.18248,\n",
       "   36.11106,\n",
       "   36.04682,\n",
       "   36.02769,\n",
       "   35.93596,\n",
       "   35.93984,\n",
       "   35.91205,\n",
       "   35.85737,\n",
       "   35.80598,\n",
       "   35.85737,\n",
       "   35.79791,\n",
       "   35.81494,\n",
       "   35.86395,\n",
       "   35.80897,\n",
       "   35.82152,\n",
       "   35.73307,\n",
       "   35.79552,\n",
       "   35.77849,\n",
       "   35.81644,\n",
       "   35.76922,\n",
       "   35.81494,\n",
       "   35.78207,\n",
       "   35.80209,\n",
       "   35.77311,\n",
       "   35.7761,\n",
       "   35.79522,\n",
       "   35.81972,\n",
       "   35.85349,\n",
       "   35.89263,\n",
       "   35.89054,\n",
       "   35.9264,\n",
       "   35.93028,\n",
       "   35.95239,\n",
       "   35.95419,\n",
       "   35.96196,\n",
       "   35.99273,\n",
       "   35.95688,\n",
       "   35.95269,\n",
       "   35.9255,\n",
       "   35.97421,\n",
       "   35.9769,\n",
       "   35.96255,\n",
       "   35.97002,\n",
       "   35.93954,\n",
       "   35.91295,\n",
       "   35.91205,\n",
       "   35.90488,\n",
       "   35.89144,\n",
       "   35.90907,\n",
       "   37.34813,\n",
       "   38.22782,\n",
       "   38.25322,\n",
       "   38.2843,\n",
       "   38.27175,\n",
       "   38.24784,\n",
       "   38.23111,\n",
       "   38.24008,\n",
       "   38.22693,\n",
       "   38.25591,\n",
       "   38.25621,\n",
       "   38.24545,\n",
       "   38.30014,\n",
       "   38.29506,\n",
       "   38.3103,\n",
       "   38.26458,\n",
       "   38.29506,\n",
       "   38.27145,\n",
       "   38.27952,\n",
       "   38.27324,\n",
       "   38.25591,\n",
       "   38.24306,\n",
       "   38.28669,\n",
       "   38.27683,\n",
       "   38.2849,\n",
       "   38.27593,\n",
       "   38.29954,\n",
       "   38.26667,\n",
       "   38.27922,\n",
       "   38.29117,\n",
       "   38.28012,\n",
       "   38.27384,\n",
       "   38.29476,\n",
       "   38.30342,\n",
       "   38.28041,\n",
       "   38.29177,\n",
       "   38.3082,\n",
       "   38.30193,\n",
       "   38.30312,\n",
       "   38.2837,\n",
       "   38.29237],\n",
       "  'val_acc': [66.22791,\n",
       "   73.30948,\n",
       "   77.64982,\n",
       "   79.31236,\n",
       "   80.65721,\n",
       "   81.63288,\n",
       "   82.23609,\n",
       "   82.89587,\n",
       "   83.34633,\n",
       "   84.07757,\n",
       "   84.59413,\n",
       "   85.08509,\n",
       "   85.36228,\n",
       "   85.70646,\n",
       "   85.94345,\n",
       "   86.36445,\n",
       "   86.56393,\n",
       "   86.72381,\n",
       "   86.97718,\n",
       "   87.07603,\n",
       "   87.32464,\n",
       "   87.3824,\n",
       "   87.65691,\n",
       "   87.6703,\n",
       "   87.73193,\n",
       "   87.91504,\n",
       "   88.19312,\n",
       "   88.27232,\n",
       "   88.29197,\n",
       "   88.45572,\n",
       "   88.57868,\n",
       "   88.70046,\n",
       "   88.67962,\n",
       "   88.81866,\n",
       "   88.84962,\n",
       "   88.95234,\n",
       "   89.10656,\n",
       "   89.1935,\n",
       "   89.13425,\n",
       "   89.31468,\n",
       "   89.48588,\n",
       "   89.47278,\n",
       "   89.53649,\n",
       "   89.66452,\n",
       "   89.70441,\n",
       "   89.6928,\n",
       "   89.86102,\n",
       "   89.91282,\n",
       "   89.95957,\n",
       "   90.11826,\n",
       "   90.09444,\n",
       "   90.22872,\n",
       "   90.32905,\n",
       "   90.3487,\n",
       "   90.49936,\n",
       "   90.5193,\n",
       "   90.55533,\n",
       "   90.65209,\n",
       "   90.73218,\n",
       "   90.67293,\n",
       "   90.7563,\n",
       "   90.85187,\n",
       "   91.04421,\n",
       "   91.00223,\n",
       "   91.02515,\n",
       "   91.15288,\n",
       "   91.17789,\n",
       "   91.23267,\n",
       "   91.34432,\n",
       "   91.38124,\n",
       "   91.36397,\n",
       "   91.35951,\n",
       "   91.43334,\n",
       "   91.52415,\n",
       "   91.58727,\n",
       "   91.69713,\n",
       "   91.80253,\n",
       "   91.76651,\n",
       "   91.82456,\n",
       "   91.89989,\n",
       "   92.08419,\n",
       "   92.01333,\n",
       "   92.02792,\n",
       "   92.0961,\n",
       "   92.21221,\n",
       "   92.20715,\n",
       "   92.29975,\n",
       "   92.31433,\n",
       "   92.33785,\n",
       "   92.32773,\n",
       "   92.38043,\n",
       "   92.41169,\n",
       "   92.48136,\n",
       "   92.44117,\n",
       "   92.53406,\n",
       "   92.57604,\n",
       "   92.52721,\n",
       "   92.61296,\n",
       "   92.56681,\n",
       "   92.56413,\n",
       "   2.89364,\n",
       "   37.897,\n",
       "   37.19779,\n",
       "   36.39102,\n",
       "   37.36722,\n",
       "   36.8213,\n",
       "   35.06521,\n",
       "   35.10017,\n",
       "   35.55735,\n",
       "   35.18623,\n",
       "   35.17816,\n",
       "   35.81283,\n",
       "   35.00874,\n",
       "   35.81821,\n",
       "   35.38255,\n",
       "   34.97109,\n",
       "   35.43902,\n",
       "   36.28883,\n",
       "   35.45785,\n",
       "   34.65645,\n",
       "   35.32876,\n",
       "   35.21581,\n",
       "   35.02756,\n",
       "   35.7456,\n",
       "   35.81014,\n",
       "   35.25615,\n",
       "   35.35834,\n",
       "   34.55157,\n",
       "   35.23195,\n",
       "   34.47358,\n",
       "   35.1943,\n",
       "   34.81511,\n",
       "   35.44709,\n",
       "   35.9446,\n",
       "   35.92578,\n",
       "   35.57348,\n",
       "   35.19968,\n",
       "   35.59231,\n",
       "   35.26691,\n",
       "   35.47667,\n",
       "   35.48743,\n",
       "   34.19658,\n",
       "   35.76442,\n",
       "   35.43902,\n",
       "   35.46322,\n",
       "   35.0948,\n",
       "   34.91193,\n",
       "   35.52508,\n",
       "   35.47667,\n",
       "   35.43095,\n",
       "   35.2696,\n",
       "   35.47129,\n",
       "   35.38793,\n",
       "   35.79938,\n",
       "   35.19968,\n",
       "   35.62727,\n",
       "   34.89041,\n",
       "   34.9442,\n",
       "   37.79481,\n",
       "   37.64152,\n",
       "   37.93734,\n",
       "   37.58774,\n",
       "   37.63883,\n",
       "   37.31881,\n",
       "   37.52319,\n",
       "   37.48286,\n",
       "   37.99113,\n",
       "   37.11174,\n",
       "   37.30805,\n",
       "   37.43176,\n",
       "   37.5474,\n",
       "   37.55278,\n",
       "   37.57967,\n",
       "   37.42369,\n",
       "   37.66841,\n",
       "   37.5716,\n",
       "   37.36453,\n",
       "   37.30537,\n",
       "   37.64421,\n",
       "   37.70875,\n",
       "   37.53126,\n",
       "   37.58774,\n",
       "   37.62001,\n",
       "   37.70069,\n",
       "   37.5958,\n",
       "   37.57698,\n",
       "   37.55009,\n",
       "   37.5716,\n",
       "   37.62808,\n",
       "   37.56622,\n",
       "   37.62001,\n",
       "   37.5958,\n",
       "   37.60387,\n",
       "   37.63077,\n",
       "   37.59312,\n",
       "   37.66035,\n",
       "   37.64421,\n",
       "   37.57429,\n",
       "   37.58774],\n",
       "  'train_loss': [1.03584,\n",
       "   0.6718080000000001,\n",
       "   0.553628,\n",
       "   0.519374,\n",
       "   0.480992,\n",
       "   0.456032,\n",
       "   0.43931000000000003,\n",
       "   0.45000599999999996,\n",
       "   0.40984999999999994,\n",
       "   0.40216799999999997,\n",
       "   0.383768,\n",
       "   0.362028,\n",
       "   0.342076,\n",
       "   0.33251,\n",
       "   0.35349250000000004,\n",
       "   0.3308025,\n",
       "   0.31453800000000004,\n",
       "   0.32644249999999997,\n",
       "   0.3187175,\n",
       "   0.320095,\n",
       "   0.310502,\n",
       "   0.316625,\n",
       "   0.3027925,\n",
       "   0.2938175,\n",
       "   0.2865275,\n",
       "   0.28348,\n",
       "   0.2829175,\n",
       "   0.27993499999999993,\n",
       "   0.2704075,\n",
       "   0.25969250000000005,\n",
       "   0.26922999999999997,\n",
       "   0.26454500000000003,\n",
       "   0.268295,\n",
       "   0.28402499999999997,\n",
       "   0.25909499999999996,\n",
       "   0.27009800000000006,\n",
       "   0.27018,\n",
       "   0.27139749999999996,\n",
       "   0.2684925,\n",
       "   0.25057399999999996,\n",
       "   0.2446175,\n",
       "   0.24542,\n",
       "   0.2453875,\n",
       "   0.250922,\n",
       "   0.25169199999999997,\n",
       "   0.23975399999999997,\n",
       "   0.232956,\n",
       "   0.234595,\n",
       "   0.23995599999999997,\n",
       "   0.23100800000000002,\n",
       "   0.229774,\n",
       "   0.229504,\n",
       "   0.2350775,\n",
       "   0.22185,\n",
       "   0.22474800000000003,\n",
       "   0.229936,\n",
       "   0.227002,\n",
       "   0.2283,\n",
       "   0.217476,\n",
       "   0.212352,\n",
       "   0.21799999999999997,\n",
       "   0.216488,\n",
       "   0.202006,\n",
       "   0.213076,\n",
       "   0.20273600000000003,\n",
       "   0.209998,\n",
       "   0.214956,\n",
       "   0.205594,\n",
       "   0.20728400000000002,\n",
       "   0.20107,\n",
       "   0.199406,\n",
       "   0.199036,\n",
       "   0.208488,\n",
       "   0.19563000000000003,\n",
       "   0.19492199999999998,\n",
       "   0.20053,\n",
       "   0.18797000000000003,\n",
       "   0.189646,\n",
       "   0.18178999999999998,\n",
       "   0.18178799999999998,\n",
       "   0.182552,\n",
       "   0.18343,\n",
       "   0.17851999999999998,\n",
       "   0.178682,\n",
       "   0.19037999999999997,\n",
       "   0.179528,\n",
       "   0.17673999999999998,\n",
       "   0.176726,\n",
       "   0.178058,\n",
       "   0.174282,\n",
       "   0.16886600000000002,\n",
       "   0.17369250000000003,\n",
       "   0.176878,\n",
       "   0.17820200000000003,\n",
       "   0.16530999999999998,\n",
       "   0.17332599999999998,\n",
       "   0.167838,\n",
       "   0.170382,\n",
       "   0.164426],\n",
       "  'val_loss': [1.045392,\n",
       "   0.6639039999999999,\n",
       "   0.5634939999999999,\n",
       "   0.515934,\n",
       "   0.48667599999999994,\n",
       "   0.46064799999999995,\n",
       "   0.45520250000000007,\n",
       "   0.436564,\n",
       "   0.41316600000000003,\n",
       "   0.40877800000000003,\n",
       "   0.389586,\n",
       "   0.362142,\n",
       "   0.352072,\n",
       "   0.35587,\n",
       "   0.3426375,\n",
       "   0.33873,\n",
       "   0.33140200000000003,\n",
       "   0.3081375,\n",
       "   0.32339249999999997,\n",
       "   0.3239825,\n",
       "   0.309874,\n",
       "   0.31838500000000003,\n",
       "   0.30888750000000004,\n",
       "   0.30907,\n",
       "   0.29895499999999997,\n",
       "   0.303186,\n",
       "   0.29457500000000003,\n",
       "   0.28868,\n",
       "   0.28690499999999997,\n",
       "   0.279905,\n",
       "   0.282254,\n",
       "   0.27720500000000003,\n",
       "   0.275705,\n",
       "   0.262835,\n",
       "   0.27254249999999997,\n",
       "   0.28425599999999995,\n",
       "   0.27988749999999996,\n",
       "   0.2733,\n",
       "   0.2586975,\n",
       "   0.27294,\n",
       "   0.24498999999999999,\n",
       "   0.2543075,\n",
       "   0.2694875,\n",
       "   0.277344,\n",
       "   0.27830199999999994,\n",
       "   0.25330400000000003,\n",
       "   0.252858,\n",
       "   0.252745,\n",
       "   0.254072,\n",
       "   0.24414000000000002,\n",
       "   0.243316,\n",
       "   0.236808,\n",
       "   0.24467999999999998,\n",
       "   0.24498,\n",
       "   0.24214600000000003,\n",
       "   0.239792,\n",
       "   0.241534,\n",
       "   0.24323200000000003,\n",
       "   0.23465399999999997,\n",
       "   0.23865600000000003,\n",
       "   0.232776,\n",
       "   0.234022,\n",
       "   0.22026400000000002,\n",
       "   0.22990399999999997,\n",
       "   0.23301600000000003,\n",
       "   0.216312,\n",
       "   0.22964400000000001,\n",
       "   0.21806999999999999,\n",
       "   0.21246,\n",
       "   0.21379800000000002,\n",
       "   0.22871999999999998,\n",
       "   0.221628,\n",
       "   0.22189199999999998,\n",
       "   0.213664,\n",
       "   0.20051199999999997,\n",
       "   0.207478,\n",
       "   0.20977,\n",
       "   0.213612,\n",
       "   0.20645800000000003,\n",
       "   0.210352,\n",
       "   0.19589,\n",
       "   0.197548,\n",
       "   0.20878999999999998,\n",
       "   0.20161,\n",
       "   0.20274199999999998,\n",
       "   0.205986,\n",
       "   0.19337200000000002,\n",
       "   0.205248,\n",
       "   0.20173000000000002,\n",
       "   0.20659199999999997,\n",
       "   0.20358799999999996,\n",
       "   0.19800500000000001,\n",
       "   0.18571000000000001,\n",
       "   0.201552,\n",
       "   0.191208,\n",
       "   0.185042,\n",
       "   0.1863,\n",
       "   0.197316,\n",
       "   0.19075999999999999],\n",
       "  'test_acc': 38.621},\n",
       " 'darts_higgs-small': {'train_acc': [48.01594,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653],\n",
       "  'val_acc': [47.99979,\n",
       "   47.229,\n",
       "   46.96568,\n",
       "   46.64307,\n",
       "   46.86628,\n",
       "   47.08426,\n",
       "   47.29527,\n",
       "   47.33887,\n",
       "   47.39293,\n",
       "   47.4714,\n",
       "   46.91511,\n",
       "   47.20285,\n",
       "   47.20982,\n",
       "   47.03718,\n",
       "   47.17843,\n",
       "   46.85582,\n",
       "   47.32492,\n",
       "   46.9308,\n",
       "   47.51674,\n",
       "   47.29701,\n",
       "   47.3999,\n",
       "   47.14355,\n",
       "   47.27783,\n",
       "   47.09996,\n",
       "   47.19587,\n",
       "   47.27958,\n",
       "   47.12088,\n",
       "   47.28655,\n",
       "   46.98661,\n",
       "   47.09821,\n",
       "   46.9465,\n",
       "   47.25865,\n",
       "   46.83489,\n",
       "   46.99358,\n",
       "   46.92034,\n",
       "   47.16797,\n",
       "   47.16797,\n",
       "   47.12263,\n",
       "   47.01625,\n",
       "   47.30748,\n",
       "   47.36677,\n",
       "   47.10519,\n",
       "   46.7756,\n",
       "   47.38421,\n",
       "   46.99881,\n",
       "   47.02323,\n",
       "   47.19761,\n",
       "   47.12263,\n",
       "   47.05985,\n",
       "   46.92906,\n",
       "   47.10519,\n",
       "   47.0738,\n",
       "   47.27958,\n",
       "   46.86105,\n",
       "   47.37723,\n",
       "   46.99881,\n",
       "   46.732,\n",
       "   47.08949,\n",
       "   47.2883,\n",
       "   47.2168,\n",
       "   47.30748,\n",
       "   46.78955,\n",
       "   47.37723,\n",
       "   47.018,\n",
       "   46.72677,\n",
       "   47.08949,\n",
       "   47.16274,\n",
       "   47.16622,\n",
       "   46.86977,\n",
       "   46.9308,\n",
       "   46.92906,\n",
       "   46.86628,\n",
       "   47.19238,\n",
       "   47.39641,\n",
       "   47.1732,\n",
       "   46.98486,\n",
       "   47.01974,\n",
       "   46.85931,\n",
       "   47.49233,\n",
       "   47.04415,\n",
       "   47.16971,\n",
       "   47.03718,\n",
       "   46.99881,\n",
       "   47.03369,\n",
       "   47.14181,\n",
       "   47.05636,\n",
       "   47.16622,\n",
       "   47.35107,\n",
       "   47.3005,\n",
       "   47.37898,\n",
       "   47.24819,\n",
       "   46.72154,\n",
       "   47.40513,\n",
       "   46.91685,\n",
       "   47.10519,\n",
       "   47.16448,\n",
       "   47.03892,\n",
       "   47.18366,\n",
       "   47.17669,\n",
       "   47.17843,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006],\n",
       "  'train_loss': [0.76073,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315],\n",
       "  'val_loss': [0.76704,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315],\n",
       "  'test_acc': 47.144},\n",
       " 'darts_otto': {'train_acc': [31.19809,\n",
       "   39.02637,\n",
       "   41.05499,\n",
       "   42.06229,\n",
       "   42.53648,\n",
       "   42.79181,\n",
       "   43.03872,\n",
       "   53.51852,\n",
       "   63.86083,\n",
       "   64.13861,\n",
       "   66.22615,\n",
       "   68.61672,\n",
       "   68.82435,\n",
       "   69.03199,\n",
       "   69.21717,\n",
       "   69.38833,\n",
       "   69.33502,\n",
       "   69.61279,\n",
       "   69.59596,\n",
       "   69.77834,\n",
       "   69.86532,\n",
       "   69.96633,\n",
       "   70.06453,\n",
       "   70.15152,\n",
       "   70.26655,\n",
       "   70.15993,\n",
       "   70.34792,\n",
       "   70.26375,\n",
       "   70.289,\n",
       "   70.31706,\n",
       "   70.50505,\n",
       "   70.40965,\n",
       "   70.79686,\n",
       "   70.73232,\n",
       "   70.89787,\n",
       "   70.82772,\n",
       "   70.87262,\n",
       "   70.94837,\n",
       "   70.90629,\n",
       "   70.97363,\n",
       "   71.02694,\n",
       "   71.07744,\n",
       "   71.00449,\n",
       "   71.17284,\n",
       "   71.22896,\n",
       "   71.25982,\n",
       "   71.22896,\n",
       "   71.36925,\n",
       "   71.22896,\n",
       "   71.41695,\n",
       "   71.38889,\n",
       "   71.39731,\n",
       "   71.40853,\n",
       "   71.59091,\n",
       "   71.5881,\n",
       "   71.64141,\n",
       "   71.7312,\n",
       "   71.62458,\n",
       "   71.81818,\n",
       "   71.82099,\n",
       "   71.93883,\n",
       "   71.86308,\n",
       "   71.75365,\n",
       "   71.83502,\n",
       "   71.95286,\n",
       "   71.86869,\n",
       "   72.03704,\n",
       "   71.922,\n",
       "   72.08474,\n",
       "   72.09035,\n",
       "   72.0651,\n",
       "   72.09315,\n",
       "   72.24467,\n",
       "   72.18855,\n",
       "   72.19136,\n",
       "   72.25309,\n",
       "   72.26431,\n",
       "   72.19978,\n",
       "   72.20539,\n",
       "   72.18855,\n",
       "   72.25589,\n",
       "   72.31762,\n",
       "   72.25309,\n",
       "   72.39338,\n",
       "   72.36251,\n",
       "   72.32323,\n",
       "   72.3541,\n",
       "   72.28395,\n",
       "   72.57576,\n",
       "   72.49158,\n",
       "   72.53367,\n",
       "   72.48317,\n",
       "   72.42705,\n",
       "   72.49158,\n",
       "   72.4523,\n",
       "   72.48878,\n",
       "   72.57015,\n",
       "   72.44669,\n",
       "   72.43827,\n",
       "   72.48878,\n",
       "   20.46857,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514,\n",
       "   26.12514],\n",
       "  'val_acc': [31.72743,\n",
       "   39.68641,\n",
       "   41.40354,\n",
       "   42.21734,\n",
       "   42.61068,\n",
       "   42.93349,\n",
       "   43.09625,\n",
       "   53.2742,\n",
       "   63.55794,\n",
       "   63.85634,\n",
       "   65.91254,\n",
       "   68.14507,\n",
       "   68.63878,\n",
       "   68.72016,\n",
       "   68.82867,\n",
       "   69.04839,\n",
       "   69.14334,\n",
       "   68.71474,\n",
       "   69.59364,\n",
       "   69.60992,\n",
       "   69.56651,\n",
       "   69.35221,\n",
       "   69.23557,\n",
       "   69.34136,\n",
       "   69.62619,\n",
       "   69.6072,\n",
       "   69.7998,\n",
       "   69.54753,\n",
       "   69.92188,\n",
       "   69.9273,\n",
       "   69.73741,\n",
       "   69.83778,\n",
       "   69.63162,\n",
       "   70.07921,\n",
       "   69.84592,\n",
       "   69.91374,\n",
       "   69.713,\n",
       "   69.90289,\n",
       "   70.00054,\n",
       "   69.9273,\n",
       "   69.68859,\n",
       "   70.23383,\n",
       "   70.01139,\n",
       "   69.64518,\n",
       "   69.8839,\n",
       "   69.78353,\n",
       "   69.94358,\n",
       "   69.78082,\n",
       "   69.91374,\n",
       "   69.85406,\n",
       "   69.83778,\n",
       "   69.79709,\n",
       "   69.93544,\n",
       "   69.87847,\n",
       "   70.06565,\n",
       "   69.73199,\n",
       "   69.76997,\n",
       "   70.23112,\n",
       "   70.22569,\n",
       "   69.9924,\n",
       "   69.94358,\n",
       "   69.75098,\n",
       "   69.84049,\n",
       "   69.95171,\n",
       "   70.08735,\n",
       "   70.15788,\n",
       "   70.21484,\n",
       "   70.15516,\n",
       "   70.03852,\n",
       "   70.22298,\n",
       "   70.28809,\n",
       "   70.15245,\n",
       "   70.13346,\n",
       "   70.21213,\n",
       "   70.22841,\n",
       "   70.53765,\n",
       "   70.36133,\n",
       "   70.30436,\n",
       "   70.18229,\n",
       "   70.38032,\n",
       "   70.36133,\n",
       "   70.51053,\n",
       "   70.42372,\n",
       "   70.30436,\n",
       "   70.22569,\n",
       "   70.69499,\n",
       "   70.53765,\n",
       "   70.22569,\n",
       "   70.53223,\n",
       "   70.44813,\n",
       "   70.28266,\n",
       "   70.51324,\n",
       "   70.44,\n",
       "   70.4834,\n",
       "   70.45898,\n",
       "   70.46441,\n",
       "   70.676,\n",
       "   70.80078,\n",
       "   70.45356,\n",
       "   70.66515,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287],\n",
       "  'train_loss': [2.27183,\n",
       "   1.51823,\n",
       "   1.50763,\n",
       "   1.33742,\n",
       "   0.96902,\n",
       "   0.87143,\n",
       "   0.86138,\n",
       "   0.83068,\n",
       "   0.81062,\n",
       "   0.78049,\n",
       "   0.81075,\n",
       "   0.80883,\n",
       "   0.79703,\n",
       "   0.80222,\n",
       "   0.78735,\n",
       "   0.80891,\n",
       "   0.78992,\n",
       "   0.75034,\n",
       "   0.78313,\n",
       "   0.75242,\n",
       "   0.80683,\n",
       "   0.76528,\n",
       "   0.75393,\n",
       "   0.74552,\n",
       "   0.76745,\n",
       "   0.74677,\n",
       "   0.72318,\n",
       "   0.69055,\n",
       "   0.74881,\n",
       "   0.7606,\n",
       "   0.75401,\n",
       "   0.72869,\n",
       "   0.70556,\n",
       "   0.74759,\n",
       "   0.72925,\n",
       "   0.73719,\n",
       "   0.73284,\n",
       "   0.76457,\n",
       "   0.75996,\n",
       "   0.75191,\n",
       "   0.74043],\n",
       "  'val_loss': [2.27219,\n",
       "   1.50627,\n",
       "   1.45705,\n",
       "   1.44046,\n",
       "   0.9847,\n",
       "   0.91536,\n",
       "   0.87113,\n",
       "   0.86421,\n",
       "   0.83841,\n",
       "   0.86073,\n",
       "   0.82078,\n",
       "   0.81861,\n",
       "   0.83946,\n",
       "   0.85712,\n",
       "   0.80918,\n",
       "   0.82123,\n",
       "   0.84452,\n",
       "   0.82944,\n",
       "   0.84063,\n",
       "   0.84342,\n",
       "   0.8192,\n",
       "   0.84448,\n",
       "   0.83761,\n",
       "   0.83548,\n",
       "   0.84639,\n",
       "   0.81971,\n",
       "   0.81814,\n",
       "   0.83831,\n",
       "   0.81664,\n",
       "   0.79435,\n",
       "   0.84909,\n",
       "   0.82283,\n",
       "   0.83302,\n",
       "   0.83732,\n",
       "   0.84189,\n",
       "   0.82202,\n",
       "   0.81157,\n",
       "   0.8498,\n",
       "   0.83136,\n",
       "   0.82129,\n",
       "   0.82463],\n",
       "  'test_acc': 26.05},\n",
       " 'darts_adult': {'train_acc': [71.47549,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.016,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297],\n",
       "  'val_acc': [71.39486,\n",
       "   75.30924,\n",
       "   75.51676,\n",
       "   75.41504,\n",
       "   75.43538,\n",
       "   75.47201,\n",
       "   75.5778,\n",
       "   75.24007,\n",
       "   75.52897,\n",
       "   75.67546,\n",
       "   75.41911,\n",
       "   75.43538,\n",
       "   75.49235,\n",
       "   75.60221,\n",
       "   75.61849,\n",
       "   75.39469,\n",
       "   75.54932,\n",
       "   75.65104,\n",
       "   75.43538,\n",
       "   75.354,\n",
       "   75.46387,\n",
       "   75.50456,\n",
       "   75.62256,\n",
       "   75.62256,\n",
       "   75.26042,\n",
       "   75.32552,\n",
       "   75.67139,\n",
       "   75.59814,\n",
       "   75.60221,\n",
       "   75.58187,\n",
       "   75.5127,\n",
       "   75.48828,\n",
       "   75.57373,\n",
       "   75.38656,\n",
       "   75.71615,\n",
       "   75.69987,\n",
       "   75.61442,\n",
       "   75.40283,\n",
       "   75.51676,\n",
       "   75.50456,\n",
       "   75.50456,\n",
       "   75.57373,\n",
       "   75.67139,\n",
       "   75.50863,\n",
       "   75.51676,\n",
       "   75.65511,\n",
       "   75.53304,\n",
       "   75.5249,\n",
       "   75.46387,\n",
       "   75.25228,\n",
       "   75.56559,\n",
       "   75.60221,\n",
       "   75.55339,\n",
       "   75.49642,\n",
       "   75.54118,\n",
       "   75.25635,\n",
       "   75.55339,\n",
       "   75.48014,\n",
       "   75.54932,\n",
       "   75.57373,\n",
       "   75.49235,\n",
       "   75.47201,\n",
       "   75.26449,\n",
       "   75.37435,\n",
       "   75.83822,\n",
       "   75.45166,\n",
       "   75.74463,\n",
       "   75.44759,\n",
       "   75.65918,\n",
       "   75.37028,\n",
       "   75.67546,\n",
       "   75.5249,\n",
       "   75.80973,\n",
       "   75.68766,\n",
       "   75.43945,\n",
       "   75.50049,\n",
       "   75.67952,\n",
       "   75.39062,\n",
       "   75.5127,\n",
       "   75.55339,\n",
       "   75.54525,\n",
       "   75.30111,\n",
       "   75.31331,\n",
       "   75.71208,\n",
       "   75.22786,\n",
       "   75.60221,\n",
       "   75.39062,\n",
       "   75.59408,\n",
       "   75.58187,\n",
       "   75.48014,\n",
       "   75.48828,\n",
       "   75.53711,\n",
       "   75.48828,\n",
       "   75.59814,\n",
       "   75.47201,\n",
       "   75.38656,\n",
       "   75.59408,\n",
       "   75.32552,\n",
       "   75.56152,\n",
       "   75.47607,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864],\n",
       "  'train_loss': [1.20772,\n",
       "   0.3793,\n",
       "   0.38491,\n",
       "   0.37229,\n",
       "   0.39245,\n",
       "   0.40602,\n",
       "   0.39913,\n",
       "   0.40372,\n",
       "   0.39307,\n",
       "   0.38515,\n",
       "   0.37947,\n",
       "   0.38791,\n",
       "   0.38515,\n",
       "   0.40381,\n",
       "   0.39718,\n",
       "   0.39011,\n",
       "   0.39416,\n",
       "   0.37082,\n",
       "   0.39308,\n",
       "   0.37895,\n",
       "   0.36788,\n",
       "   0.37917,\n",
       "   0.38911,\n",
       "   0.36512,\n",
       "   0.37718],\n",
       "  'val_loss': [1.19492,\n",
       "   0.40743,\n",
       "   0.41047,\n",
       "   0.40469,\n",
       "   0.3989,\n",
       "   0.39716,\n",
       "   0.39623,\n",
       "   0.40162,\n",
       "   0.40869,\n",
       "   0.3997,\n",
       "   0.41222,\n",
       "   0.39587,\n",
       "   0.40075,\n",
       "   0.40467,\n",
       "   0.40318,\n",
       "   0.40928,\n",
       "   0.39276,\n",
       "   0.39569,\n",
       "   0.39878,\n",
       "   0.39399,\n",
       "   0.40564,\n",
       "   0.40951,\n",
       "   0.4046,\n",
       "   0.39531,\n",
       "   0.40477],\n",
       "  'test_acc': 76.377},\n",
       " 'darts_churn': {'train_acc': [79.09722,\n",
       "   79.91319,\n",
       "   79.96528,\n",
       "   79.94792,\n",
       "   79.80903,\n",
       "   79.87847,\n",
       "   80.17361,\n",
       "   80.15625,\n",
       "   80.06944,\n",
       "   80.22569,\n",
       "   80.55555,\n",
       "   80.74653,\n",
       "   80.97222,\n",
       "   81.11111,\n",
       "   81.35417,\n",
       "   81.28472,\n",
       "   81.94444,\n",
       "   82.25694,\n",
       "   82.10069,\n",
       "   82.56944,\n",
       "   83.00347,\n",
       "   82.67361,\n",
       "   83.21181,\n",
       "   83.35069,\n",
       "   83.87153,\n",
       "   83.47222,\n",
       "   84.01042,\n",
       "   84.07986,\n",
       "   84.16667,\n",
       "   84.32291,\n",
       "   84.67014,\n",
       "   84.54861,\n",
       "   84.75694,\n",
       "   84.94792,\n",
       "   84.60069,\n",
       "   84.86111,\n",
       "   85.12153,\n",
       "   85.19097,\n",
       "   85.27778,\n",
       "   85.24305,\n",
       "   85.20833,\n",
       "   85.48611,\n",
       "   85.52083,\n",
       "   85.57292,\n",
       "   85.69444,\n",
       "   85.64236,\n",
       "   85.90278,\n",
       "   85.78125,\n",
       "   85.85069,\n",
       "   85.85069,\n",
       "   85.9375,\n",
       "   86.05903,\n",
       "   85.90278,\n",
       "   85.97222,\n",
       "   86.16319,\n",
       "   86.16319,\n",
       "   86.18055,\n",
       "   86.18056,\n",
       "   86.25,\n",
       "   86.28472,\n",
       "   86.3368,\n",
       "   86.37153,\n",
       "   86.28472,\n",
       "   86.38889,\n",
       "   86.42361,\n",
       "   86.21528,\n",
       "   86.3368,\n",
       "   86.35417,\n",
       "   86.35417,\n",
       "   86.42361,\n",
       "   86.31944,\n",
       "   86.40625,\n",
       "   86.45833,\n",
       "   86.35417,\n",
       "   86.40625,\n",
       "   86.35417,\n",
       "   86.38889,\n",
       "   86.44097,\n",
       "   86.44097,\n",
       "   86.37153,\n",
       "   86.44097,\n",
       "   86.47569,\n",
       "   86.31944,\n",
       "   86.52778,\n",
       "   86.35416,\n",
       "   86.26736,\n",
       "   86.49306,\n",
       "   86.42361,\n",
       "   86.42361,\n",
       "   86.45833,\n",
       "   86.45833,\n",
       "   86.37153,\n",
       "   86.35417,\n",
       "   86.37153,\n",
       "   86.44097,\n",
       "   86.45833,\n",
       "   86.42361,\n",
       "   86.40625,\n",
       "   86.44097,\n",
       "   86.31944,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528],\n",
       "  'val_acc': [76.04167,\n",
       "   76.45833,\n",
       "   76.40625,\n",
       "   76.5625,\n",
       "   76.45833,\n",
       "   76.82292,\n",
       "   76.92708,\n",
       "   77.08333,\n",
       "   76.92708,\n",
       "   77.29167,\n",
       "   77.13542,\n",
       "   77.96875,\n",
       "   78.17708,\n",
       "   78.38542,\n",
       "   78.59375,\n",
       "   79.01042,\n",
       "   79.42708,\n",
       "   79.21875,\n",
       "   79.47917,\n",
       "   79.58333,\n",
       "   80.0,\n",
       "   79.63542,\n",
       "   80.20833,\n",
       "   80.10417,\n",
       "   80.83333,\n",
       "   80.46875,\n",
       "   80.78125,\n",
       "   80.9375,\n",
       "   81.04167,\n",
       "   80.72917,\n",
       "   80.88542,\n",
       "   81.25,\n",
       "   81.19792,\n",
       "   81.19792,\n",
       "   81.19792,\n",
       "   81.61458,\n",
       "   81.5625,\n",
       "   81.92708,\n",
       "   81.875,\n",
       "   81.66667,\n",
       "   81.77083,\n",
       "   81.61458,\n",
       "   81.45833,\n",
       "   81.5625,\n",
       "   81.77083,\n",
       "   81.71875,\n",
       "   81.51042,\n",
       "   81.51042,\n",
       "   81.61458,\n",
       "   81.5625,\n",
       "   81.82292,\n",
       "   81.82292,\n",
       "   81.66667,\n",
       "   81.66667,\n",
       "   81.71875,\n",
       "   81.77083,\n",
       "   81.77083,\n",
       "   81.82292,\n",
       "   81.77083,\n",
       "   81.92708,\n",
       "   81.97917,\n",
       "   81.97917,\n",
       "   82.03125,\n",
       "   82.1875,\n",
       "   81.97917,\n",
       "   82.23958,\n",
       "   82.39583,\n",
       "   82.44792,\n",
       "   82.08333,\n",
       "   82.03125,\n",
       "   82.1875,\n",
       "   82.23958,\n",
       "   82.23958,\n",
       "   82.39583,\n",
       "   82.34375,\n",
       "   82.34375,\n",
       "   82.34375,\n",
       "   82.23958,\n",
       "   82.29167,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   82.13542,\n",
       "   82.1875,\n",
       "   82.08333,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   81.92708,\n",
       "   82.03125,\n",
       "   82.03125,\n",
       "   82.03125,\n",
       "   82.03125,\n",
       "   82.03125,\n",
       "   82.03125,\n",
       "   82.13542,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   82.1875,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625],\n",
       "  'train_loss': [0.65316, 0.41608, 0.38322, 0.34424, 0.3263, 0.33479],\n",
       "  'val_loss': [0.66589, 0.48198, 0.43744, 0.42778, 0.42192, 0.41891],\n",
       "  'test_acc': 79.65},\n",
       " 'gdas_covtype': {'train_acc': [29.21844,\n",
       "   30.54843,\n",
       "   38.60462,\n",
       "   36.75559,\n",
       "   36.30828,\n",
       "   37.48618,\n",
       "   35.53645,\n",
       "   33.10773,\n",
       "   35.14561,\n",
       "   37.13478,\n",
       "   33.20604,\n",
       "   29.66306,\n",
       "   33.7442,\n",
       "   33.88822,\n",
       "   35.8015,\n",
       "   34.75058,\n",
       "   33.90555,\n",
       "   35.47041,\n",
       "   36.09044,\n",
       "   37.02303,\n",
       "   34.24649,\n",
       "   37.41984,\n",
       "   36.07012,\n",
       "   34.97887,\n",
       "   35.63476,\n",
       "   37.91378,\n",
       "   36.31933,\n",
       "   33.95515,\n",
       "   37.64724,\n",
       "   33.60674,\n",
       "   33.25744,\n",
       "   34.53425,\n",
       "   37.08398,\n",
       "   35.47131,\n",
       "   36.27003,\n",
       "   36.43228,\n",
       "   36.29841,\n",
       "   35.08376,\n",
       "   36.06265,\n",
       "   36.31096,\n",
       "   40.78593,\n",
       "   41.55745,\n",
       "   41.20874,\n",
       "   41.3196,\n",
       "   41.95487,\n",
       "   41.75228,\n",
       "   42.73715,\n",
       "   42.29611,\n",
       "   42.70368,\n",
       "   41.86523,\n",
       "   40.50086,\n",
       "   42.90269,\n",
       "   44.23956,\n",
       "   45.26029,\n",
       "   44.11436,\n",
       "   43.52929,\n",
       "   45.32962,\n",
       "   45.41567,\n",
       "   44.00858,\n",
       "   46.57117,\n",
       "   47.35464,\n",
       "   43.71246,\n",
       "   46.10413,\n",
       "   45.62245,\n",
       "   44.91248,\n",
       "   46.49079,\n",
       "   45.52534,\n",
       "   45.27464,\n",
       "   44.65969,\n",
       "   47.12038,\n",
       "   47.58861,\n",
       "   46.90852,\n",
       "   49.08355,\n",
       "   46.91629,\n",
       "   47.86083,\n",
       "   47.99858,\n",
       "   48.34938,\n",
       "   48.77279,\n",
       "   48.46233,\n",
       "   48.71273,\n",
       "   48.7193,\n",
       "   48.22268,\n",
       "   47.99828,\n",
       "   47.73682,\n",
       "   48.01591,\n",
       "   48.89441,\n",
       "   49.49412,\n",
       "   49.60677,\n",
       "   49.42449,\n",
       "   49.83506,\n",
       "   48.98704,\n",
       "   49.83894,\n",
       "   48.3195,\n",
       "   49.51593,\n",
       "   48.15545,\n",
       "   49.35009,\n",
       "   49.65308,\n",
       "   48.89202,\n",
       "   49.19142,\n",
       "   50.39562,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954,\n",
       "   48.6954],\n",
       "  'val_acc': [30.6194,\n",
       "   31.65224,\n",
       "   35.82645,\n",
       "   32.84912,\n",
       "   37.76141,\n",
       "   37.72419,\n",
       "   37.76141,\n",
       "   35.92321,\n",
       "   33.56547,\n",
       "   39.1021,\n",
       "   37.06799,\n",
       "   34.97166,\n",
       "   35.65674,\n",
       "   37.05161,\n",
       "   34.7421,\n",
       "   33.86469,\n",
       "   33.88463,\n",
       "   37.49881,\n",
       "   34.30354,\n",
       "   35.74725,\n",
       "   34.8088,\n",
       "   36.52106,\n",
       "   35.1634,\n",
       "   36.39482,\n",
       "   34.69179,\n",
       "   35.70944,\n",
       "   36.3445,\n",
       "   36.07208,\n",
       "   37.07246,\n",
       "   36.19742,\n",
       "   30.44374,\n",
       "   38.45126,\n",
       "   36.37398,\n",
       "   36.90334,\n",
       "   38.45036,\n",
       "   37.06769,\n",
       "   34.86388,\n",
       "   34.81892,\n",
       "   38.25743,\n",
       "   39.24084,\n",
       "   43.32067,\n",
       "   40.02686,\n",
       "   40.80007,\n",
       "   40.51008,\n",
       "   39.23995,\n",
       "   42.23305,\n",
       "   42.22621,\n",
       "   42.99019,\n",
       "   44.21595,\n",
       "   45.47387,\n",
       "   41.40357,\n",
       "   44.85072,\n",
       "   41.85761,\n",
       "   45.18507,\n",
       "   46.41173,\n",
       "   45.89367,\n",
       "   46.53618,\n",
       "   43.1069,\n",
       "   44.78611,\n",
       "   46.98308,\n",
       "   44.46069,\n",
       "   47.83727,\n",
       "   44.85935,\n",
       "   44.19987,\n",
       "   43.43083,\n",
       "   45.07878,\n",
       "   45.10319,\n",
       "   44.87334,\n",
       "   48.11684,\n",
       "   46.72673,\n",
       "   47.87151,\n",
       "   47.9909,\n",
       "   48.74059,\n",
       "   48.38034,\n",
       "   48.9889,\n",
       "   50.14023,\n",
       "   48.66884,\n",
       "   48.23623,\n",
       "   49.23006,\n",
       "   49.1988,\n",
       "   49.00944,\n",
       "   49.46914,\n",
       "   48.28923,\n",
       "   49.06006,\n",
       "   50.42099,\n",
       "   49.2518,\n",
       "   49.84042,\n",
       "   49.48701,\n",
       "   50.26022,\n",
       "   49.02701,\n",
       "   50.05091,\n",
       "   48.66586,\n",
       "   49.91008,\n",
       "   50.87414,\n",
       "   50.73927,\n",
       "   49.00438,\n",
       "   49.43312,\n",
       "   49.97707,\n",
       "   49.78891,\n",
       "   49.01897,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979,\n",
       "   49.33979],\n",
       "  'train_loss': [3.652826,\n",
       "   1.691758,\n",
       "   1.70471,\n",
       "   1.727325,\n",
       "   1.336702,\n",
       "   1.4888959999999998,\n",
       "   2.0262925,\n",
       "   1.5973499999999998,\n",
       "   1.173658,\n",
       "   1.3458739999999998,\n",
       "   1.1082625,\n",
       "   1.60767,\n",
       "   1.8019919999999998,\n",
       "   1.302724,\n",
       "   1.40971,\n",
       "   1.3134775000000003,\n",
       "   1.246752,\n",
       "   1.091474,\n",
       "   1.3554049999999997,\n",
       "   1.2441720000000003,\n",
       "   1.4392599999999998,\n",
       "   1.4422819999999998,\n",
       "   1.104475,\n",
       "   1.2847939999999998,\n",
       "   1.2643900000000001,\n",
       "   1.5543559999999998,\n",
       "   1.4148800000000001,\n",
       "   1.3985539999999999,\n",
       "   1.256822,\n",
       "   1.588324,\n",
       "   1.644005,\n",
       "   1.258436,\n",
       "   1.2383700000000002,\n",
       "   1.493515,\n",
       "   1.38242,\n",
       "   1.142134,\n",
       "   1.6670539999999998,\n",
       "   1.2915324999999998,\n",
       "   1.46083,\n",
       "   1.2844039999999999,\n",
       "   1.25637,\n",
       "   1.2741099999999999,\n",
       "   1.2329219999999999,\n",
       "   1.305215,\n",
       "   1.235766,\n",
       "   1.1093119999999999,\n",
       "   1.051656,\n",
       "   1.444392,\n",
       "   1.2594375,\n",
       "   1.241078,\n",
       "   1.219914,\n",
       "   1.4608199999999998,\n",
       "   1.384944,\n",
       "   1.313382,\n",
       "   1.5011550000000002,\n",
       "   1.012578,\n",
       "   1.0763260000000001,\n",
       "   1.081238,\n",
       "   1.200254,\n",
       "   1.2225625,\n",
       "   1.0635499999999998,\n",
       "   0.9948600000000001,\n",
       "   1.235032,\n",
       "   1.2237839999999998,\n",
       "   1.111644,\n",
       "   1.4733399999999999,\n",
       "   1.23264,\n",
       "   1.191524,\n",
       "   1.3466840000000002,\n",
       "   1.2548,\n",
       "   1.1525820000000002,\n",
       "   1.312912,\n",
       "   1.040794,\n",
       "   1.26717,\n",
       "   1.2775400000000001,\n",
       "   1.202296,\n",
       "   1.079208,\n",
       "   1.057396,\n",
       "   1.068724,\n",
       "   1.1131925,\n",
       "   1.152628,\n",
       "   1.0682180000000001,\n",
       "   1.299325,\n",
       "   1.071776,\n",
       "   1.25484,\n",
       "   1.048196,\n",
       "   1.0348959999999998,\n",
       "   1.2254775,\n",
       "   1.102152,\n",
       "   1.036068,\n",
       "   1.2519099999999999,\n",
       "   1.121668,\n",
       "   1.000782,\n",
       "   1.05586,\n",
       "   1.067454,\n",
       "   1.0501859999999998,\n",
       "   1.0483425,\n",
       "   1.001026,\n",
       "   0.998518],\n",
       "  'val_loss': [5.4524099999999995,\n",
       "   1.8778940000000002,\n",
       "   1.669934,\n",
       "   4.6331475,\n",
       "   1.4954260000000001,\n",
       "   1.7243679999999997,\n",
       "   1.8349625,\n",
       "   1.182134,\n",
       "   1.446106,\n",
       "   1.432836,\n",
       "   1.32024,\n",
       "   1.2727799999999998,\n",
       "   1.451728,\n",
       "   1.102754,\n",
       "   1.2569840000000003,\n",
       "   1.3227875,\n",
       "   1.5932319999999998,\n",
       "   1.431972,\n",
       "   1.0608275,\n",
       "   1.4385080000000001,\n",
       "   1.588702,\n",
       "   1.113142,\n",
       "   1.5407425,\n",
       "   1.2723539999999998,\n",
       "   1.3952179999999998,\n",
       "   1.206924,\n",
       "   1.2630219999999999,\n",
       "   1.048164,\n",
       "   1.035748,\n",
       "   1.3578480000000002,\n",
       "   1.5170016666666666,\n",
       "   1.8643460000000005,\n",
       "   1.5796179999999997,\n",
       "   1.4731025,\n",
       "   1.216748,\n",
       "   1.1351239999999998,\n",
       "   1.2527180000000002,\n",
       "   1.067355,\n",
       "   1.2955140000000003,\n",
       "   1.1017480000000002,\n",
       "   4.161794,\n",
       "   1.5287879999999998,\n",
       "   1.444014,\n",
       "   1.7366275,\n",
       "   1.0647600000000002,\n",
       "   1.2785220000000002,\n",
       "   1.1649999999999998,\n",
       "   1.0313459999999999,\n",
       "   1.0686425,\n",
       "   1.135198,\n",
       "   1.1933939999999998,\n",
       "   1.1924480000000002,\n",
       "   1.245032,\n",
       "   1.0555340000000002,\n",
       "   1.031685,\n",
       "   1.0288419999999998,\n",
       "   1.03573,\n",
       "   1.27643,\n",
       "   1.272552,\n",
       "   1.0663225,\n",
       "   1.425436,\n",
       "   1.0963559999999999,\n",
       "   1.324686,\n",
       "   1.121518,\n",
       "   1.065748,\n",
       "   1.0710775,\n",
       "   1.2691620000000001,\n",
       "   1.6201059999999998,\n",
       "   1.253466,\n",
       "   1.0257825,\n",
       "   1.033138,\n",
       "   1.075298,\n",
       "   1.079948,\n",
       "   1.181958,\n",
       "   1.0505200000000001,\n",
       "   1.02589,\n",
       "   1.223724,\n",
       "   1.227448,\n",
       "   1.0249359999999998,\n",
       "   1.1132925,\n",
       "   1.194772,\n",
       "   1.0656079999999999,\n",
       "   0.9890774999999999,\n",
       "   1.216362,\n",
       "   1.0642479999999999,\n",
       "   1.0002739999999999,\n",
       "   1.1102400000000001,\n",
       "   1.0222625,\n",
       "   1.0476740000000002,\n",
       "   1.0166520000000001,\n",
       "   1.036456,\n",
       "   1.0237079999999998,\n",
       "   1.0696660000000002,\n",
       "   1.0939275,\n",
       "   1.0556379999999999,\n",
       "   1.41007,\n",
       "   1.1213375,\n",
       "   1.0469460000000002,\n",
       "   1.113896],\n",
       "  'test_acc': 48.76},\n",
       " 'gdas_higgs-small': {'train_acc': [47.24037,\n",
       "   47.47056,\n",
       "   47.41213,\n",
       "   47.53962,\n",
       "   47.56972,\n",
       "   48.31695,\n",
       "   47.62638,\n",
       "   48.0726,\n",
       "   47.13236,\n",
       "   47.56441,\n",
       "   47.98761,\n",
       "   47.61045,\n",
       "   48.24967,\n",
       "   48.71182,\n",
       "   48.3807,\n",
       "   48.7189,\n",
       "   47.70429,\n",
       "   49.0695,\n",
       "   48.27977,\n",
       "   48.22842,\n",
       "   48.35945,\n",
       "   48.57371,\n",
       "   48.31873,\n",
       "   47.53608,\n",
       "   47.60336,\n",
       "   47.38557,\n",
       "   48.34351,\n",
       "   48.58787,\n",
       "   48.18061,\n",
       "   48.05666,\n",
       "   48.55954,\n",
       "   48.32404,\n",
       "   48.7012,\n",
       "   47.87605,\n",
       "   47.40151,\n",
       "   48.20009,\n",
       "   48.46923,\n",
       "   47.72377,\n",
       "   47.88668,\n",
       "   48.44799,\n",
       "   47.48119,\n",
       "   48.25675,\n",
       "   48.13988,\n",
       "   48.29216,\n",
       "   48.5259,\n",
       "   48.63037,\n",
       "   47.7344,\n",
       "   48.61266,\n",
       "   48.60381,\n",
       "   48.99513,\n",
       "   47.80876,\n",
       "   48.69411,\n",
       "   47.86366,\n",
       "   47.8672,\n",
       "   47.98406,\n",
       "   48.82337,\n",
       "   47.95219,\n",
       "   48.08853,\n",
       "   48.52767,\n",
       "   47.66711,\n",
       "   48.01594,\n",
       "   48.30456,\n",
       "   47.78043,\n",
       "   48.21071,\n",
       "   48.89597,\n",
       "   47.68482,\n",
       "   48.51704,\n",
       "   47.95396,\n",
       "   48.27269,\n",
       "   47.81231,\n",
       "   48.3683,\n",
       "   48.1328,\n",
       "   47.75564,\n",
       "   50.31784,\n",
       "   48.29039,\n",
       "   47.90792,\n",
       "   48.56485,\n",
       "   48.46038,\n",
       "   48.14166,\n",
       "   48.51881,\n",
       "   47.444,\n",
       "   48.56131,\n",
       "   48.37716,\n",
       "   48.38778,\n",
       "   48.22488,\n",
       "   47.68836,\n",
       "   48.0602,\n",
       "   48.19655,\n",
       "   48.10093,\n",
       "   48.41788,\n",
       "   47.87074,\n",
       "   47.71846,\n",
       "   49.31386,\n",
       "   47.76627,\n",
       "   48.27623,\n",
       "   48.31873,\n",
       "   48.02302,\n",
       "   47.6494,\n",
       "   48.56839,\n",
       "   48.35945,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653],\n",
       "  'val_acc': [48.57003,\n",
       "   47.94573,\n",
       "   47.09124,\n",
       "   47.55336,\n",
       "   47.29701,\n",
       "   47.89167,\n",
       "   48.03816,\n",
       "   47.97538,\n",
       "   48.2108,\n",
       "   47.95968,\n",
       "   48.24044,\n",
       "   47.95445,\n",
       "   47.62137,\n",
       "   47.90737,\n",
       "   47.9248,\n",
       "   48.08873,\n",
       "   48.8508,\n",
       "   47.30225,\n",
       "   48.36251,\n",
       "   48.37821,\n",
       "   48.34682,\n",
       "   48.05385,\n",
       "   47.72077,\n",
       "   47.50977,\n",
       "   48.15848,\n",
       "   49.18736,\n",
       "   48.45494,\n",
       "   47.69287,\n",
       "   47.37898,\n",
       "   47.41211,\n",
       "   47.56731,\n",
       "   48.51772,\n",
       "   48.37123,\n",
       "   48.09222,\n",
       "   47.86203,\n",
       "   48.6921,\n",
       "   47.85854,\n",
       "   47.88644,\n",
       "   48.8072,\n",
       "   48.02769,\n",
       "   48.54911,\n",
       "   48.37123,\n",
       "   48.02595,\n",
       "   48.18115,\n",
       "   48.26835,\n",
       "   48.04164,\n",
       "   48.13581,\n",
       "   47.84284,\n",
       "   48.21952,\n",
       "   47.94747,\n",
       "   47.45745,\n",
       "   47.61265,\n",
       "   48.74965,\n",
       "   48.14628,\n",
       "   48.29102,\n",
       "   47.89865,\n",
       "   47.53767,\n",
       "   49.09842,\n",
       "   48.90834,\n",
       "   48.19685,\n",
       "   48.51772,\n",
       "   47.63358,\n",
       "   49.11586,\n",
       "   48.72524,\n",
       "   47.69461,\n",
       "   48.08524,\n",
       "   48.36949,\n",
       "   48.1829,\n",
       "   48.81941,\n",
       "   48.80545,\n",
       "   47.44699,\n",
       "   47.80448,\n",
       "   48.90485,\n",
       "   48.93101,\n",
       "   48.36251,\n",
       "   48.57875,\n",
       "   47.91783,\n",
       "   48.7357,\n",
       "   48.70431,\n",
       "   48.96589,\n",
       "   48.12535,\n",
       "   47.92306,\n",
       "   47.77658,\n",
       "   47.68241,\n",
       "   48.58573,\n",
       "   48.00502,\n",
       "   48.45668,\n",
       "   48.89788,\n",
       "   47.88818,\n",
       "   48.65723,\n",
       "   48.25439,\n",
       "   47.75216,\n",
       "   48.32589,\n",
       "   48.32764,\n",
       "   47.80622,\n",
       "   49.16643,\n",
       "   48.05734,\n",
       "   49.5745,\n",
       "   48.20557,\n",
       "   48.17592,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006],\n",
       "  'train_loss': [3.54437,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69401,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69265,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69217,\n",
       "   0.69315,\n",
       "   0.69032,\n",
       "   0.69206,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69228,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69137,\n",
       "   0.6912,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69147,\n",
       "   0.6904,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315],\n",
       "  'val_loss': [1.62554,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69154,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69121,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69289,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69163,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69003,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69126,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69591,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.68957,\n",
       "   0.69315],\n",
       "  'test_acc': 47.144},\n",
       " 'gdas_otto': {'train_acc': [16.82379,\n",
       "   17.18013,\n",
       "   18.6055,\n",
       "   19.73345,\n",
       "   22.43547,\n",
       "   18.25196,\n",
       "   21.31594,\n",
       "   24.69136,\n",
       "   21.06902,\n",
       "   21.23737,\n",
       "   19.22278,\n",
       "   21.66667,\n",
       "   18.33053,\n",
       "   21.40572,\n",
       "   21.75645,\n",
       "   23.14254,\n",
       "   21.62177,\n",
       "   23.46801,\n",
       "   22.96296,\n",
       "   23.51291,\n",
       "   24.13861,\n",
       "   23.77946,\n",
       "   25.94837,\n",
       "   26.67789,\n",
       "   24.25365,\n",
       "   26.16162,\n",
       "   20.60325,\n",
       "   22.23906,\n",
       "   23.67284,\n",
       "   27.46352,\n",
       "   23.9927,\n",
       "   27.71324,\n",
       "   26.53199,\n",
       "   26.47868,\n",
       "   26.56566,\n",
       "   25.78283,\n",
       "   26.50112,\n",
       "   25.89506,\n",
       "   27.77497,\n",
       "   28.49327,\n",
       "   27.76375,\n",
       "   29.14983,\n",
       "   25.7211,\n",
       "   27.67396,\n",
       "   27.13524,\n",
       "   27.8844,\n",
       "   26.85746,\n",
       "   28.16779,\n",
       "   27.44388,\n",
       "   28.2716,\n",
       "   26.47868,\n",
       "   27.55051,\n",
       "   27.18294,\n",
       "   29.41077,\n",
       "   27.60943,\n",
       "   27.29237,\n",
       "   27.97419,\n",
       "   28.00224,\n",
       "   29.33502,\n",
       "   28.5101,\n",
       "   27.53928,\n",
       "   28.99832,\n",
       "   29.54545,\n",
       "   27.76936,\n",
       "   29.74467,\n",
       "   29.11616,\n",
       "   28.36981,\n",
       "   28.43154,\n",
       "   27.23064,\n",
       "   27.8367,\n",
       "   29.02357,\n",
       "   28.03872,\n",
       "   30.44332,\n",
       "   29.92985,\n",
       "   28.85802,\n",
       "   27.55892,\n",
       "   28.79068,\n",
       "   28.01908,\n",
       "   27.9321,\n",
       "   27.9349,\n",
       "   27.56173,\n",
       "   28.19865,\n",
       "   29.55668,\n",
       "   26.4927,\n",
       "   30.05612,\n",
       "   29.29854,\n",
       "   27.67116,\n",
       "   29.78676,\n",
       "   28.03311,\n",
       "   27.30079,\n",
       "   29.17789,\n",
       "   27.89282,\n",
       "   29.25365,\n",
       "   30.41246,\n",
       "   29.04882,\n",
       "   28.18743,\n",
       "   30.21605,\n",
       "   29.02918,\n",
       "   28.31089,\n",
       "   28.19024,\n",
       "   18.88889,\n",
       "   26.14478,\n",
       "   26.2514,\n",
       "   27.7413,\n",
       "   30.95118,\n",
       "   35.9624,\n",
       "   38.3642,\n",
       "   42.65432,\n",
       "   44.76712,\n",
       "   45.46577,\n",
       "   46.13636,\n",
       "   46.62177,\n",
       "   46.80696,\n",
       "   47.55331,\n",
       "   49.71661,\n",
       "   53.9422,\n",
       "   55.18519,\n",
       "   55.68182,\n",
       "   56.46745,\n",
       "   56.92761,\n",
       "   57.62907,\n",
       "   58.18462,\n",
       "   58.38945,\n",
       "   59.6633,\n",
       "   60.34231,\n",
       "   60.97643,\n",
       "   61.38047,\n",
       "   62.06229,\n",
       "   62.81987,\n",
       "   63.45679,\n",
       "   63.83558,\n",
       "   64.04882,\n",
       "   64.14141,\n",
       "   64.09371,\n",
       "   64.64085,\n",
       "   64.66049,\n",
       "   64.94949,\n",
       "   64.89899,\n",
       "   64.93827,\n",
       "   64.96633,\n",
       "   65.37318,\n",
       "   65.18238,\n",
       "   65.25814,\n",
       "   65.2862,\n",
       "   65.4349,\n",
       "   65.09259,\n",
       "   65.3844,\n",
       "   65.37318,\n",
       "   65.51066,\n",
       "   65.63412,\n",
       "   65.56958,\n",
       "   65.40685,\n",
       "   65.39562,\n",
       "   65.34512,\n",
       "   65.58361,\n",
       "   65.48822,\n",
       "   65.36756,\n",
       "   65.48541,\n",
       "   65.39001,\n",
       "   65.56678,\n",
       "   65.59203,\n",
       "   65.46296,\n",
       "   65.44893,\n",
       "   65.47138,\n",
       "   65.35634,\n",
       "   65.53872,\n",
       "   65.46296,\n",
       "   65.48822,\n",
       "   65.53311,\n",
       "   65.50786,\n",
       "   65.46016,\n",
       "   65.50505,\n",
       "   65.31987,\n",
       "   65.58923,\n",
       "   65.40685,\n",
       "   65.41526,\n",
       "   65.45174,\n",
       "   65.31706,\n",
       "   65.31706,\n",
       "   65.29461,\n",
       "   65.42368,\n",
       "   65.24691,\n",
       "   65.31706,\n",
       "   65.3844,\n",
       "   65.42368,\n",
       "   65.40123,\n",
       "   65.35354,\n",
       "   65.29461,\n",
       "   65.47419,\n",
       "   65.289,\n",
       "   65.1459,\n",
       "   65.15713,\n",
       "   65.03367,\n",
       "   65.16835,\n",
       "   65.24972,\n",
       "   65.17677,\n",
       "   65.08137,\n",
       "   64.98597,\n",
       "   64.94388],\n",
       "  'val_acc': [16.11599,\n",
       "   17.38553,\n",
       "   19.5855,\n",
       "   20.06836,\n",
       "   19.50412,\n",
       "   19.57194,\n",
       "   20.10362,\n",
       "   24.3788,\n",
       "   23.25304,\n",
       "   18.84223,\n",
       "   16.72906,\n",
       "   18.96701,\n",
       "   21.91569,\n",
       "   21.14529,\n",
       "   22.0676,\n",
       "   21.22125,\n",
       "   23.96376,\n",
       "   21.69868,\n",
       "   22.97092,\n",
       "   25.79481,\n",
       "   23.71419,\n",
       "   24.8508,\n",
       "   25.8355,\n",
       "   27.50109,\n",
       "   24.38151,\n",
       "   23.20421,\n",
       "   22.75662,\n",
       "   23.31814,\n",
       "   24.14551,\n",
       "   27.22168,\n",
       "   24.77756,\n",
       "   26.12576,\n",
       "   26.46213,\n",
       "   26.26953,\n",
       "   26.58149,\n",
       "   28.37999,\n",
       "   27.31391,\n",
       "   27.35731,\n",
       "   27.17285,\n",
       "   25.32552,\n",
       "   25.4503,\n",
       "   27.43869,\n",
       "   27.25423,\n",
       "   27.57433,\n",
       "   27.5472,\n",
       "   28.38813,\n",
       "   29.43522,\n",
       "   27.85102,\n",
       "   27.58518,\n",
       "   27.84831,\n",
       "   27.87272,\n",
       "   27.38715,\n",
       "   27.56076,\n",
       "   27.46582,\n",
       "   30.53114,\n",
       "   28.05718,\n",
       "   28.19824,\n",
       "   29.21007,\n",
       "   29.89095,\n",
       "   28.84115,\n",
       "   27.73166,\n",
       "   27.83203,\n",
       "   29.03646,\n",
       "   29.56814,\n",
       "   27.7615,\n",
       "   27.2678,\n",
       "   28.20095,\n",
       "   27.63672,\n",
       "   27.06434,\n",
       "   27.87001,\n",
       "   27.5472,\n",
       "   27.98937,\n",
       "   29.86654,\n",
       "   29.66037,\n",
       "   28.67025,\n",
       "   29.74175,\n",
       "   31.09266,\n",
       "   28.96593,\n",
       "   29.99132,\n",
       "   30.16764,\n",
       "   27.86458,\n",
       "   27.06163,\n",
       "   29.87739,\n",
       "   28.75705,\n",
       "   27.74523,\n",
       "   28.54818,\n",
       "   29.38368,\n",
       "   28.62956,\n",
       "   28.22537,\n",
       "   27.27593,\n",
       "   28.86827,\n",
       "   28.84657,\n",
       "   28.42611,\n",
       "   28.59701,\n",
       "   29.82042,\n",
       "   30.37109,\n",
       "   28.33659,\n",
       "   28.30675,\n",
       "   29.08529,\n",
       "   27.68283,\n",
       "   25.42287,\n",
       "   25.65009,\n",
       "   25.49861,\n",
       "   27.21535,\n",
       "   33.35016,\n",
       "   34.38526,\n",
       "   39.35875,\n",
       "   42.06009,\n",
       "   43.87781,\n",
       "   45.08962,\n",
       "   45.41782,\n",
       "   45.01389,\n",
       "   45.82176,\n",
       "   46.50341,\n",
       "   51.19919,\n",
       "   54.50644,\n",
       "   55.03661,\n",
       "   55.7435,\n",
       "   56.75334,\n",
       "   57.30876,\n",
       "   57.13204,\n",
       "   57.51073,\n",
       "   58.06614,\n",
       "   58.92451,\n",
       "   59.73239,\n",
       "   60.38879,\n",
       "   61.34814,\n",
       "   61.75208,\n",
       "   62.23176,\n",
       "   62.73668,\n",
       "   63.46882,\n",
       "   63.21636,\n",
       "   63.9485,\n",
       "   63.87276,\n",
       "   63.74653,\n",
       "   64.20096,\n",
       "   64.30194,\n",
       "   64.9331,\n",
       "   64.45342,\n",
       "   65.2108,\n",
       "   64.52916,\n",
       "   64.90785,\n",
       "   65.43802,\n",
       "   65.18556,\n",
       "   64.75637,\n",
       "   64.98359,\n",
       "   64.95834,\n",
       "   65.2613,\n",
       "   65.16031,\n",
       "   65.05933,\n",
       "   65.18556,\n",
       "   65.10982,\n",
       "   65.03408,\n",
       "   65.41277,\n",
       "   64.88261,\n",
       "   65.2108,\n",
       "   65.2613,\n",
       "   65.5895,\n",
       "   65.10982,\n",
       "   65.28654,\n",
       "   65.31179,\n",
       "   65.13507,\n",
       "   64.88261,\n",
       "   65.69048,\n",
       "   65.08457,\n",
       "   65.33704,\n",
       "   65.36228,\n",
       "   65.31179,\n",
       "   64.90785,\n",
       "   65.31179,\n",
       "   65.38753,\n",
       "   64.9331,\n",
       "   65.13507,\n",
       "   65.61474,\n",
       "   64.8826,\n",
       "   65.16031,\n",
       "   65.28654,\n",
       "   65.13507,\n",
       "   65.23605,\n",
       "   65.13507,\n",
       "   64.75637,\n",
       "   65.56425,\n",
       "   65.28654,\n",
       "   64.68064,\n",
       "   65.03408,\n",
       "   64.90785,\n",
       "   64.85736,\n",
       "   64.78162,\n",
       "   64.98359,\n",
       "   64.88261,\n",
       "   64.90785,\n",
       "   65.2108,\n",
       "   65.08458,\n",
       "   64.88261,\n",
       "   64.95834,\n",
       "   64.9331,\n",
       "   64.9331,\n",
       "   64.80687,\n",
       "   64.35243],\n",
       "  'train_loss': [8.78487,\n",
       "   2.16872,\n",
       "   2.19722,\n",
       "   1.95965,\n",
       "   2.19277,\n",
       "   2.14127,\n",
       "   2.10726,\n",
       "   2.12263,\n",
       "   2.03924,\n",
       "   2.14822,\n",
       "   2.15634,\n",
       "   2.14328,\n",
       "   2.14815,\n",
       "   1.90936,\n",
       "   1.8087,\n",
       "   1.87549,\n",
       "   1.89506,\n",
       "   1.93537,\n",
       "   2.00678,\n",
       "   1.9076,\n",
       "   1.80783,\n",
       "   1.87765,\n",
       "   1.85531,\n",
       "   1.78537,\n",
       "   1.84525,\n",
       "   1.98236,\n",
       "   1.89184,\n",
       "   1.78548,\n",
       "   1.8399,\n",
       "   1.78692,\n",
       "   1.80219,\n",
       "   1.9118,\n",
       "   1.90648,\n",
       "   2.12392,\n",
       "   1.97624,\n",
       "   2.13682,\n",
       "   1.82443,\n",
       "   1.84456],\n",
       "  'val_loss': [2.18752,\n",
       "   2.19722,\n",
       "   1.87514,\n",
       "   2.15059,\n",
       "   2.19722,\n",
       "   2.19722,\n",
       "   2.16221,\n",
       "   2.19722,\n",
       "   2.14462,\n",
       "   1.97517,\n",
       "   2.13684,\n",
       "   1.96772,\n",
       "   2.01902,\n",
       "   2.13179,\n",
       "   2.09916,\n",
       "   1.84917,\n",
       "   2.02301,\n",
       "   2.08071,\n",
       "   2.02363,\n",
       "   1.83035,\n",
       "   1.87336,\n",
       "   1.86838,\n",
       "   1.78864,\n",
       "   1.93009,\n",
       "   1.80639,\n",
       "   1.94258,\n",
       "   1.82401,\n",
       "   2.03836,\n",
       "   1.87217,\n",
       "   1.79221,\n",
       "   1.79479,\n",
       "   2.12904,\n",
       "   2.1364,\n",
       "   1.86857,\n",
       "   1.97138,\n",
       "   1.98418,\n",
       "   1.84351,\n",
       "   1.96056],\n",
       "  'test_acc': 66.063},\n",
       " 'gdas_adult': {'train_acc': [71.47123,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.8734,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96724,\n",
       "   75.96297,\n",
       "   75.82647,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96724,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.95871,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96724,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.92885,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.94591,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.95444,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   65.03007,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297],\n",
       "  'val_acc': [67.1224,\n",
       "   75.30924,\n",
       "   74.51172,\n",
       "   74.32048,\n",
       "   74.40186,\n",
       "   75.46387,\n",
       "   74.52799,\n",
       "   75.24007,\n",
       "   75.52897,\n",
       "   75.67546,\n",
       "   75.41911,\n",
       "   75.43538,\n",
       "   75.49235,\n",
       "   75.60221,\n",
       "   75.61849,\n",
       "   75.39469,\n",
       "   75.54932,\n",
       "   75.67546,\n",
       "   75.43538,\n",
       "   75.354,\n",
       "   75.48828,\n",
       "   75.50456,\n",
       "   75.62256,\n",
       "   75.62256,\n",
       "   75.26042,\n",
       "   75.32552,\n",
       "   75.67139,\n",
       "   75.59814,\n",
       "   75.62256,\n",
       "   75.58187,\n",
       "   75.5127,\n",
       "   75.48828,\n",
       "   75.57373,\n",
       "   74.54834,\n",
       "   75.71615,\n",
       "   75.68766,\n",
       "   75.61442,\n",
       "   75.31738,\n",
       "   75.51676,\n",
       "   75.50456,\n",
       "   75.50456,\n",
       "   75.57373,\n",
       "   75.67139,\n",
       "   75.50863,\n",
       "   75.51676,\n",
       "   75.65511,\n",
       "   75.53304,\n",
       "   75.5249,\n",
       "   75.46387,\n",
       "   75.25228,\n",
       "   75.56559,\n",
       "   75.60221,\n",
       "   75.55745,\n",
       "   75.49642,\n",
       "   75.54118,\n",
       "   75.25635,\n",
       "   75.55339,\n",
       "   75.43538,\n",
       "   75.54932,\n",
       "   75.57373,\n",
       "   75.49235,\n",
       "   75.47201,\n",
       "   75.26449,\n",
       "   75.37435,\n",
       "   75.83822,\n",
       "   75.45166,\n",
       "   75.74463,\n",
       "   75.44759,\n",
       "   75.65918,\n",
       "   75.37028,\n",
       "   75.67546,\n",
       "   75.5249,\n",
       "   75.80973,\n",
       "   75.68766,\n",
       "   75.43945,\n",
       "   75.50049,\n",
       "   75.66325,\n",
       "   75.39062,\n",
       "   75.5127,\n",
       "   75.55339,\n",
       "   75.54525,\n",
       "   75.30111,\n",
       "   75.31331,\n",
       "   75.73242,\n",
       "   75.22786,\n",
       "   75.60221,\n",
       "   75.39062,\n",
       "   75.59408,\n",
       "   75.58187,\n",
       "   75.48014,\n",
       "   75.48828,\n",
       "   75.53711,\n",
       "   75.48828,\n",
       "   75.59814,\n",
       "   75.47201,\n",
       "   75.38656,\n",
       "   75.59408,\n",
       "   75.32552,\n",
       "   75.56152,\n",
       "   75.47607,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864],\n",
       "  'train_loss': [13.66828,\n",
       "   0.69315,\n",
       "   0.59502,\n",
       "   0.55853,\n",
       "   0.69315,\n",
       "   0.52815,\n",
       "   0.54892,\n",
       "   0.53308,\n",
       "   0.69279,\n",
       "   0.45329,\n",
       "   0.55307,\n",
       "   0.55535,\n",
       "   0.56836,\n",
       "   0.57217,\n",
       "   0.53927,\n",
       "   0.54672,\n",
       "   0.51448,\n",
       "   0.45884,\n",
       "   0.56463,\n",
       "   0.43044,\n",
       "   0.63938,\n",
       "   0.48607,\n",
       "   0.4757,\n",
       "   0.50117],\n",
       "  'val_loss': [1.97927,\n",
       "   0.6813,\n",
       "   0.53508,\n",
       "   0.5302,\n",
       "   0.69315,\n",
       "   0.55733,\n",
       "   0.559,\n",
       "   0.55942,\n",
       "   0.57682,\n",
       "   0.69315,\n",
       "   0.52197,\n",
       "   0.51508,\n",
       "   0.59128,\n",
       "   0.49124,\n",
       "   0.5777,\n",
       "   0.49817,\n",
       "   1.14496,\n",
       "   0.52312,\n",
       "   0.54855,\n",
       "   0.5692,\n",
       "   0.52926,\n",
       "   0.55702,\n",
       "   0.54246,\n",
       "   0.55809],\n",
       "  'test_acc': 76.377},\n",
       " 'gdas_churn': {'train_acc': [60.03472,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.84375,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   59.16667,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   77.04861,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.375,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   62.36111,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   76.02431,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.75694,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   58.95833,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   62.13542,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528],\n",
       "  'val_acc': [41.14583,\n",
       "   76.5625,\n",
       "   41.14583,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   58.85417,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   75.98958,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.40625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   75.67708,\n",
       "   76.5625,\n",
       "   58.85417,\n",
       "   76.5625,\n",
       "   58.85417,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.61458,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625],\n",
       "  'train_loss': [8.27976, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315],\n",
       "  'val_loss': [4.21944, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315],\n",
       "  'test_acc': 79.65},\n",
       " 'drnaso_covtype': {'train_acc': [48.52358,\n",
       "   43.6031,\n",
       "   45.44286,\n",
       "   46.75553,\n",
       "   48.17607,\n",
       "   49.99402,\n",
       "   50.71266,\n",
       "   51.87712,\n",
       "   49.11642,\n",
       "   51.6297,\n",
       "   51.28309,\n",
       "   52.67165,\n",
       "   50.79453,\n",
       "   51.01117,\n",
       "   51.35032,\n",
       "   52.67972,\n",
       "   51.81885,\n",
       "   52.31308,\n",
       "   52.57155,\n",
       "   52.42454,\n",
       "   52.484,\n",
       "   52.58948,\n",
       "   52.37224,\n",
       "   52.23688,\n",
       "   52.69974,\n",
       "   52.74038,\n",
       "   52.73111,\n",
       "   52.89068,\n",
       "   52.71468,\n",
       "   52.82494,\n",
       "   52.84616,\n",
       "   53.16558,\n",
       "   52.36179,\n",
       "   53.24686,\n",
       "   52.71498,\n",
       "   53.3876,\n",
       "   52.94267,\n",
       "   52.98002,\n",
       "   52.911,\n",
       "   53.19875,\n",
       "   53.2857,\n",
       "   53.22594,\n",
       "   53.37953,\n",
       "   53.29676,\n",
       "   53.56658,\n",
       "   53.1363,\n",
       "   53.52027,\n",
       "   53.5125,\n",
       "   53.46768,\n",
       "   53.436,\n",
       "   53.46917,\n",
       "   53.39447,\n",
       "   53.42973,\n",
       "   53.37266,\n",
       "   53.44019,\n",
       "   53.45513,\n",
       "   53.39507,\n",
       "   53.62246,\n",
       "   53.45184,\n",
       "   53.56987,\n",
       "   53.44706,\n",
       "   53.27017,\n",
       "   53.75095,\n",
       "   53.56389,\n",
       "   53.60782,\n",
       "   53.56628,\n",
       "   53.5636,\n",
       "   53.65264,\n",
       "   53.68521,\n",
       "   53.5128,\n",
       "   53.57166,\n",
       "   53.6123,\n",
       "   53.70433,\n",
       "   53.5131,\n",
       "   53.75095,\n",
       "   53.62246,\n",
       "   53.48232,\n",
       "   53.58272,\n",
       "   53.57943,\n",
       "   53.73989,\n",
       "   53.64577,\n",
       "   53.76828,\n",
       "   53.73302,\n",
       "   53.55613,\n",
       "   53.82685,\n",
       "   53.43511,\n",
       "   53.60125,\n",
       "   53.61648,\n",
       "   53.61678,\n",
       "   53.69597,\n",
       "   53.77426,\n",
       "   53.65682,\n",
       "   53.61947,\n",
       "   53.67684,\n",
       "   53.67894,\n",
       "   53.67535,\n",
       "   53.63172,\n",
       "   53.68402,\n",
       "   53.69866,\n",
       "   53.66997,\n",
       "   68.1144,\n",
       "   74.63261,\n",
       "   76.06451,\n",
       "   76.55874,\n",
       "   76.92239,\n",
       "   76.99978,\n",
       "   76.92149,\n",
       "   76.7437,\n",
       "   76.28383,\n",
       "   76.06122,\n",
       "   75.64289,\n",
       "   75.52486,\n",
       "   74.95981,\n",
       "   74.485,\n",
       "   74.09207,\n",
       "   73.52762,\n",
       "   73.02024,\n",
       "   72.5783,\n",
       "   71.97949,\n",
       "   71.6771,\n",
       "   71.00746,\n",
       "   70.60527,\n",
       "   70.19799,\n",
       "   69.6431,\n",
       "   69.26541,\n",
       "   68.85514,\n",
       "   68.34687,\n",
       "   67.97665,\n",
       "   67.42893,\n",
       "   67.00701,\n",
       "   66.54894,\n",
       "   66.1808,\n",
       "   65.89425,\n",
       "   65.24613,\n",
       "   65.08358,\n",
       "   64.60907,\n",
       "   64.23825,\n",
       "   63.86623,\n",
       "   63.51274,\n",
       "   63.19809,\n",
       "   62.76034,\n",
       "   62.52398,\n",
       "   62.21471,\n",
       "   61.81491,\n",
       "   61.34219,\n",
       "   61.22894,\n",
       "   60.91758,\n",
       "   60.57575,\n",
       "   60.22464,\n",
       "   60.00143,\n",
       "   59.60581,\n",
       "   59.51617,\n",
       "   59.20541,\n",
       "   58.97981,\n",
       "   58.72672,\n",
       "   58.36964,\n",
       "   58.228,\n",
       "   58.03677,\n",
       "   57.73676,\n",
       "   57.52969,\n",
       "   57.27779,\n",
       "   57.04263,\n",
       "   56.84482,\n",
       "   56.60667,\n",
       "   56.43874,\n",
       "   56.16981,\n",
       "   56.04251,\n",
       "   55.90536,\n",
       "   55.73235,\n",
       "   55.59012,\n",
       "   55.30954,\n",
       "   55.25306,\n",
       "   55.02507,\n",
       "   54.88941,\n",
       "   54.62228,\n",
       "   54.56311,\n",
       "   54.4167,\n",
       "   54.35185,\n",
       "   54.06918,\n",
       "   54.00165,\n",
       "   53.77724,\n",
       "   53.67625,\n",
       "   53.57734,\n",
       "   53.41628,\n",
       "   53.23909,\n",
       "   53.17963,\n",
       "   53.04128,\n",
       "   52.83151,\n",
       "   52.78669,\n",
       "   52.75801,\n",
       "   52.56049,\n",
       "   52.46368,\n",
       "   52.3839,\n",
       "   52.23778,\n",
       "   52.16129,\n",
       "   52.07792,\n",
       "   51.95451,\n",
       "   51.81616,\n",
       "   51.75102],\n",
       "  'val_acc': [47.88521,\n",
       "   43.83515,\n",
       "   46.6782,\n",
       "   47.23496,\n",
       "   47.98167,\n",
       "   51.21624,\n",
       "   50.61779,\n",
       "   52.00046,\n",
       "   50.41444,\n",
       "   52.37471,\n",
       "   51.72447,\n",
       "   52.98953,\n",
       "   50.96644,\n",
       "   51.80217,\n",
       "   52.47297,\n",
       "   53.02645,\n",
       "   51.84773,\n",
       "   52.75819,\n",
       "   52.79005,\n",
       "   53.21402,\n",
       "   53.06129,\n",
       "   53.14644,\n",
       "   52.47207,\n",
       "   53.28369,\n",
       "   52.99459,\n",
       "   53.44834,\n",
       "   53.26732,\n",
       "   53.34473,\n",
       "   53.38313,\n",
       "   53.35128,\n",
       "   53.23337,\n",
       "   53.75709,\n",
       "   52.37799,\n",
       "   53.6112,\n",
       "   53.39921,\n",
       "   53.60107,\n",
       "   53.67432,\n",
       "   53.46501,\n",
       "   53.02883,\n",
       "   53.39951,\n",
       "   53.8601,\n",
       "   53.71302,\n",
       "   53.66598,\n",
       "   53.73386,\n",
       "   53.89107,\n",
       "   53.51741,\n",
       "   53.95567,\n",
       "   54.00688,\n",
       "   53.85147,\n",
       "   53.83599,\n",
       "   53.78954,\n",
       "   53.98068,\n",
       "   53.81068,\n",
       "   53.89791,\n",
       "   53.74458,\n",
       "   53.76423,\n",
       "   53.94615,\n",
       "   53.98396,\n",
       "   54.02683,\n",
       "   53.98902,\n",
       "   54.04797,\n",
       "   54.01135,\n",
       "   53.85504,\n",
       "   53.96431,\n",
       "   54.02624,\n",
       "   53.83896,\n",
       "   53.92798,\n",
       "   53.93126,\n",
       "   53.83986,\n",
       "   53.97949,\n",
       "   53.9524,\n",
       "   54.03576,\n",
       "   53.91488,\n",
       "   54.05601,\n",
       "   54.06792,\n",
       "   54.00361,\n",
       "   53.95865,\n",
       "   53.88303,\n",
       "   54.04172,\n",
       "   53.94912,\n",
       "   53.98515,\n",
       "   54.03517,\n",
       "   54.1361,\n",
       "   54.22453,\n",
       "   53.98932,\n",
       "   53.97264,\n",
       "   54.00272,\n",
       "   54.06583,\n",
       "   54.08042,\n",
       "   53.99706,\n",
       "   53.83926,\n",
       "   54.13818,\n",
       "   54.14712,\n",
       "   54.06048,\n",
       "   54.10514,\n",
       "   54.05125,\n",
       "   54.10514,\n",
       "   53.95091,\n",
       "   54.15367,\n",
       "   54.16081,\n",
       "   72.86809,\n",
       "   75.21043,\n",
       "   75.44709,\n",
       "   69.69208,\n",
       "   74.42786,\n",
       "   74.9926,\n",
       "   71.78432,\n",
       "   74.73981,\n",
       "   74.29878,\n",
       "   75.53583,\n",
       "   74.2181,\n",
       "   74.02716,\n",
       "   72.05325,\n",
       "   74.19658,\n",
       "   72.98104,\n",
       "   72.01291,\n",
       "   71.95374,\n",
       "   71.74398,\n",
       "   71.94568,\n",
       "   70.36171,\n",
       "   69.88571,\n",
       "   67.99516,\n",
       "   68.89068,\n",
       "   68.74815,\n",
       "   68.71857,\n",
       "   68.34476,\n",
       "   68.2856,\n",
       "   67.27982,\n",
       "   67.11577,\n",
       "   66.65322,\n",
       "   65.30859,\n",
       "   65.59096,\n",
       "   65.14186,\n",
       "   65.01277,\n",
       "   64.67393,\n",
       "   64.62283,\n",
       "   64.12801,\n",
       "   63.13567,\n",
       "   63.01735,\n",
       "   62.9044,\n",
       "   62.35848,\n",
       "   62.44991,\n",
       "   61.83676,\n",
       "   61.62969,\n",
       "   61.04343,\n",
       "   61.28009,\n",
       "   60.89283,\n",
       "   60.89283,\n",
       "   60.42759,\n",
       "   60.21514,\n",
       "   59.05876,\n",
       "   59.38954,\n",
       "   59.35727,\n",
       "   58.71991,\n",
       "   58.70647,\n",
       "   58.42947,\n",
       "   58.69571,\n",
       "   58.44023,\n",
       "   58.09332,\n",
       "   58.02071,\n",
       "   57.81095,\n",
       "   56.89929,\n",
       "   57.5716,\n",
       "   56.87508,\n",
       "   56.8213,\n",
       "   56.6465,\n",
       "   56.46632,\n",
       "   56.26462,\n",
       "   56.18663,\n",
       "   56.02797,\n",
       "   55.75904,\n",
       "   55.8209,\n",
       "   55.43095,\n",
       "   55.10017,\n",
       "   55.22388,\n",
       "   55.21312,\n",
       "   54.90117,\n",
       "   54.68065,\n",
       "   54.62418,\n",
       "   54.42517,\n",
       "   54.73444,\n",
       "   54.15087,\n",
       "   53.8685,\n",
       "   53.97607,\n",
       "   53.91959,\n",
       "   54.04061,\n",
       "   53.53503,\n",
       "   53.34409,\n",
       "   53.46511,\n",
       "   53.28224,\n",
       "   53.19887,\n",
       "   53.01869,\n",
       "   53.01062,\n",
       "   52.817,\n",
       "   52.79279,\n",
       "   52.62068,\n",
       "   52.43243,\n",
       "   52.47277,\n",
       "   52.3894],\n",
       "  'train_loss': [1.240686,\n",
       "   1.339814,\n",
       "   1.076564,\n",
       "   1.0850379999999997,\n",
       "   1.2076600000000002,\n",
       "   1.0233219999999998,\n",
       "   1.009196,\n",
       "   0.9977825,\n",
       "   1.387342,\n",
       "   0.9701000000000001,\n",
       "   1.002918,\n",
       "   0.9701460000000001,\n",
       "   1.0761299999999998,\n",
       "   1.6955939999999998,\n",
       "   1.02163,\n",
       "   1.0906939999999998,\n",
       "   1.024336,\n",
       "   0.995308,\n",
       "   0.95494,\n",
       "   0.9810680000000002,\n",
       "   1.099138,\n",
       "   0.9856999999999999,\n",
       "   0.9998560000000001,\n",
       "   1.031644,\n",
       "   0.942052,\n",
       "   0.9839525,\n",
       "   0.958474,\n",
       "   0.9703339999999999,\n",
       "   0.921002,\n",
       "   0.98504,\n",
       "   0.928068,\n",
       "   0.9581540000000001,\n",
       "   0.9352360000000001,\n",
       "   0.9362579999999999,\n",
       "   0.9423640000000001,\n",
       "   0.9420379999999999,\n",
       "   0.94138,\n",
       "   0.9275100000000001,\n",
       "   0.9240639999999999,\n",
       "   0.9205920000000001,\n",
       "   0.9623799999999999,\n",
       "   0.9503440000000001,\n",
       "   0.91686,\n",
       "   0.9087120000000001,\n",
       "   0.909576,\n",
       "   0.894614,\n",
       "   0.913646,\n",
       "   0.89238,\n",
       "   1.221695,\n",
       "   0.90926,\n",
       "   0.907484,\n",
       "   0.907212,\n",
       "   0.899638,\n",
       "   0.91363,\n",
       "   0.888916,\n",
       "   0.90336,\n",
       "   0.909385,\n",
       "   0.89005,\n",
       "   0.9091100000000001,\n",
       "   0.8999539999999999,\n",
       "   0.905678,\n",
       "   0.9095660000000001,\n",
       "   0.8944639999999999,\n",
       "   0.905515,\n",
       "   0.9014960000000001,\n",
       "   0.887688,\n",
       "   0.897038,\n",
       "   0.9621300000000002,\n",
       "   0.8956479999999999,\n",
       "   0.920024,\n",
       "   0.903845,\n",
       "   0.897788,\n",
       "   0.9100539999999999,\n",
       "   0.9038780000000001,\n",
       "   0.9198920000000002,\n",
       "   0.9027939999999999,\n",
       "   0.892878,\n",
       "   0.9083325,\n",
       "   0.8903380000000001,\n",
       "   0.8938560000000001,\n",
       "   0.893366,\n",
       "   0.877046,\n",
       "   0.891764,\n",
       "   0.895158,\n",
       "   0.8892125000000001,\n",
       "   0.8758840000000001,\n",
       "   0.8895,\n",
       "   0.87812,\n",
       "   0.885272,\n",
       "   0.8922460000000001,\n",
       "   0.8887899999999999,\n",
       "   0.9022439999999999,\n",
       "   0.8940649999999999,\n",
       "   0.893426,\n",
       "   0.8967880000000001,\n",
       "   0.889864,\n",
       "   0.8854240000000001,\n",
       "   0.879766,\n",
       "   0.879876],\n",
       "  'val_loss': [1.209504,\n",
       "   1.212918,\n",
       "   1.1175700000000002,\n",
       "   1.23551,\n",
       "   1.16757,\n",
       "   1.034126,\n",
       "   1.1474199999999999,\n",
       "   0.9532525000000001,\n",
       "   1.0940940000000001,\n",
       "   0.9788479999999999,\n",
       "   0.9735060000000001,\n",
       "   1.0090240000000001,\n",
       "   0.971152,\n",
       "   0.9836720000000001,\n",
       "   0.9602299999999999,\n",
       "   0.952888,\n",
       "   0.996764,\n",
       "   1.046528,\n",
       "   0.956536,\n",
       "   0.9392060000000001,\n",
       "   0.9396280000000001,\n",
       "   0.9411379999999999,\n",
       "   0.9539860000000001,\n",
       "   0.9140940000000001,\n",
       "   0.946,\n",
       "   0.9633,\n",
       "   0.946444,\n",
       "   0.9422559999999999,\n",
       "   0.9281140000000001,\n",
       "   0.930566,\n",
       "   0.932738,\n",
       "   0.9019300000000001,\n",
       "   0.916586,\n",
       "   0.9294619999999998,\n",
       "   0.935068,\n",
       "   0.95923,\n",
       "   0.92509,\n",
       "   0.916525,\n",
       "   0.899046,\n",
       "   0.9042020000000001,\n",
       "   0.908352,\n",
       "   0.9208360000000001,\n",
       "   0.9018979999999999,\n",
       "   0.897096,\n",
       "   0.9045020000000001,\n",
       "   0.918522,\n",
       "   0.971444,\n",
       "   0.894256,\n",
       "   0.89332,\n",
       "   0.8922380000000001,\n",
       "   0.8808159999999999,\n",
       "   0.896406,\n",
       "   0.894942,\n",
       "   0.8921240000000001,\n",
       "   0.8844160000000001,\n",
       "   0.8871,\n",
       "   0.8930933333333333,\n",
       "   0.8859400000000001,\n",
       "   0.899982,\n",
       "   0.9338839999999999,\n",
       "   0.891254,\n",
       "   0.8923960000000001,\n",
       "   0.8922640000000002,\n",
       "   0.9006925,\n",
       "   0.885052,\n",
       "   0.90595,\n",
       "   0.881218,\n",
       "   0.908984,\n",
       "   0.885438,\n",
       "   0.87271,\n",
       "   0.9006475,\n",
       "   0.8887919999999999,\n",
       "   0.8900560000000001,\n",
       "   0.88382,\n",
       "   0.87209,\n",
       "   0.9421220000000001,\n",
       "   0.8714680000000001,\n",
       "   0.8815775,\n",
       "   0.891456,\n",
       "   0.8739039999999999,\n",
       "   0.8989240000000001,\n",
       "   0.874816,\n",
       "   0.8848480000000001,\n",
       "   0.8840920000000001,\n",
       "   0.9115599999999999,\n",
       "   0.8746659999999998,\n",
       "   0.8784179999999999,\n",
       "   0.897644,\n",
       "   0.87904,\n",
       "   0.8844480000000001,\n",
       "   0.879304,\n",
       "   0.882818,\n",
       "   0.8884425,\n",
       "   0.87942,\n",
       "   0.879554,\n",
       "   0.8857480000000001,\n",
       "   0.8778,\n",
       "   0.891578,\n",
       "   0.9036580000000001],\n",
       "  'test_acc': 54.375},\n",
       " 'drnaso_higgs-small': {'train_acc': [47.95573,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14652,\n",
       "   47.14652,\n",
       "   47.14653,\n",
       "   47.14653,\n",
       "   47.14653],\n",
       "  'val_acc': [47.86551,\n",
       "   47.229,\n",
       "   46.96568,\n",
       "   46.64307,\n",
       "   46.86628,\n",
       "   47.08426,\n",
       "   47.29527,\n",
       "   47.33887,\n",
       "   47.39293,\n",
       "   47.4714,\n",
       "   46.91511,\n",
       "   47.20285,\n",
       "   47.20982,\n",
       "   47.03718,\n",
       "   47.17843,\n",
       "   46.85582,\n",
       "   47.32492,\n",
       "   46.9308,\n",
       "   47.51674,\n",
       "   47.29701,\n",
       "   47.3999,\n",
       "   47.14355,\n",
       "   47.27783,\n",
       "   47.09996,\n",
       "   47.19587,\n",
       "   47.27958,\n",
       "   47.12088,\n",
       "   47.28655,\n",
       "   46.98661,\n",
       "   47.09821,\n",
       "   46.9465,\n",
       "   47.25865,\n",
       "   46.83489,\n",
       "   46.99358,\n",
       "   46.92034,\n",
       "   47.16797,\n",
       "   47.16797,\n",
       "   47.12263,\n",
       "   47.01625,\n",
       "   47.30748,\n",
       "   47.36677,\n",
       "   47.10519,\n",
       "   46.7756,\n",
       "   47.38421,\n",
       "   46.99881,\n",
       "   47.02323,\n",
       "   47.19761,\n",
       "   47.12263,\n",
       "   47.05985,\n",
       "   46.92906,\n",
       "   47.10519,\n",
       "   47.0738,\n",
       "   47.27958,\n",
       "   46.86105,\n",
       "   47.37723,\n",
       "   46.99881,\n",
       "   46.732,\n",
       "   47.08949,\n",
       "   47.2883,\n",
       "   47.2168,\n",
       "   47.30748,\n",
       "   46.78955,\n",
       "   47.37723,\n",
       "   47.018,\n",
       "   46.72677,\n",
       "   47.08949,\n",
       "   47.16274,\n",
       "   47.16622,\n",
       "   46.86977,\n",
       "   46.9308,\n",
       "   46.92906,\n",
       "   46.86628,\n",
       "   47.19238,\n",
       "   47.39641,\n",
       "   47.1732,\n",
       "   46.98486,\n",
       "   47.01974,\n",
       "   46.85931,\n",
       "   47.49233,\n",
       "   47.04415,\n",
       "   47.16971,\n",
       "   47.03718,\n",
       "   46.99881,\n",
       "   47.03369,\n",
       "   47.14181,\n",
       "   47.05636,\n",
       "   47.16622,\n",
       "   47.35107,\n",
       "   47.3005,\n",
       "   47.37898,\n",
       "   47.24819,\n",
       "   46.72154,\n",
       "   47.40513,\n",
       "   46.91685,\n",
       "   47.10519,\n",
       "   47.16448,\n",
       "   47.03892,\n",
       "   47.18366,\n",
       "   47.17669,\n",
       "   47.17843,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006,\n",
       "   47.10006],\n",
       "  'train_loss': [2.11762,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315],\n",
       "  'val_loss': [1.40412,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315,\n",
       "   0.69315],\n",
       "  'test_acc': 47.144},\n",
       " 'drnaso_otto': {'train_acc': [16.86308,\n",
       "   14.55387,\n",
       "   20.97643,\n",
       "   30.90067,\n",
       "   44.91582,\n",
       "   42.99383,\n",
       "   34.89618,\n",
       "   33.19585,\n",
       "   35.70426,\n",
       "   36.8743,\n",
       "   36.61055,\n",
       "   40.11223,\n",
       "   47.12963,\n",
       "   54.11897,\n",
       "   53.71493,\n",
       "   50.5303,\n",
       "   52.57576,\n",
       "   53.66442,\n",
       "   54.41919,\n",
       "   53.72334,\n",
       "   54.33221,\n",
       "   56.08586,\n",
       "   57.32604,\n",
       "   53.06678,\n",
       "   55.70707,\n",
       "   54.20314,\n",
       "   56.28788,\n",
       "   54.36588,\n",
       "   56.16723,\n",
       "   58.24916,\n",
       "   59.81481,\n",
       "   57.85354,\n",
       "   62.23906,\n",
       "   61.19529,\n",
       "   57.48878,\n",
       "   62.91526,\n",
       "   62.71605,\n",
       "   61.81538,\n",
       "   64.49214,\n",
       "   63.61672,\n",
       "   64.99439,\n",
       "   64.99439,\n",
       "   66.26824,\n",
       "   66.78451,\n",
       "   65.49383,\n",
       "   66.11953,\n",
       "   66.98934,\n",
       "   67.8367,\n",
       "   68.51291,\n",
       "   67.9349,\n",
       "   68.38103,\n",
       "   68.37823,\n",
       "   70.08137,\n",
       "   70.48541,\n",
       "   70.1431,\n",
       "   71.02974,\n",
       "   69.53984,\n",
       "   70.0477,\n",
       "   71.02694,\n",
       "   69.67452,\n",
       "   71.2037,\n",
       "   70.30303,\n",
       "   71.79854,\n",
       "   71.98653,\n",
       "   72.0174,\n",
       "   71.90516,\n",
       "   71.35522,\n",
       "   71.57407,\n",
       "   71.6358,\n",
       "   71.76487,\n",
       "   72.29798,\n",
       "   70.66498,\n",
       "   71.76768,\n",
       "   71.60213,\n",
       "   72.53086,\n",
       "   71.75926,\n",
       "   71.98934,\n",
       "   71.66386,\n",
       "   72.4523,\n",
       "   71.71437,\n",
       "   72.13524,\n",
       "   71.94725,\n",
       "   72.25028,\n",
       "   72.00617,\n",
       "   72.97699,\n",
       "   72.44108,\n",
       "   72.64029,\n",
       "   72.18575,\n",
       "   72.20819,\n",
       "   72.69921,\n",
       "   71.95847,\n",
       "   72.04265,\n",
       "   72.70483,\n",
       "   72.32043,\n",
       "   71.19248,\n",
       "   72.16049,\n",
       "   72.52525,\n",
       "   72.83951,\n",
       "   72.92368,\n",
       "   73.34456,\n",
       "   21.31874,\n",
       "   32.6431,\n",
       "   38.08923,\n",
       "   43.46801,\n",
       "   44.48092,\n",
       "   44.93547,\n",
       "   44.74747,\n",
       "   43.70651,\n",
       "   43.86364,\n",
       "   49.42761,\n",
       "   51.25421,\n",
       "   50.73513,\n",
       "   50.76599,\n",
       "   50.34792,\n",
       "   56.08866,\n",
       "   56.71437,\n",
       "   56.39169,\n",
       "   55.45455,\n",
       "   54.88215,\n",
       "   54.52862,\n",
       "   53.55499,\n",
       "   52.92368,\n",
       "   52.03984,\n",
       "   51.48709,\n",
       "   51.22054,\n",
       "   50.47138,\n",
       "   50.17957,\n",
       "   50.16554,\n",
       "   48.77946,\n",
       "   48.6532,\n",
       "   48.21268,\n",
       "   47.33165,\n",
       "   47.14646,\n",
       "   47.28114,\n",
       "   46.25701,\n",
       "   46.01571,\n",
       "   45.49383,\n",
       "   44.73625,\n",
       "   44.25926,\n",
       "   43.59708,\n",
       "   43.68687,\n",
       "   43.38384,\n",
       "   42.99663,\n",
       "   42.74691,\n",
       "   41.95847,\n",
       "   41.71998,\n",
       "   41.34119,\n",
       "   40.85297,\n",
       "   40.81369,\n",
       "   40.2862,\n",
       "   39.82884,\n",
       "   39.54265,\n",
       "   39.25645,\n",
       "   38.96184,\n",
       "   38.33614,\n",
       "   38.21268,\n",
       "   37.92649,\n",
       "   37.56734,\n",
       "   37.42985,\n",
       "   37.05387,\n",
       "   36.78732,\n",
       "   36.54602,\n",
       "   36.12514,\n",
       "   35.88664,\n",
       "   35.52189,\n",
       "   35.53872,\n",
       "   35.00561,\n",
       "   34.73625,\n",
       "   34.67733,\n",
       "   34.3743,\n",
       "   34.13019,\n",
       "   34.09371,\n",
       "   33.69529,\n",
       "   33.23513,\n",
       "   33.48765,\n",
       "   33.25758,\n",
       "   32.89282,\n",
       "   32.87879,\n",
       "   32.61504,\n",
       "   32.54489,\n",
       "   32.46352,\n",
       "   31.91639,\n",
       "   32.00617,\n",
       "   31.83221,\n",
       "   31.54882,\n",
       "   31.45342,\n",
       "   31.4422,\n",
       "   31.22615,\n",
       "   30.61728,\n",
       "   30.70146,\n",
       "   30.7211,\n",
       "   30.6229,\n",
       "   30.60045,\n",
       "   30.33951,\n",
       "   30.00842,\n",
       "   30.18238,\n",
       "   29.88777,\n",
       "   29.54265,\n",
       "   29.83726],\n",
       "  'val_acc': [17.51573,\n",
       "   14.87901,\n",
       "   21.98079,\n",
       "   36.57227,\n",
       "   45.85775,\n",
       "   46.19954,\n",
       "   35.08843,\n",
       "   35.1888,\n",
       "   39.47483,\n",
       "   39.09234,\n",
       "   39.33105,\n",
       "   39.59961,\n",
       "   46.01237,\n",
       "   51.14746,\n",
       "   52.00467,\n",
       "   52.3112,\n",
       "   51.17188,\n",
       "   56.03299,\n",
       "   54.92893,\n",
       "   52.47938,\n",
       "   52.65028,\n",
       "   56.33681,\n",
       "   55.13238,\n",
       "   54.6441,\n",
       "   53.49392,\n",
       "   58.30621,\n",
       "   57.55208,\n",
       "   56.42361,\n",
       "   57.41102,\n",
       "   56.15777,\n",
       "   59.53505,\n",
       "   61.27116,\n",
       "   62.62478,\n",
       "   60.97819,\n",
       "   60.77745,\n",
       "   61.07313,\n",
       "   62.96387,\n",
       "   64.2334,\n",
       "   64.50467,\n",
       "   64.4043,\n",
       "   64.2117,\n",
       "   62.6492,\n",
       "   65.02279,\n",
       "   65.02007,\n",
       "   66.3303,\n",
       "   65.59245,\n",
       "   67.88737,\n",
       "   69.05382,\n",
       "   68.6849,\n",
       "   67.76801,\n",
       "   68.55197,\n",
       "   66.72635,\n",
       "   69.76183,\n",
       "   69.23828,\n",
       "   70.7628,\n",
       "   71.2538,\n",
       "   69.46072,\n",
       "   69.04026,\n",
       "   69.93815,\n",
       "   71.38401,\n",
       "   70.97982,\n",
       "   70.71669,\n",
       "   70.66786,\n",
       "   70.24468,\n",
       "   70.41016,\n",
       "   72.47993,\n",
       "   72.40668,\n",
       "   70.90115,\n",
       "   72.39583,\n",
       "   72.66981,\n",
       "   71.82075,\n",
       "   71.47352,\n",
       "   71.84787,\n",
       "   71.94824,\n",
       "   70.68414,\n",
       "   71.45182,\n",
       "   72.2602,\n",
       "   71.8316,\n",
       "   70.93913,\n",
       "   72.52604,\n",
       "   72.0459,\n",
       "   71.7258,\n",
       "   72.46365,\n",
       "   72.40668,\n",
       "   71.03407,\n",
       "   72.57758,\n",
       "   72.56944,\n",
       "   72.13813,\n",
       "   70.90658,\n",
       "   72.52604,\n",
       "   72.0242,\n",
       "   71.95638,\n",
       "   72.71593,\n",
       "   72.61556,\n",
       "   72.85428,\n",
       "   71.57661,\n",
       "   72.41482,\n",
       "   72.602,\n",
       "   71.69596,\n",
       "   71.19141,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   25.42287,\n",
       "   34.66296,\n",
       "   10.60338,\n",
       "   13.7844,\n",
       "   13.93587,\n",
       "   15.40015,\n",
       "   23.47892,\n",
       "   18.53067,\n",
       "   27.69503,\n",
       "   37.94496,\n",
       "   38.19742,\n",
       "   33.7541,\n",
       "   41.45418,\n",
       "   43.87781,\n",
       "   33.82984,\n",
       "   49.48245,\n",
       "   24.86746,\n",
       "   40.26761,\n",
       "   40.04039,\n",
       "   45.21585,\n",
       "   47.10931,\n",
       "   35.77379,\n",
       "   44.86241,\n",
       "   30.39636,\n",
       "   37.0361,\n",
       "   32.26458,\n",
       "   20.87857,\n",
       "   30.21964,\n",
       "   30.16915,\n",
       "   42.21156,\n",
       "   41.40369,\n",
       "   38.52562,\n",
       "   24.71598,\n",
       "   38.32366,\n",
       "   36.88462,\n",
       "   37.66726,\n",
       "   29.76521,\n",
       "   32.34032,\n",
       "   37.06135,\n",
       "   37.31381,\n",
       "   39.23252,\n",
       "   36.45544,\n",
       "   37.99546,\n",
       "   39.86367,\n",
       "   28.67963,\n",
       "   36.27872,\n",
       "   38.24792,\n",
       "   37.51578,\n",
       "   35.87478,\n",
       "   37.54102,\n",
       "   33.24918,\n",
       "   37.76824,\n",
       "   35.67281,\n",
       "   35.04166,\n",
       "   34.00656,\n",
       "   32.36556,\n",
       "   34.30952,\n",
       "   30.97703,\n",
       "   35.74855,\n",
       "   34.63772,\n",
       "   35.64756,\n",
       "   32.99672,\n",
       "   32.49179,\n",
       "   33.22393,\n",
       "   33.40066,\n",
       "   31.88589,\n",
       "   33.32492,\n",
       "   32.61803,\n",
       "   32.26458,\n",
       "   33.22393,\n",
       "   32.26458,\n",
       "   32.39081,\n",
       "   32.54229,\n",
       "   31.179,\n",
       "   31.22949,\n",
       "   32.23933,\n",
       "   31.20424,\n",
       "   30.97703,\n",
       "   31.5072,\n",
       "   31.38096,\n",
       "   30.16915,\n",
       "   31.05276,\n",
       "   30.29538,\n",
       "   30.59833,\n",
       "   31.43146,\n",
       "   30.64883,\n",
       "   30.67407,\n",
       "   30.1439,\n",
       "   30.8003,\n",
       "   29.96718,\n",
       "   30.09341,\n",
       "   29.61373,\n",
       "   29.26029,\n",
       "   29.84095,\n",
       "   29.00783,\n",
       "   29.18455,\n",
       "   29.538],\n",
       "  'train_loss': [2.36834,\n",
       "   2.15341,\n",
       "   2.17165,\n",
       "   1.92105,\n",
       "   1.75966,\n",
       "   1.51906,\n",
       "   1.64404,\n",
       "   1.48133,\n",
       "   1.28607,\n",
       "   1.24846,\n",
       "   1.60792,\n",
       "   1.38307,\n",
       "   1.17598,\n",
       "   1.09542,\n",
       "   1.02516,\n",
       "   1.25452,\n",
       "   1.3041,\n",
       "   1.11332,\n",
       "   0.90759,\n",
       "   0.9364,\n",
       "   0.74881,\n",
       "   0.86426,\n",
       "   0.80295,\n",
       "   1.06206,\n",
       "   0.83455,\n",
       "   0.75288,\n",
       "   0.66429,\n",
       "   0.73004,\n",
       "   0.7277,\n",
       "   0.77963,\n",
       "   0.76229,\n",
       "   0.72664,\n",
       "   0.79621,\n",
       "   0.71522,\n",
       "   0.83266,\n",
       "   0.87844,\n",
       "   0.73437,\n",
       "   0.71876,\n",
       "   0.75183],\n",
       "  'val_loss': [2.26498,\n",
       "   2.15539,\n",
       "   1.54841,\n",
       "   1.8182,\n",
       "   1.74804,\n",
       "   1.63227,\n",
       "   1.74196,\n",
       "   1.57551,\n",
       "   1.46235,\n",
       "   1.42521,\n",
       "   1.39775,\n",
       "   1.14363,\n",
       "   1.64139,\n",
       "   1.41202,\n",
       "   1.07144,\n",
       "   0.99569,\n",
       "   1.07313,\n",
       "   0.98692,\n",
       "   1.01354,\n",
       "   0.89545,\n",
       "   0.84816,\n",
       "   1.37775,\n",
       "   0.85084,\n",
       "   0.79466,\n",
       "   0.82424,\n",
       "   0.81919,\n",
       "   0.75093,\n",
       "   0.72824,\n",
       "   0.99277,\n",
       "   0.75987,\n",
       "   0.76768,\n",
       "   0.78848,\n",
       "   0.72568,\n",
       "   0.72238,\n",
       "   0.9894,\n",
       "   0.80662,\n",
       "   0.98801,\n",
       "   0.74452,\n",
       "   0.78681],\n",
       "  'test_acc': 47.115},\n",
       " 'drnaso_adult': {'train_acc': [71.47549,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.95871,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.93738,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.93738,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.93311,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   66.10076,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297,\n",
       "   75.96297],\n",
       "  'val_acc': [71.39486,\n",
       "   75.30924,\n",
       "   75.51676,\n",
       "   75.41504,\n",
       "   75.43538,\n",
       "   75.47201,\n",
       "   75.5778,\n",
       "   75.24007,\n",
       "   75.52897,\n",
       "   75.67546,\n",
       "   75.41911,\n",
       "   75.43538,\n",
       "   75.49235,\n",
       "   75.60221,\n",
       "   75.61849,\n",
       "   75.39469,\n",
       "   75.54932,\n",
       "   75.65104,\n",
       "   75.43538,\n",
       "   75.354,\n",
       "   75.46387,\n",
       "   75.50456,\n",
       "   75.62256,\n",
       "   75.62256,\n",
       "   75.26042,\n",
       "   75.32552,\n",
       "   75.67139,\n",
       "   75.59814,\n",
       "   75.60221,\n",
       "   75.58187,\n",
       "   75.5127,\n",
       "   75.48828,\n",
       "   75.57373,\n",
       "   75.38656,\n",
       "   75.71615,\n",
       "   75.69987,\n",
       "   75.61442,\n",
       "   75.40283,\n",
       "   75.51676,\n",
       "   75.50456,\n",
       "   75.50456,\n",
       "   75.57373,\n",
       "   75.65918,\n",
       "   75.50456,\n",
       "   75.51676,\n",
       "   75.65511,\n",
       "   75.53304,\n",
       "   75.5249,\n",
       "   75.46387,\n",
       "   75.25228,\n",
       "   75.56559,\n",
       "   75.60221,\n",
       "   75.55339,\n",
       "   75.49642,\n",
       "   75.54118,\n",
       "   75.25635,\n",
       "   75.55339,\n",
       "   75.48014,\n",
       "   75.54932,\n",
       "   75.57373,\n",
       "   75.49235,\n",
       "   75.47201,\n",
       "   75.26449,\n",
       "   75.37435,\n",
       "   75.83822,\n",
       "   75.45166,\n",
       "   75.74463,\n",
       "   75.44759,\n",
       "   75.65918,\n",
       "   75.37028,\n",
       "   75.67546,\n",
       "   75.5249,\n",
       "   75.80973,\n",
       "   75.68766,\n",
       "   75.43945,\n",
       "   75.50049,\n",
       "   75.67952,\n",
       "   75.39062,\n",
       "   75.5127,\n",
       "   75.55339,\n",
       "   75.54525,\n",
       "   75.30111,\n",
       "   75.31331,\n",
       "   75.71208,\n",
       "   75.22786,\n",
       "   75.60221,\n",
       "   75.39062,\n",
       "   75.59408,\n",
       "   75.58187,\n",
       "   75.48014,\n",
       "   75.48828,\n",
       "   75.53711,\n",
       "   75.48828,\n",
       "   75.59814,\n",
       "   75.46794,\n",
       "   75.37842,\n",
       "   75.59408,\n",
       "   75.32552,\n",
       "   75.56152,\n",
       "   75.47607,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864,\n",
       "   75.50864],\n",
       "  'train_loss': [4.2585,\n",
       "   0.53685,\n",
       "   0.4211,\n",
       "   0.53137,\n",
       "   0.57266,\n",
       "   0.44208,\n",
       "   0.45013,\n",
       "   0.51197,\n",
       "   0.50879,\n",
       "   0.39849,\n",
       "   0.40905,\n",
       "   0.42635,\n",
       "   0.44307,\n",
       "   0.49287,\n",
       "   0.40953,\n",
       "   0.41123,\n",
       "   0.40477,\n",
       "   0.41784,\n",
       "   0.39345,\n",
       "   0.42247,\n",
       "   0.39546,\n",
       "   0.41061,\n",
       "   0.41279,\n",
       "   0.40368,\n",
       "   0.40181],\n",
       "  'val_loss': [1.99388,\n",
       "   0.56263,\n",
       "   0.45028,\n",
       "   0.46403,\n",
       "   0.47315,\n",
       "   0.43482,\n",
       "   0.47564,\n",
       "   0.45587,\n",
       "   0.40939,\n",
       "   0.4497,\n",
       "   0.44173,\n",
       "   0.41663,\n",
       "   0.40475,\n",
       "   0.44318,\n",
       "   0.42854,\n",
       "   0.49269,\n",
       "   0.46023,\n",
       "   0.40483,\n",
       "   0.40699,\n",
       "   0.49164,\n",
       "   0.41023,\n",
       "   0.41708,\n",
       "   0.4047,\n",
       "   0.41519,\n",
       "   0.44788],\n",
       "  'test_acc': 76.377},\n",
       " 'drnaso_churn': {'train_acc': [79.02778,\n",
       "   37.86458,\n",
       "   79.75694,\n",
       "   79.96528,\n",
       "   79.73958,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.94792,\n",
       "   79.96528,\n",
       "   79.73958,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528,\n",
       "   79.96528],\n",
       "  'val_acc': [76.04167,\n",
       "   69.58333,\n",
       "   76.5625,\n",
       "   76.61458,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.61458,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625,\n",
       "   76.5625],\n",
       "  'train_loss': [0.66916, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315],\n",
       "  'val_loss': [0.67365, 0.69315, 0.69315, 0.69315, 0.69315, 0.69315],\n",
       "  'test_acc': 79.65}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e61f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
